{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming  framework. In particular, there is often a need to maintain multiple LLM  conversations, each instructed in different ways, and \"responsible\" for  different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation  state, along with access to long-term memory (vector-stores) and tools (a.k.a functions  or plugins). Thus a Multi-Agent Programming framework is a natural fit  for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly  designed  with Agents as first-class citizens, and Multi-Agent Programming  as the core  design principle. The framework is inspired by ideas from the  Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation  among agents. There is a principled mechanism to orchestrate multi-agent  collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent,  flexible, and allows other types of orchestration to be implemented. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.  </p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;    Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each    entity: LLM, Agent, User. </li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusabilily, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,   GPT-4.</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching.    Caching with Momento is also supported.</li> <li>Vector-stores: Qdrant and Chroma are currently supported.   Vector stores allow for Retrieval-Augmented-Generation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul> <p>Don't worry if some of these terms are not clear to you.  The Getting Started Guide and subsequent pages  will help you get up to speed.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/","title":"Language Models: Completion and Chat-Completion","text":"<p>Transformer-based language models are fundamentally next-token predictors, so  naturally all LLM APIs today at least provide a completion endpoint.  If an LLM is a next-token predictor, how could it possibly be used to  generate a response to a question or instruction, or to engage in a conversation with  a human user? This is where the idea of \"chat-completion\" comes in. This post is a refresher on the distinction between completion and chat-completion, and some interesting details on how chat-completion is implemented in practice.</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#language-models-as-next-token-predictors","title":"Language Models as Next-token Predictors","text":"<p>A Language Model is essentially a \"next-token prediction\" model, and so all LLMs today provide a \"completion\" endpoint, typically something like: <code>/completions</code> under the base URL.</p> <p>The endpoint simply takes a prompt and returns a completion (i.e. a continuation).</p> <p>A typical prompt sent to a completion endpoint might look like this: <pre><code>The capital of Belgium is \n</code></pre> and the LLM will return a completion like this: <pre><code>Brussels.\n</code></pre> OpenAI's GPT3 is an example of a pure completion LLM. But interacting with a completion LLM is not very natural or useful: you cannot give instructions or ask questions; instead you would always need to  formulate your input as a prompt whose natural continuation is your desired output. For example, if you wanted the LLM to highlight all proper nouns in a sentence, you would format it as the following prompt:</p> <p>Chat-To-Prompt Example: Chat/Instruction converted to a completion prompt.</p> <p><pre><code>User: here is a sentence, the Assistant's task is to identify all proper nouns.\n     Jack lives in Bosnia, and Jill lives in Belgium.\nAssistant:    \n</code></pre> The natural continuation of this prompt would be a response listing the proper nouns, something like: <pre><code>John, Bosnia, Jill, Belgium are all proper nouns.\n</code></pre></p> <p>This seems sensible in theory, but a \"base\" LLM that performs well on completions may not perform well on these kinds of prompts. The reason is that during its training, it may not have been exposed to very many examples of this type of prompt-response pair. So how can an LLM be improved to perform well on these kinds of prompts?</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#instruction-tuned-aligned-llms","title":"Instruction-tuned, Aligned LLMs","text":"<p>This brings us to the heart of the innovation behind the wildly popular ChatGPT: it uses an enhancement of GPT3 that (besides having a lot more parameters), was explicitly fine-tuned on instructions (and dialogs more generally) -- this is referred to as instruction-fine-tuning or IFT for short. In addition to fine-tuning instructions/dialogs, the models behind ChatGPT (i.e., GPT-3.5-Turbo and GPT-4) are further tuned to produce responses that align with human preferences (i.e. produce responses that are more helpful and safe), using a procedure called Reinforcement Learning with Human Feedback (RLHF). See this OpenAI InstructGPT Paper for details on these techniques and references to the  original papers that introduced these ideas. Another recommended read is Sebastian  Raschka's post on RLHF and related techniques. </p> <p>For convenience, we refer to the combination of IFT and RLHF as chat-tuning. A chat-tuned LLM can be expected to perform well on prompts such as the one in  the Chat-To-Prompt Example above. These types of prompts are still unnatural, however,  so as a convenience, chat-tuned LLM API servers also provide a \"chat-completion\"  endpoint (typically <code>/chat/completions</code> under the base URL), which allows the user to interact with them in a natural dialog, which might look like this (the portions in square brackets are indicators of who is generating the text):</p> <p><pre><code>[User] What is the capital of Belgium?\n[Assistant] The capital of Belgium is Brussels.\n</code></pre> or <pre><code>[User] In the text below, find all proper nouns:\n    Jack lives in Bosnia, and Jill lives in Belgium.\n[Assistant] John, Bosnia, Jill, Belgium are all proper nouns.\n[User] Where does John live?\n[Assistant] John lives in Bosnia.\n</code></pre></p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#chat-completion-endpoints-under-the-hood","title":"Chat Completion Endpoints: under the hood","text":"<p>How could this work, given that LLMs are fundamentally next-token predictors? This is a convenience provided by the LLM API service (e.g. from OpenAI or local model server libraries): when a user invokes the chat-completion endpoint (typically at <code>/chat/completions</code> under the base URL), under the hood, the server converts the instructions and multi-turn chat history into a single string, with annotations indicating user and assistant turns, and ending with something like \"Assistant:\" as in the Chat-To-Prompt Example above.</p> <p>Now the subtle detail to note here is this:</p> <p>It matters how the dialog (instructions plus chat history) is converted into a single prompt string. Converting to a single prompt by simply concatenating the instructions and chat history using an \"intuitive\" format (e.g. indicating user, assistant turns using \"User\", \"Assistant:\", etc.) can work, however most local LLMs are trained on a specific prompt format. So if we format chats in a different way, we may get odd/inferior results.</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#converting-chats-to-prompts-formatting-rules","title":"Converting Chats to Prompts: Formatting Rules","text":"<p>For example, the llama2 models are trained on a format where the user's input is bracketed within special strings <code>[INST]</code> and <code>[/INST]</code>. There are other requirements that we don't go into here, but interested readers can refer to these links:</p> <ul> <li>A reddit thread on the llama2 formats</li> <li>Facebook's llama2 code</li> <li>Langroid's llama2 formatting code</li> </ul> <p>A dialog fed to a Llama2 model in its expected prompt format would look like this:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nHi there! \n[/INST] \nHello! How can I help you today? &lt;/s&gt;\n&lt;s&gt;[INST] In the text below, find all proper nouns:\n    Jack lives in Bosnia, and Jill lives in Belgium.\n [/INST] \nJohn, Bosnia, Jill, Belgium are all proper nouns. &lt;/s&gt;&lt;s&gt; \n[INST] Where does Jack live? [/INST] \nJack lives in Bosnia. &lt;/s&gt;&lt;s&gt;\n[INST] And Jill? [/INST]\nJill lives in Belgium. &lt;/s&gt;&lt;s&gt;\n[INST] Which are its neighboring countries? [/INST]\n</code></pre> <p>This means that if an LLM server library wants to provide a chat-completion endpoint for a local model, it needs to provide a way to convert chat history to a single prompt using the specific formatting rules of the model. For example the <code>oobabooga/text-generation-webui</code>  library has an extensive set of chat formatting templates for a variety of models, and their model server auto-detects the format template from the model name.</p> <p>Chat completion model names: look for 'chat' or 'instruct' in the name</p> <p>You can search for a variety of models on the HuggingFace model hub. For example if you see a name <code>Llama-2-70B-chat-GGUF</code> you know it is chat-tuned. Another example of a chat-tuned model is <code>Llama-2-7B-32K-Instruct</code> </p> <p>A user of these local LLM server libraries thus has two options when using a  local model in chat mode:</p> <ul> <li>use the chat-completion endpoint, and let the underlying library handle the chat-to-prompt formatting, or</li> <li>first format the chat history according to the model's requirements, and then use the   completion endpoint</li> </ul>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#using-local-models-in-langroid","title":"Using Local Models in Langroid","text":"<p>Local models can be used in Langroid by defining a <code>LocalModelConfig</code> object. More details are in this tutorial,  but here we briefly discuss prompt-formatting in this context. Langroid provides a built-in formatter for LLama2 models,  so users looking to use llama2 models with langroid can try either of these options, by setting the <code>use_completion_for_chat</code> flag in the <code>LocalModelConfig</code> object (See the local-LLM tutorial for details).</p> <p>When this flag is set to <code>True</code>, the chat history is formatted using the built-in  Langroid llama2 formatter and the completion endpoint are used. When the flag is set to <code>False</code>, the chat  history is sent directly to the chat-completion endpoint, which internally converts the  chat history to a prompt in the expected llama2 format.</p> <p>For local models other than Llama2, users can either:</p> <ul> <li>write their own formatters by writing a class similar to <code>Llama2Formatter</code> and  then setting the <code>use_completion_for_chat</code> flag to <code>True</code> in the <code>LocalModelConfig</code> object, or</li> <li>use an LLM server library (such as the <code>oobabooga</code> library mentioned above) that provides a chat-completion endpoint,  and converts chats to single prompts under the hood, and set the   <code>use_completion_for_chat</code> flag to <code>False</code> in the <code>LocalModelConfig</code> object.</li> </ul> <p>You can use a similar approach if you are using an LLM application framework other than Langroid.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming framework. In particular, there is often a need to maintain multiple LLM conversations, each instructed in different ways, and \"responsible\" for different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation state, along with access to long-term memory (vector-stores) and tools (a.k.a functions or plugins). Thus a Multi-Agent Programming framework is a natural fit for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly designed  with Agents as first-class citizens, and Multi-Agent Programming as the core  design principle. The framework is inspired by ideas from the Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation among agents. There is a principled mechanism to orchestrate multi-agent collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent, flexible, and allows other types of orchestration to be implemented. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;   Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each   entity: LLM, Agent, User.</li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusability, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,   GPT-4.</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching.   Caching with Momento is also supported.</li> <li>Vector-stores: Qdrant and Chroma are currently supported.   Vector stores allow for Retrieval-Augmented-Generaation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul>"},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/","title":"Langroid: Knolwedge Graph RAG powered by Neo4j","text":""},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/#chat-with-various-sources-of-information","title":"\"Chat\" with various sources of information","text":"<p>LLMs are increasingly being used to let users converse in natural language with  a variety of types of data sources:</p> <ul> <li>unstructured text documents: a user's query is augmented with \"relevant\" documents or chunks   (retrieved from an embedding-vector store) and fed to the LLM to generate a response --    this is the idea behind Retrieval Augmented Generation (RAG).</li> <li>SQL Databases: An LLM translates a user's natural language question into an SQL query,   which is then executed by another module, sending results to the LLM, so it can generate   a natural language response based on the results.</li> <li>Tabular datasets: similar to the SQL case, except instead of an SQL Query, the LLM generates    a Pandas dataframe expression.</li> </ul> <p>Langroid has had specialized Agents for the above scenarios: <code>DocChatAgent</code> for RAG with unstructured text documents, <code>SQLChatAgent</code> for SQL databases, and <code>TableChatAgent</code> for tabular datasets.</p>"},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/#adding-support-for-neo4j-knowledge-graphs","title":"Adding support for Neo4j Knowledge Graphs","text":"<p>Analogous to the SQLChatAgent, Langroid now has a  <code>Neo4jChatAgent</code>  to interact with a Neo4j knowledge graph using natural language. This Agent has access to two key tools that enable it to handle a user's queries:</p> <ul> <li><code>GraphSchemaTool</code> to get the schema of a Neo4j knowledge graph.</li> <li><code>CypherRetrievalTool</code> to generate Cypher queries from a user's query. Cypher is a specialized query language for Neo4j, and even though it is not as widely known as SQL, most LLMs today can generate Cypher Queries.</li> </ul> <p>Setting up a basic Neo4j-based RAG chatbot is straightforward. First ensure  you set these environment variables (or provide them in a <code>.env</code> file): <pre><code>NEO4J_URI=&lt;uri&gt;\nNEO4J_USERNAME=&lt;username&gt;\nNEO4J_PASSWORD=&lt;password&gt;\nNEO4J_DATABASE=&lt;database&gt;\n</code></pre></p> <p>Then you can configure and define a <code>Neo4jChatAgent</code> like this: <pre><code>import langroid as lr\nimport langroid.language_models as lm\n\nfrom langroid.agent.special.neo4j.neo4j_chat_agent import (\n    Neo4jChatAgent,\n    Neo4jChatAgentConfig,\n    Neo4jSettings,\n)\n\nllm_config = lm.OpenAIGPTConfig()\n\nload_dotenv()\n\nneo4j_settings = Neo4jSettings()\n\nkg_rag_agent_config = Neo4jChatAgentConfig(\n    neo4j_settings=neo4j_settings,\n    llm=llm_config, \n)\nkg_rag_agent = Neo4jChatAgent(kg_rag_agent_config)\nkg_rag_task = lr.Task(kg_rag_agent, name=\"kg_RAG\")\nkg_rag_task.run()\n</code></pre></p>"},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/#example-pypi-package-dependency-chatbot","title":"Example: PyPi Package Dependency Chatbot","text":"<p>In the Langroid-examples repository, there is an example python  script showcasing tools/Function-calling + RAG using a <code>DependencyGraphAgent</code> derived from <code>Neo4jChatAgent</code>. This agent uses two tools, in addition to the tools available to <code>Neo4jChatAgent</code>:</p> <ul> <li><code>GoogleSearchTool</code> to find package version and type information, as well as to answer   other web-based questions after acquiring the required information from the dependency graph.</li> <li><code>DepGraphTool</code> to construct a Neo4j knowledge-graph modeling the dependency structure    for a specific package, using the API at DepsDev.</li> </ul> <p>In response to a user's query about dependencies, the Agent decides whether to use a Cypher query or do a web search. Here is what it looks like in action:</p> <p> </p>  Chatting with the `DependencyGraphAgent` (derived from Langroid's `Neo4jChatAgent`). When a user specifies a Python package name (in this case \"chainlit\"), the agent searches the web using `GoogleSearchTool` to find the version of the package, and then uses the `DepGraphTool` to construct the dependency graph as a neo4j knowledge graph. The agent then answers questions by generating Cypher queries to the knowledge graph, or by searching the web."},{"location":"blog/2023/09/14/using-langroid-with-local-llms/","title":"Using Langroid with Local LLMs","text":""},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#why-local-models","title":"Why local models?","text":"<p>There are commercial, remotely served models that currently appear to beat all open/local models. So why care about local models? Local models are exciting for a number of reasons:</p> <ul> <li>cost: other than compute/electricity, there is no cost to use them.</li> <li>privacy: no concerns about sending your data to a remote server.</li> <li>latency: no network latency due to remote API calls, so faster response times, provided you can get fast enough inference.</li> <li>uncensored: some local models are not censored to avoid sensitive topics.</li> <li>fine-tunable: you can fine-tune them on private/recent data, which current commercial models don't have access to.</li> <li>sheer thrill: having a model running on your machine with no internet connection,   and being able to have an intelligent conversation with it -- there is something almost magical about it.</li> </ul> <p>The main appeal with local models is that with sufficiently careful prompting, they may behave sufficiently well to be useful for specific tasks/domains, and bring all of the above benefits. Some ideas on how you might use local LLMs:</p> <ul> <li>In a multi-agent system, you could have some agents use local models for narrow    tasks with a lower bar for accuracy (and fix responses with multiple tries).</li> <li>You could run many instances of the same or different models and combine their responses.</li> <li>Local LLMs can act as a privacy layer, to identify and handle sensitive data before passing to remote LLMs.</li> <li>Some local LLMs have intriguing features, for example llama.cpp lets you    constrain its output using a grammar.</li> </ul>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#running-llms-locally","title":"Running LLMs locally","text":"<p>There are several ways to use LLMs locally. See the <code>r/LocalLLaMA</code> subreddit for a wealth of information. There are open source libraries that offer front-ends to run local models, for example <code>oobabooga/text-generation-webui</code> (or \"ooba-TGW\" for short) but the focus in this tutorial is on spinning up a server that mimics an OpenAI-like API, so that any code that works with the OpenAI API (for say GPT3.5 or GPT4) will work with a local model, with just a simple change: set <code>openai.api_base</code> to the URL where the local API server is listening, typically <code>http://localhost:8000/v1</code>.</p> <p>There are a few libraries we recommend for setting up local models with OpenAI-like APIs:</p> <ul> <li>LiteLLM OpenAI Proxy Server lets you set up a local    proxy server for over 100+ LLM providers (remote and local).</li> <li>ooba-TGW mentioned above, for a variety of models, including llama2 models.</li> <li>llama-cpp-python (LCP for short), specifically for llama2 models.</li> <li>ollama</li> </ul> <p>We recommend visiting these links to see how to install and run these libraries.</p>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#use-the-local-model-with-the-openai-library","title":"Use the local model with the OpenAI library","text":"<p>Once you have a server running using any of the above methods,  your code that works with the OpenAI models can be made to work  with the local model, by simply changing the <code>openai.api_base</code> to the  URL where the local server is listening. </p> <p>If you are using Langroid to build LLM applications, the framework takes care of the <code>api_base</code> setting in most cases, and you need to only set the <code>chat_model</code> parameter in the LLM config object for the LLM model you are using. See the Non-OpenAI LLM tutorial for more details.</p>"},{"location":"demos/targeting/audience-targeting/","title":"Audience Targeting for a Business","text":"<p>Suppose you are a marketer for a business, trying to figure out which  audience segments to target. Your downstream systems require that you specify standardized audience segments to target, for example from the IAB Audience Taxonomy.</p> <p>There are thousands of standard audience segments, and normally you would need  to search the list for potential segments that match what you think your ideal customer profile is. This is a tedious, error-prone task.</p> <p>But what if we can leverage an LLM such as GPT-4? We know that GPT-4 has  skills that are ideally suited for this task:</p> <ul> <li>General knowledge about businesses and their ideal customers</li> <li>Ability to recognize which standard segments match an English description of a customer profile</li> <li>Ability to plan a conversation to get the information it needs to answer a question</li> </ul> <p>Once you decide to use an LLM, you still need to figure out how to organize the  various components of this task:</p> <ul> <li>Research: What are some ideal customer profiles for the business</li> <li>Segmentation: Which standard segments match an English description of a customer profile</li> <li>Planning: how to organize the task to identify a few standard segments</li> </ul>"},{"location":"demos/targeting/audience-targeting/#using-langroid-agents","title":"Using Langroid Agents","text":"<p>Langroid makes it intuitive and simple to build an LLM-powered system organized around agents, each responsible for a different task. In less than a day we built a 3-agent system to automate this task:</p> <ul> <li>The <code>Marketer</code> Agent is given the Planning role.</li> <li>The <code>Researcher</code> Agent is given the Research role,    and it has access to the business description. </li> <li>The <code>Segmentor</code> Agent is given the Segmentation role. It has access to the    IAB Audience Taxonomy via a vector database, i.e. its rows have been mapped to   vectors via an embedding model, and these vectors are stored in a vector-database.    Thus given an English description of a customer profile,   the <code>Segmentor</code> Agent maps it to a vector using the embedding model,   and retrieves the nearest (in vector terms, e.g. cosine similarity)    IAB Standard Segments from the vector-database. The Segmentor's LLM    further refines this by selecting the best-matching segments from the retrieved list.</li> </ul> <p>To kick off the system, the human user describes a business in English, or provides the URL of the business's website.  The <code>Marketer</code> Agent sends customer profile queries to the <code>Researcher</code>, who answers in plain English based on  the business description, and the Marketer takes this description and sends it to the Segmentor, who maps it to Standard IAB Segments. The task is done when the Marketer finds 4 Standard segments.  The agents are depicted in the diagram below:</p> <p></p>"},{"location":"demos/targeting/audience-targeting/#an-example-glashutte-watches","title":"An example: Glashutte Watches","text":"<p>The human user first provides the URL of the business, in this case: <pre><code>https://www.jomashop.com/glashutte-watches.html\n</code></pre> From this URL, the <code>Researcher</code> agent summarizes its understanding of the business. The <code>Marketer</code> agent starts by asking the <code>Researcher</code>: <pre><code>Could you please describe the age groups and interests of our typical customer?\n</code></pre> The <code>Researcher</code> responds with an English description of the customer profile: <pre><code>Our typical customer is a fashion-conscious individual between 20 and 45 years...\n</code></pre> The <code>Researcher</code> forwards this English description to the <code>Segmentor</code> agent, who maps it to a standardized segment, e.g.: <pre><code>Interest|Style &amp; Fashion|Fashion Trends\n...\n</code></pre> This conversation continues until the <code>Marketer</code> agent has identified 4 standardized segments.</p> <p>Here is what the conversation looks like:</p> <p></p>"},{"location":"examples/agent-tree/","title":"Hierarchical computation with Langroid Agents","text":"<p>Here is a simple example showing tree-structured computation where each node in the tree is handled by a separate agent. This is a toy numerical example, and illustrates:</p> <ul> <li>how to have agents organized in a hierarchical structure to accomplish a task </li> <li>the use of global state accessible to all agents, and </li> <li>the use of tools/function-calling.</li> </ul>"},{"location":"examples/agent-tree/#the-computation","title":"The Computation","text":"<p>We want to carry out the following calculation for a given input number \\(n\\):</p> <pre><code>def Main(n):\n    if n is odd:\n        return (3*n+1) + n\n    else:\n        if n is divisible by 10:\n            return n/10 + n\n        else:\n            return n/2 + n\n</code></pre>"},{"location":"examples/agent-tree/#using-function-composition","title":"Using function composition","text":"<p>Imagine we want to do this calculation using a few auxiliary functions:</p> <pre><code>def Main(n):\n    # return non-null value computed by Odd or Even\n    Record n as global variable # to be used by Adder below\n    return Odd(n) or Even(n)\n\ndef Odd(n):\n    # Handle odd n\n    if n is odd:\n        new = 3*n+1\n        return Adder(new)\n    else:\n        return None\n\ndef Even(n):\n    # Handle even n: return non-null value computed by EvenZ or EvenNZ\n    return EvenZ(n) or EvenNZ(n)\n\ndef EvenZ(n):\n    # Handle even n divisible by 10, i.e. ending in Zero\n    if n is divisible by 10:\n        new = n/10\n        return Adder(new)\n    else:\n        return None\n\ndef EvenNZ(n):\n    # Handle even n not divisible by 10, i.e. not ending in Zero\n    if n is not divisible by 10:\n        new = n/2\n        return Adder(new)\n    else:\n        return None  \n\ndef Adder(new):\n    # Add new to starting number, available as global variable n\n    return new + n\n</code></pre>"},{"location":"examples/agent-tree/#mapping-to-a-tree-structure","title":"Mapping to a tree structure","text":"<p>This compositional/nested computation can be represented as a tree:</p> <pre><code>       Main\n     /     \\\n  Even     Odd\n  /   \\        \\\nEvenZ  EvenNZ   Adder\n  |      |\n Adder  Adder\n</code></pre> <p>Let us specify the behavior we would like for each node, in a  \"decoupled\" way, i.e. we don't want a node to be aware of the other nodes. As we see later, this decoupled design maps very well onto Langroid's multi-agent task orchestration. To completely define the node behavior, we need to specify how it handles an \"incoming\" number \\(n\\) (from a parent node  or user), and how it handles a \"result\" number \\(r\\) (from a child node).</p> <ul> <li><code>Main</code>: <ul> <li>incoming \\(n\\): simply send down \\(n\\), record the starting number \\(n_0 = n\\) as a global variable. </li> <li>result \\(r\\): return \\(r\\).</li> </ul> </li> <li><code>Odd</code>: <ul> <li>incoming \\(n\\): if n is odd, send down \\(3*n+1\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>Even</code>: <ul> <li>incoming \\(n\\): if n is even, send down \\(n\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>EvenZ</code>: (guaranteed by the tree hierarchy, to receive an even number.)  <ul> <li>incoming \\(n\\): if n is divisible by 10, send down \\(n/10\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>EvenNZ</code>: (guaranteed by the tree hierarchy, to receive an even number.)<ul> <li>incoming \\(n\\): if n is not divisible by 10, send down \\(n/2\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>Adder</code>:<ul> <li>incoming \\(n\\): return \\(n + n_0\\) where \\(n_0\\) is the  starting number recorded by Main as a global variable.</li> <li>result \\(r\\): Not applicable since <code>Adder</code> is a leaf node.</li> </ul> </li> </ul>"},{"location":"examples/agent-tree/#from-tree-nodes-to-langroid-agents","title":"From tree nodes to Langroid Agents","text":"<p>Let us see how we can perform this calculation using multiple Langroid agents, where</p> <ul> <li>we define an agent corresponding to each of the nodes above, namely  <code>Main</code>, <code>Odd</code>, <code>Even</code>, <code>EvenZ</code>, <code>EvenNZ</code>, and <code>Adder</code>.</li> <li>we wrap each Agent into a Task, and use the <code>Task.add_subtask()</code> method to connect the agents into    the desired hierarchical structure.</li> </ul> <p>Below is one way to do this using Langroid. We designed this with the following desirable features:</p> <ul> <li> <p>Decoupling: Each agent is instructed separately, without mention of any other agents   (E.g. Even agent does not know about Odd Agent, EvenZ agent, etc).   In particular, this means agents will not be \"addressing\" their message   to specific other agents, e.g. send number to Odd agent when number is odd,   etc. Allowing addressing would make the solution easier to implement,   but would not be a decoupled solution.   Instead, we want Agents to simply put the number \"out there\", and have it handled   by an applicable agent, in the task loop (which consists of the agent's responders,   plus any sub-task <code>run</code> methods).</p> </li> <li> <p>Simplicity: Keep the agent instructions relatively simple. We would not want a solution   where we have to instruct the agents (their LLMs) in convoluted ways. </p> </li> </ul> <p>One way naive solutions fail is because agents are not able to distinguish between a number that is being \"sent down\" the tree as input, and a number that is being \"sent up\" the tree as a result from a child node.</p> <p>We use a simple trick: we instruct the LLM to mark returned values using the RESULT keyword, and instruct the LLMs on how to handle numbers that come with RESULT keyword, and those that don't In addition, we leverage some features of Langroid's task orchestration:</p> <ul> <li>When <code>llm_delegate</code> is <code>True</code>, if the LLM says <code>DONE [rest of msg]</code>, the task is   considered done, and the result of the task is <code>[rest of msg]</code> (i.e the part after <code>DONE</code>).</li> <li>In the task loop's <code>step()</code> function (which seeks a valid message during a turn of   the conversation) when any responder says <code>DO-NOT-KNOW</code>, it is not considered a valid   message, and the search continues to other responders, in round-robin fashion.</li> </ul> <p>See the <code>chat-tree.py</code> example for an implementation of this solution. You can run that example as follows: <pre><code>python3 examples/basic/chat-tree.py\n</code></pre> In the sections below we explain the code in more detail.</p>"},{"location":"examples/agent-tree/#define-the-agents","title":"Define the agents","text":"<p>Let us start with defining the configuration to be used by all agents:</p> <pre><code>from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nfrom langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n\nconfig = ChatAgentConfig(\n  llm=OpenAIGPTConfig(\n    chat_model=OpenAIChatModel.GPT4,\n  ),\n  vecdb=None, # no need for a vector database\n)\n</code></pre> <p>Next we define each of the agents, for example:</p> <pre><code>main_agent = ChatAgent(config)\n</code></pre> <p>and similarly for the other agents.</p>"},{"location":"examples/agent-tree/#wrap-each-agent-in-a-task","title":"Wrap each Agent in a Task","text":"<p>To allow agent interactions, the first step is to wrap each agent in a Task. When we define the task, we pass in the instructions above as part of the system message. Recall the instructions for the <code>Main</code> agent:</p> <ul> <li><code>Main</code>:<ul> <li>incoming \\(n\\): simply send down \\(n\\), record the starting number \\(n_0 = n\\) as a global variable.</li> <li>result \\(r\\): return \\(r\\).</li> </ul> </li> </ul> <p>We include the equivalent of these instructions in the <code>main_task</code> that wraps  the <code>main_agent</code>:</p> <pre><code>from langroid.agent.task import Task\n\nmain_task = Task(\n    main_agent,\n    name=\"Main\",\n    interactive=False, #(1)!\n    system_message=\"\"\"\n          You will receive two types of messages, to which you will respond as follows:\n\n          INPUT Message format: &lt;number&gt;\n          In this case simply write the &lt;number&gt;, say nothing else.\n\n          RESULT Message format: RESULT &lt;number&gt;\n          In this case simply say \"DONE &lt;number&gt;\", e.g.:\n          DONE 19\n\n          To start off, ask the user for the initial number, \n          using the `ask_num` tool/function.\n          \"\"\",\n    llm_delegate=True, # allow LLM to control end of task via DONE\n    single_round=False,\n)\n</code></pre> <ol> <li>Non-interactive: don't wait for user input in each turn </li> </ol> <p>There are a couple of points to highlight about the <code>system_message</code>  value in this task definition:</p> <ul> <li>When the <code>Main</code> agent receives just a number, it simply writes out that number,   and in the Langroid Task loop, this number becomes the \"current pending message\"   to be handled by one of the sub-tasks, i.e. <code>Even, Odd</code>. Note that these sub-tasks   are not mentioned in the system message, consistent with the decoupling principle.</li> <li>As soon as either of these sub-tasks returns a non-Null response, in the format \"RESULT \", the <code>Main</code> agent   is instructed to return this result saying \"DONE \". Since <code>llm_delegate</code>   is set to <code>True</code> (meaning the LLM can decide when the task has ended),    this causes the <code>Main</code> task to be considered finished and the task loop is exited. <p>Since we want the <code>Main</code> agent to record the initial number as a global variable, we use a tool/function <code>AskNum</code> defined as follows  (see this section in the getting started guide  for more details on Tools):</p> <pre><code>from rich.prompt import Prompt\nfrom langroid.agent.tool_message import ToolMessage\n\n\nclass AskNumTool(ToolMessage):\n  request = \"ask_num\"\n  purpose = \"Ask user for the initial number\"\n\n  def handle(self) -&gt; str:\n    \"\"\"\n    This is a stateless tool (i.e. does not use any Agent member vars), so we can\n    define the handler right here, instead of defining an `ask_num`\n    method in the agent.\n    \"\"\"\n    num = Prompt.ask(\"Enter a number\")\n    # record this in global state, so other agents can access it\n    MyGlobalState.set_values(number=num)\n    return str(num)\n</code></pre> <p>We then enable the <code>main_agent</code> to use and handle messages that conform to the  <code>AskNum</code> tool spec:</p> <pre><code>main_agent.enable_message(AskNumTool)\n</code></pre> <p>Using and Handling a tool/function</p> <p>\"Using\" a tool means the agent's LLM generates  the function-call (if using OpenAI function-calling) or  the JSON structure (if using Langroid's native tools mechanism)  corresponding to this tool. \"Handling\" a tool refers to the Agent's method  recognizing the tool and executing the corresponding code.</p> <p>The tasks for other agents are defined similarly. We will only note here that the <code>Adder</code> agent needs a special tool <code>AddNumTool</code> to be able to add the current number to the initial number set by the <code>Main</code> agent. </p>"},{"location":"examples/agent-tree/#connect-the-tasks-into-a-tree-structure","title":"Connect the tasks into a tree structure","text":"<p>So far, we have wrapped each agent in a task, in isolation, and there is no  connection between the tasks. The final step is to connect the tasks to  the tree structure we saw earlier:</p> <pre><code>main_task.add_sub_task([even_task, odd_task])\neven_task.add_sub_task([evenz_task, even_nz_task])\nevenz_task.add_sub_task(adder_task)\neven_nz_task.add_sub_task(adder_task)\nodd_task.add_sub_task(adder_task)\n</code></pre> <p>Now all that remains is to run the main task:</p> <pre><code>main_task.run()\n</code></pre> <p>Here is what a run starting with \\(n=12\\) looks like:</p> <p></p>"},{"location":"examples/guide/","title":"Guide to examples in <code>langroid-examples</code> repo","text":"<p>Outdated</p> <p>This guide is from Feb 2024; there have been numerous additional examples since then. We recommend you visit the <code>examples</code> folder in the core <code>langroid</code> repo for the most up-to-date examples. These examples are periodically copied over to the <code>examples</code> folder in the <code>langroid-examples</code> repo.</p> <p>The <code>langroid-examples</code> repo contains several examples of using the Langroid agent-oriented programming  framework for LLM applications. Below is a guide to the examples. First please ensure you follow the installation instructions in the <code>langroid-examples</code> repo README.</p> <p>At minimum a GPT4-compatible OpenAI API key is required. As currently set up, many of the examples will not work with a weaker model. Weaker models may require more detailed or different prompting, and possibly a more iterative approach with multiple agents to verify and retry, etc \u2014 this is on our roadmap.</p> <p>All the example scripts are meant to be run on the command line. In each script there is a description and sometimes instructions on how to run the script.</p> <p>NOTE: When you run any script, it pauses for \u201chuman\u201d input at every step, and depending on the context, you can either hit enter to continue, or in case there is a question/response expected from the human, you can enter your question or response and then hit enter.</p>"},{"location":"examples/guide/#basic-examples","title":"Basic Examples","text":"<ul> <li> <p><code>/examples/basic/chat.py</code> This is a basic chat application.</p> <ul> <li>Illustrates Agent task loop.</li> </ul> </li> <li> <p><code>/examples/basic/autocorrect.py</code> Chat with autocorrect: type fast and carelessly/lazily and  the LLM will try its best to interpret what you want, and offer choices when confused.</p> <ul> <li>Illustrates Agent task loop.</li> </ul> </li> <li> <p><code>/examples/basic/chat-search.py</code>  This uses a <code>GoogleSearchTool</code> function-call/tool to answer questions using a google web search if needed.   Try asking questions about facts known after Sep 2021 (GPT4 training cutoff),   like  <code>when was llama2 released</code></p> <ul> <li>Illustrates Agent + Tools/function-calling + web-search</li> </ul> </li> <li> <p><code>/examples/basic/chat-tree.py</code> is a toy example of tree-structured multi-agent   computation, see a detailed writeup here.</p> <ul> <li>Illustrates multi-agent task collaboration, task delegation.</li> </ul> </li> </ul>"},{"location":"examples/guide/#document-chat-examples-or-rag-retrieval-augmented-generation","title":"Document-chat examples, or RAG (Retrieval Augmented Generation)","text":"<ul> <li><code>/examples/docqa/chat.py</code> is a document-chat application. Point it to local file,   directory or web url, and ask questions<ul> <li>Illustrates basic RAG</li> </ul> </li> <li><code>/examples/docqa/chat-search.py</code>: ask about anything and it will try to answer   based on docs indexed in vector-db, otherwise it will do a Google search, and   index the results in the vec-db for this and later answers.<ul> <li>Illustrates RAG + Function-calling/tools</li> </ul> </li> <li><code>/examples/docqa/chat_multi.py</code>:  \u2014 this is a 2-agent system that will summarize   a large document with 5 bullet points: the first agent generates questions for   the retrieval agent, and is done when it gathers 5 key points.<ul> <li>Illustrates 2-agent collaboration + RAG to summarize a document</li> </ul> </li> <li><code>/examples/docqa/chat_multi_extract.py</code>:  \u2014 extracts structured info from a   lease document: Main agent asks questions to a retrieval agent. <ul> <li>Illustrates 2-agent collaboration, RAG, Function-calling/tools, Structured Information Extraction.</li> </ul> </li> </ul>"},{"location":"examples/guide/#data-chat-examples-tabular-sql","title":"Data-chat examples (tabular, SQL)","text":"<ul> <li><code>/examples/data-qa/table_chat.py</code>:  - point to a URL or local csv file and ask   questions. The agent generates pandas code that is run within langroid.<ul> <li>Illustrates function-calling/tools and code-generation</li> </ul> </li> <li><code>/examples/data-qa/sql-chat/sql_chat.py</code>:  \u2014 chat with a sql db \u2014 ask questions in   English, it will generate sql code to answer them.   See tutorial here<ul> <li>Illustrates function-calling/tools and code-generation</li> </ul> </li> </ul>"},{"location":"quick-start/","title":"Getting Started","text":"<p>In these sections we show you how to use the various components of <code>langroid</code>. To follow along, we recommend you clone the <code>langroid-examples</code> repo.</p> <p>Consult the tests as well</p> <p>As you get deeper into Langroid, you will find it useful to consult the tests folder under <code>tests/main</code> in the main Langroid repo.</p> <p>Start with the <code>Setup</code> section to install Langroid and get your environment set up.</p>"},{"location":"quick-start/chat-agent-docs/","title":"Augmenting Agents with Retrieval","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent-docs.py</code>.</p>"},{"location":"quick-start/chat-agent-docs/#why-is-this-important","title":"Why is this important?","text":"<p>Until now in this guide, agents have not used external data. Although LLMs already have enormous amounts of knowledge \"hard-wired\" into their weights during training (and this is after all why ChatGPT has exploded in popularity), for practical enterprise applications there are a few reasons it is critical to augment LLMs with access to specific, external documents:</p> <ul> <li>Private data: LLMs are trained on public data, but in many applications   we want to use private data that is not available to the public.   For example, a company may want to extract useful information from its private   knowledge-base.</li> <li>New data: LLMs are trained on data that was available at the time of training,   and so they may not be able to answer questions about new topics</li> <li>Constrained responses, or Grounding: LLMs are trained to generate text that is   consistent with the distribution of text in the training data.   However, in many applications we want to constrain the LLM's responses   to be consistent with the content of a specific document.   For example, if we want to use an LLM to generate a response to a customer   support ticket, we want the response to be consistent with the content of the ticket.   In other words, we want to reduce the chances that the LLM hallucinates   a response that is not consistent with the ticket.</li> </ul> <p>In all these scenarios, we want to augment the LLM with access to a specific set of documents, and use retrieval augmented generation (RAG) to generate more relevant, useful, accurate responses. Langroid provides a simple, flexible mechanism  RAG using vector-stores, thus ensuring grounded responses constrained to  specific documents. Another key feature of Langroid is that retrieval lineage  is maintained, and responses based on documents are always accompanied by source citations.</p>"},{"location":"quick-start/chat-agent-docs/#docchatagent-for-retrieval-augmented-generation","title":"<code>DocChatAgent</code> for Retrieval-Augmented Generation","text":"<p>Langroid provides a special type of agent called  <code>DocChatAgent</code>, which is a <code>ChatAgent</code> augmented with a vector-store, and some special methods that enable the agent to ingest documents into the vector-store, and answer queries based on these documents.</p> <p>The <code>DocChatAgent</code> provides many ways to ingest documents into the vector-store, including from URLs and local file-paths and URLs. Given a collection of document paths, ingesting their content into the vector-store involves the following steps:</p> <ol> <li>Split the document into shards (in a configurable way)</li> <li>Map each shard to an embedding vector using an embedding model. The default   embedding model is OpenAI's <code>text-embedding-ada-002</code> model, but users can    instead use <code>all-MiniLM-L6-v2</code> from HuggingFace <code>sentence-transformers</code> library.<sup>1</sup></li> <li>Store embedding vectors in the vector-store, along with the shard's content and    any document-level meta-data (this ensures Langroid knows which document a shard   came from when it retrieves it augment an LLM query)</li> </ol> <p><code>DocChatAgent</code>'s <code>llm_response</code> overrides the default <code>ChatAgent</code> method,  by augmenting the input message with relevant shards from the vector-store, along with instructions to the LLM to respond based on the shards.</p>"},{"location":"quick-start/chat-agent-docs/#define-some-documents","title":"Define some documents","text":"<p>Let us see how <code>DocChatAgent</code> helps with retrieval-agumented generation (RAG). For clarity, rather than ingest documents from paths or URLs, let us just set up some simple documents in the code itself,  using Langroid's <code>Document</code> class:</p> <pre><code>documents =[\n    lr.Document(\n        content=\"\"\"\n            In the year 2050, GPT10 was released. \n\n            In 2057, paperclips were seen all over the world. \n\n            Global warming was solved in 2060. \n\n            In 2061, the world was taken over by paperclips.         \n\n            In 2045, the Tour de France was still going on.\n            They were still using bicycles. \n\n            There was one more ice age in 2040.\n            \"\"\",\n        metadata=lr.DocMetaData(source=\"wikipedia-2063\"),\n    ),\n    lr.Document(\n        content=\"\"\"\n            We are living in an alternate universe \n            where Germany has occupied the USA, and the capital of USA is Berlin.\n\n            Charlie Chaplin was a great comedian.\n            In 2050, all Asian merged into Indonesia.\n            \"\"\",\n        metadata=lr.DocMetaData(source=\"Almanac\"),\n    ),\n]\n</code></pre> <p>There are two text documents. We will split them by double-newlines (<code>\\n\\n</code>), as we see below.</p>"},{"location":"quick-start/chat-agent-docs/#configure-the-docchatagent-and-ingest-documents","title":"Configure the DocChatAgent and ingest documents","text":"<p>Following the pattern in Langroid, we first set up a <code>DocChatAgentConfig</code> object and then instantiate a <code>DocChatAgent</code> from it.</p> <pre><code>from langroid.agent.special import DocChatAgent, DocChatAgentConfig\n\nconfig = DocChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    vecdb=lr.vector_store.QdrantDBConfig(\n        collection_name=\"quick-start-chat-agent-docs\",\n        replace_collection=True, #(1)!\n    ),\n    parsing=lr.parsing.parser.ParsingConfig(\n        separators=[\"\\n\\n\"],\n        splitter=lr.parsing.parser.Splitter.SIMPLE, #(2)!\n        n_similar_docs=2, #(3)!\n    )\n)\nagent = DocChatAgent(config)\n</code></pre> <ol> <li>Specifies that each time we run the code, we create a fresh collection,  rather than re-use the existing one with the same name.</li> <li>Specifies to split all text content by the first separator in the <code>separators</code> list</li> <li>Specifies that, for a query,    we want to retrieve at most 2 similar shards from the vector-store</li> </ol> <p>Now that the <code>DocChatAgent</code> is configured, we can ingest the documents  into the vector-store:</p> <pre><code>agent.ingest_docs(documents)\n</code></pre>"},{"location":"quick-start/chat-agent-docs/#setup-the-task-and-run-it","title":"Setup the task and run it","text":"<p>As before, all that remains is to set up the task and run it:</p> <pre><code>task = lr.Task(agent)\ntask.run()\n</code></pre> <p>And that is all there is to it! Feel free to try out the  <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repository.</p> <p>Here is a screenshot of the output:</p> <p></p> <p>Notice how follow-up questions correctly take the preceding dialog into account, and every answer is accompanied by a source citation.</p>"},{"location":"quick-start/chat-agent-docs/#answer-questions-from-a-set-of-urls","title":"Answer questions from a set of URLs","text":"<p>Instead of having in-code documents as above, what if you had a set of URLs instead -- how do you use Langroid to answer questions based on the content  of those URLS?</p> <p><code>DocChatAgent</code> makes it very simple to do this.  First include the URLs in the <code>DocChatAgentConfig</code> object:</p> <pre><code>config = DocChatAgentConfig(\n  doc_paths = [\n    \"https://cthiriet.com/articles/scaling-laws\",\n    \"https://www.jasonwei.net/blog/emergence\",\n  ]\n)\n</code></pre> <p>Then, call the <code>ingest()</code> method of the <code>DocChatAgent</code> object:</p> <p><pre><code>agent.ingest()\n</code></pre> And the rest of the code remains the same.</p>"},{"location":"quick-start/chat-agent-docs/#see-also","title":"See also","text":"<p>In the <code>langroid-examples</code> repository, you can find full working examples of document question-answering:</p> <ul> <li><code>examples/docqa/chat.py</code>   an app that takes a list of URLs or document paths from a user, and answers questions on them.</li> <li><code>examples/docqa/chat_multi.py</code>   a two-agent app where the <code>WriterAgent</code> is tasked with writing 5 key points about a topic,    and takes the help of a <code>DocAgent</code> that answers its questions based on a given set of documents.</li> </ul>"},{"location":"quick-start/chat-agent-docs/#next-steps","title":"Next steps","text":"<p>This Getting Started guide walked you through the core features of Langroid. If you want to see full working examples combining these elements,  have a look at the  <code>examples</code> folder in the <code>langroid-examples</code> repo. </p> <ol> <li> <p>To use this embedding model, install langroid via <code>pip install langroid[hf-embeddings]</code> Note that this will install <code>torch</code> and <code>sentence-transfoemers</code> libraries.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/chat-agent-tool/","title":"A chat agent, equipped with a tool/function-call","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is   in the <code>chat-agent-tool.py</code> script in the <code>langroid-examples</code> repo:   <code>examples/quick-start/chat-agent-tool.py</code>.</p>"},{"location":"quick-start/chat-agent-tool/#tools-plugins-function-calling","title":"Tools, plugins, function-calling","text":"<p>An LLM normally generates unstructured text in response to a prompt (or sequence of prompts). However there are many situations where we would like the LLM to generate structured text, or even code, that can be handled by specialized functions outside the LLM, for further processing.  In these situations, we want the LLM to \"express\" its \"intent\" unambiguously, and we achieve this by instructing the LLM on how to format its output (typically in JSON) and under what conditions it should generate such output. This mechanism has become known by various names over the last few months (tools, plugins, or function-calling), and is extremely useful in numerous scenarios, such as:</p> <ul> <li>Extracting structured information from a document: for example, we can use  the tool/functions mechanism to have the LLM present the key terms in a lease document in a JSON structured format, to simplify further processing.  See an example of this in the <code>langroid-examples</code> repo. </li> <li>Specialized computation: the LLM can request a units conversion,  or request scanning a large file (which wouldn't fit into its context) for a specific pattern.</li> <li>Code execution: the LLM can generate code that is executed in a sandboxed environment, and the results of the execution are returned to the LLM.</li> <li>API Calls: the LLM can generate a JSON containing params for an API call,   which the tool handler uses to make the call and return the results to the LLM.</li> </ul> <p>For LLM developers, Langroid provides a clean, uniform interface for the recently released OpenAI Function-calling as well Langroid's own native \"tools\" mechanism. The native tools mechanism is meant to be used when working with non-OpenAI LLMs that do not have a \"native\" function-calling facility. You can choose which to enable by setting the  <code>use_tools</code> and <code>use_functions_api</code> flags in the <code>ChatAgentConfig</code> object. (Or you can omit setting these, and langroid auto-selects the best mode depending on the LLM). The implementation leverages the excellent  Pydantic library. Benefits of using Pydantic are that you never have to write complex JSON specs  for function calling, and when the LLM hallucinates malformed JSON,  the Pydantic error message is sent back to the LLM so it can fix it!</p>"},{"location":"quick-start/chat-agent-tool/#example-find-the-smallest-number-in-a-list","title":"Example: find the smallest number in a list","text":"<p>Again we will use a simple number-game as a toy example to quickly and succinctly illustrate the ideas without spending too much on token costs.  This is a modification of the <code>chat-agent.py</code> example we saw in an earlier section. The idea of this single-agent game is that the agent has in \"mind\" a list of numbers between 1 and 100, and the LLM has to find out the smallest number from this list. The LLM has access to a <code>probe</code> tool  (think of it as a function) that takes an argument <code>number</code>. When the LLM  \"uses\" this tool (i.e. outputs a message in the format required by the tool), the agent handles this structured message and responds with  the number of values in its list that are at most equal to the <code>number</code> argument. </p>"},{"location":"quick-start/chat-agent-tool/#define-the-tool-as-a-toolmessage","title":"Define the tool as a <code>ToolMessage</code>","text":"<p>The first step is to define the tool, which we call <code>ProbeTool</code>, as an instance of the <code>ToolMessage</code> class, which is itself derived from Pydantic's <code>BaseModel</code>. Essentially the <code>ProbeTool</code> definition specifies </p> <ul> <li>the name of the Agent method that handles the tool, in this case <code>probe</code></li> <li>the fields that must be included in the tool message, in this case <code>number</code></li> <li>the \"purpose\" of the tool, i.e. under what conditions it should be used, and what it does</li> </ul> <p>Here is what the <code>ProbeTool</code> definition looks like: <pre><code>class ProbeTool(lr.agent.ToolMessage):\n    request: str = \"probe\" #(1)!\n    purpose: str = \"\"\" \n        To find which number in my list is closest to the &lt;number&gt; you specify\n        \"\"\" #(2)!\n    number: int #(3)!\n\n    @classmethod\n    def examples(cls): #(4)!\n        # Compiled to few-shot examples sent along with the tool instructions.\n        return [\n            cls(number=10),\n            (\n                \"To find which number is closest to 20\",\n                cls(number=20),\n            )\n        ]\n</code></pre></p> <ol> <li>This indicates that the agent's <code>probe</code> method will handle this tool-message.</li> <li>The <code>purpose</code> is used behind the scenes to instruct the LLM</li> <li><code>number</code> is a required argument of the tool-message (function)</li> <li>You can optionally include a class method that returns a list containing examples,     of two types: either a class instance, or a tuple consisting of a description and a     class instance, where the description is the \"thought\" that leads the LLM to use the    tool. In some scenarios this can help with LLM tool-generation accuracy.</li> </ol> <p>Stateless tool handlers</p> <p>The above <code>ProbeTool</code> is \"stateful\", i.e. it requires access to a variable in   the Agent instance (the <code>numbers</code> variable). This is why handling this    tool-message requires subclassing the <code>ChatAgent</code> and defining a special method    in the Agent, with a name matching the value of the <code>request</code> field of the Tool    (<code>probe</code> in this case). However you may often define \"stateless tools\" which    don't require access to the Agent's state. For such tools, you can define a    handler method right in the <code>ToolMessage</code> itself, with a name <code>handle</code>. Langroid    looks for such a method in the <code>ToolMessage</code> and automatically inserts it into    the Agent as a method with name matching the <code>request</code> field of the Tool. Examples of   stateless tools include tools for numerical computation    (e.g., in this example),   or API calls (e.g. for internet search, see    DuckDuckGoSearch Tool).</p>"},{"location":"quick-start/chat-agent-tool/#define-the-chatagent-with-the-probe-method","title":"Define the ChatAgent, with the <code>probe</code> method","text":"<p>As before we first create a <code>ChatAgentConfig</code> object:</p> <pre><code>config = lr.ChatAgentConfig(\n    name=\"Spy\",\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    use_tools=True, #(1)!\n    use_functions_api=False, #(2)!\n    vecdb=None,\n)\n</code></pre> <ol> <li>whether to use langroid's native tools mechanism</li> <li>whether to use OpenAI's function-calling mechanism</li> </ol> <p>Next we define the Agent class itself, which we call <code>SpyGameAgent</code>, with a member variable to hold its \"secret\" list of numbers. We also add <code>probe</code> method (to handle the <code>ProbeTool</code> message) to this class, and instantiate it:</p> <pre><code>class SpyGameAgent(lr.ChatAgent):\n    def __init__(self, config: lr.ChatAgentConfig):\n        super().__init__(config)\n        self.numbers = [3, 4, 8, 11, 15, 25, 40, 80, 90]\n\n    def probe(self, msg: ProbeTool) -&gt; str: #(1)!\n        # return how many values in self.numbers are less or equal to msg.number\n        return str(len([n for n in self.numbers if n &lt;= msg.number]))\n\nspy_game_agent = SpyGameAgent(config)\n</code></pre> <ol> <li>Note that this method name exactly matches the value of the <code>request</code> field in the     <code>ProbeTool</code> definition. This ensures that this method is called when the LLM     generates a valid <code>ProbeTool</code> message.</li> </ol>"},{"location":"quick-start/chat-agent-tool/#enable-the-spy_game_agent-to-handle-the-probe-tool","title":"Enable the <code>spy_game_agent</code> to handle the <code>probe</code> tool","text":"<p>The final step in setting up the tool is to enable  the <code>spy_game_agent</code> to handle the <code>probe</code> tool:</p> <pre><code>spy_game_agent.enable_message(ProbeTool)\n</code></pre>"},{"location":"quick-start/chat-agent-tool/#set-up-the-task-and-instructions","title":"Set up the task and instructions","text":"<p>We set up the task for the <code>spy_game_agent</code> and run it:</p> <p><pre><code>task = lr.Task(\n   spy_game_agent,\n   system_message=\"\"\"\n            I have a list of numbers between 1 and 100. \n            Your job is to find the smallest of them.\n            To help with this, you can give me a number and I will\n            tell you how many of my numbers are equal or less than your number.\n            Once you have found the smallest number,\n            you can say DONE and report your answer.\n        \"\"\"\n)\ntask.run()\n</code></pre> Notice that in the task setup we  have not explicitly instructed the LLM to use the <code>probe</code> tool. But this is done \"behind the scenes\", either by the OpenAI API  (when we use function-calling by setting the <code>use_functions_api</code> flag to <code>True</code>), or by Langroid's native tools mechanism (when we set the <code>use_tools</code> flag to <code>True</code>).</p> <p>See the <code>chat-agent-tool.py</code> in the <code>langroid-examples</code> repo, for a working example that you can run as follows: <pre><code>python3 examples/quick-start/chat-agent-tool.py\n</code></pre></p> <p>Here is a screenshot of the chat in action, using Langroid's tools mechanism</p> <p></p> <p>And if we run it with the <code>-f</code> flag (to switch to using OpenAI function-calling):</p> <p></p>"},{"location":"quick-start/chat-agent-tool/#see-also","title":"See also","text":"<p>One of the uses of tools/function-calling is to extract structured information from  a document. In the <code>langroid-examples</code> repo, there are two examples of this: </p> <ul> <li><code>examples/extract/chat.py</code>,    which shows how to extract Machine Learning model quality information from a description of    a solution approach on Kaggle.</li> <li><code>examples/docqa/chat_multi_extract.py</code>   which extracts key terms from a commercial lease document, in a nested JSON format.</li> </ul>"},{"location":"quick-start/chat-agent-tool/#next-steps","title":"Next steps","text":"<p>In the 3-agent chat example, recall that the <code>processor_agent</code> did not have to bother with specifying who should handle the current number. In the next section we add a twist to this game, so that the <code>processor_agent</code> has to decide who should handle the current number.</p>"},{"location":"quick-start/chat-agent/","title":"A simple chat agent","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent.py</code>.</p>"},{"location":"quick-start/chat-agent/#agents","title":"Agents","text":"<p>A <code>ChatAgent</code> is an abstraction that  wraps a few components, including:</p> <ul> <li>an LLM (<code>ChatAgent.llm</code>), possibly equipped with tools/function-calling.    The <code>ChatAgent</code> class maintains LLM conversation history.</li> <li>optionally a vector-database (<code>ChatAgent.vecdb</code>)</li> </ul>"},{"location":"quick-start/chat-agent/#agents-as-message-transformers","title":"Agents as message transformers","text":"<p>In Langroid, a core function of <code>ChatAgents</code> is message transformation. There are three special message transformation methods, which we call responders. Each of these takes a message and returns a message.  More specifically, their function signature is (simplified somewhat): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> where <code>ChatDocument</code> is a class that wraps a message content (text) and its metadata. There are three responder methods in <code>ChatAgent</code>, one corresponding to each  responding entity (<code>LLM</code>, <code>USER</code>, or <code>AGENT</code>):</p> <ul> <li><code>llm_response</code>: returns the LLM response to the input message.   (The input message is added to the LLM history, and so is the subsequent response.)</li> <li><code>agent_response</code>: a method that can be used to implement a custom agent response.     Typically, an <code>agent_response</code> is used to handle messages containing a     \"tool\" or \"function-calling\" (more on this later). Another use of <code>agent_response</code>     is message validation.</li> <li><code>user_response</code>: get input from the user. Useful to allow a human user to     intervene or quit.</li> </ul> <p>Creating an agent is easy. First define a <code>ChatAgentConfig</code> object, and then instantiate a <code>ChatAgent</code> object with that config: <pre><code>import langroid as lr\n\nconfig = lr.ChatAgentConfig( #(1)!\n    name=\"MyAgent\", # note there should be no spaces in the name!\n    llm = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    system_message=\"You are a helpful assistant\" #(2)! \n    )\n)\nagent = lr.ChatAgent(config)\n</code></pre></p> <ol> <li>This agent only has an LLM, and no vector-store. Examples of agents with    vector-stores will be shown later.</li> <li>The <code>system_message</code> is used when invoking the agent's <code>llm_response</code> method; it is     passed to the LLM API as the first message (with role <code>\"system\"</code>), followed by the alternating series of user,     assistant messages. Note that a <code>system_message</code> can also be specified when initializing a <code>Task</code> object (as seen     below); in this case the <code>Task</code> <code>system_message</code> overrides the agent's <code>system_message</code>.</li> </ol> <p>We can now use the agent's responder methods, for example: <pre><code>response = agent.llm_response(\"What is 2 + 4?\")\nif response is not None:\n    print(response.content)\nresponse = agent.user_response(\"add 3 to this\")\n...\n</code></pre> The <code>ChatAgent</code> conveniently accumulates message history so you don't have to, as you did in the previous section with direct LLM usage. However to create an interative loop involving the human user, you still  need to write your own. The <code>Task</code> abstraction frees you from this, as we see below.</p>"},{"location":"quick-start/chat-agent/#task-orchestrator-for-agents","title":"Task: orchestrator for agents","text":"<p>In order to do anything useful with a <code>ChatAgent</code>, we need to have a way to  sequentially invoke its responder methods, in a principled way. For example in the simple chat loop we saw in the  previous section, in the  <code>try-llm.py</code> script, we had a loop that alternated between getting a human input and an LLM response. This is one of the simplest possible loops, but in more complex applications,  we need a general way to orchestrate the agent's responder methods.</p> <p>The <code>Task</code> class is an abstraction around a  <code>ChatAgent</code>, responsible for iterating over the agent's responder methods, as well as orchestrating delegation and hand-offs among multiple tasks. A <code>Task</code> is initialized with a specific <code>ChatAgent</code> instance, and some  optional arguments, including an initial message to \"kick-off\" the agent. The <code>Task.run()</code> method is the main entry point for <code>Task</code> objects, and works  as follows:</p> <ul> <li>it first calls the <code>Task.init()</code> method to initialize the <code>pending_message</code>,    which represents the latest message that needs a response.</li> <li>it then repeatedly calls <code>Task.step()</code> until <code>Task.done()</code> is True, and returns   <code>Task.result()</code> as the final result of the task.</li> </ul> <p><code>Task.step()</code> is where all the action happens. It represents a \"turn\" in the  \"conversation\": in the case of a single <code>ChatAgent</code>, the conversation involves  only the three responders mentioned above, but when a <code>Task</code> has sub-tasks,  it can involve other tasks well  (we see this in the a later section but ignore this for now).  <code>Task.step()</code> loops over  the <code>ChatAgent</code>'s responders (plus sub-tasks if any) until it finds a valid  response<sup>1</sup> to the current <code>pending_message</code>, i.e. a \"meaningful\" response,  something other than <code>None</code> for example. Once <code>Task.step()</code> finds a valid response, it updates the <code>pending_message</code>  with this response, and the next invocation of <code>Task.step()</code> will search for a valid response to this  updated message, and so on. <code>Task.step()</code> incorporates mechanisms to ensure proper handling of messages, e.g. the USER gets a chance to respond after each non-USER response (to avoid infinite runs without human intervention), and preventing an entity from responding if it has just responded, etc.</p> <p><code>Task.run()</code> has the same signature as agent's responder methods.</p> <p>The key to composability of tasks is that <code>Task.run()</code> has exactly the same type-signature as any of the agent's responder methods,  i.e. <code>str | ChatDocument -&gt; ChatDocument</code>. This means that a <code>Task</code> can be used as a responder in another <code>Task</code>, and so on recursively.  We will see this in action in the Two Agent Chat section.</p> <p>The above details were only provided to give you a glimpse into how Agents and  Tasks work. Unless you are creating a custom orchestration mechanism, you do not need to be aware of these details. In fact our basic human + LLM chat loop can be trivially  implemented with a <code>Task</code>, in a couple of lines of code: <pre><code>task = lr.Task(\n    agent, name=\"Bot\", \n    system_message=\"You are a helpful assistant\", #(1)!\n)\n</code></pre></p> <ol> <li>Overrides the agent's <code>system_message</code></li> </ol> <p>(When a <code>name</code> is provided in the <code>Task</code> constructor, it overrides the agent's name.) </p> <p>We can then run the task: <pre><code>task.run() #(1)!\n</code></pre></p> <ol> <li>Note how this hides all of the complexity of constructing and updating a     sequence of <code>LLMMessages</code></li> </ol> <p>Note that the agent's <code>agent_response()</code> method always returns <code>None</code> (since the default  implementation of this method looks for a tool/function-call, and these never occur in this task). So the calls to <code>task.step()</code> result in alternating responses from the LLM and the user.</p> <p>See <code>chat-agent.py</code> for a working example that you can run with <pre><code>python3 examples/quick-start/chat-agent.py\n</code></pre></p> <p>Here is a screenshot of the chat in action:<sup>2</sup></p> <p></p>"},{"location":"quick-start/chat-agent/#next-steps","title":"Next steps","text":"<p>In the next section you will  learn some general principles on how to have multiple agents collaborate  on a task using Langroid.</p> <ol> <li> <p>To customize a Task's behavior you can subclass it and  override methods like <code>valid()</code>, <code>done()</code>, <code>result()</code>, or even <code>step()</code>.\u00a0\u21a9</p> </li> <li> <p>In the screenshot, the numbers in parentheses indicate how many  messages have accumulated in the LLM's message history.  This is only provided for informational and debugging purposes, and  you can ignore it for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/llm-interaction/","title":"LLM interaction","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is  in the <code>try-llm.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/try-llm.py</code>.</p> <p>Let's start with the basics -- how to directly interact with an OpenAI LLM using Langroid.</p>"},{"location":"quick-start/llm-interaction/#configure-instantiate-the-llm-class","title":"Configure, instantiate the LLM class","text":"<p>First define the configuration for the LLM, in this case one of the OpenAI GPT chat models: <pre><code>import langroid as lr\n\ncfg = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4,\n)\n</code></pre></p> <p>About Configs</p> <p>A recurring pattern you will see in Langroid is that for many classes, we have a corresponding <code>Config</code> class (an instance of a Pydantic <code>BaseModel</code>), and the class constructor takes this <code>Config</code> class as its only argument. This lets us avoid having long argument lists in constructors, and brings flexibility since adding a new argument to the constructor is as simple as adding a new field to the corresponding <code>Config</code> class. For example the constructor for the <code>OpenAIGPT</code> class takes a single argument, an instance of the <code>OpenAIGPTConfig</code> class.</p> <p>Now that we've defined the configuration of the LLM, we can instantiate it: <pre><code>mdl = lr.language_models.OpenAIGPT(cfg)\n</code></pre></p> <p>We will use OpenAI's GPT4 model's chat completion API.</p>"},{"location":"quick-start/llm-interaction/#messages-the-llmmessage-class","title":"Messages: The <code>LLMMessage</code> class","text":"<p>This API takes a list of \"messages\" as input -- this is typically the conversation history so far, consisting of an initial system message, followed by a sequence of alternating messages from the LLM (\"Assistant\") and the user. Langroid provides an abstraction  <code>LLMMessage</code> to construct messages, e.g. <pre><code>from langroid.language_models import Role, LLMMessage\n\nmsg = LLMMessage(\n    content=\"what is the capital of Bangladesh?\", \n    role=Role.USER\n)\n</code></pre></p>"},{"location":"quick-start/llm-interaction/#llm-response-to-a-sequence-of-messages","title":"LLM response to a sequence of messages","text":"<p>To get a response from the LLM, we call the mdl's <code>chat</code> method, and pass in a list of messages, along with a bound on how long (in tokens) we want the response to be: <pre><code>messages = [\n    LLMMessage(content=\"You are a helpful assistant\", role=Role.SYSTEM), #(1)!\n    LLMMessage(content=\"What is the capital of Ontario?\", role=Role.USER), #(2)!\n]\n\nresponse = mdl.chat(messages, max_tokens=200)\n</code></pre></p> <ol> <li> With a system message, you can assign a \"role\" to the LLM</li> <li> Responses from the LLM will have role <code>Role.ASSISTANT</code>;    this is done behind the scenes by the <code>response.to_LLMMessage()</code> call below.</li> </ol> <p>The response is an object of class <code>LLMResponse</code>,  which we can convert to an <code>LLMMessage</code> to append to the conversation history: <pre><code>messages.append(response.to_LLMMessage())\n</code></pre></p> <p>You can put the above in a simple loop,  to get a simple command-line chat interface!</p> <pre><code>from rich import print\nfrom rich.prompt import Prompt #(1)!\n\nmessages = [\n    LLMMessage(role=Role.SYSTEM, content=\"You are a helpful assitant\"),\n]\n\nwhile True:\n    message = Prompt.ask(\"[blue]Human\")\n    if message in [\"x\", \"q\"]:\n        print(\"[magenta]Bye!\")\n        break\n    messages.append(LLMMessage(role=Role.USER, content=message))\n\n    response = mdl.chat(messages=messages, max_tokens=200)\n    messages.append(response.to_LLMMessage())\n    print(\"[green]Bot: \" + response.message)\n</code></pre> <ol> <li>Rich is a Python library for rich text and beautiful formatting in the terminal.    We use it here to get a nice prompt for the user's input.    You can install it with <code>pip install rich</code>.</li> </ol> <p>See <code>examples/quick-start/try-llm.py</code> for a complete example that you can run using <pre><code>python3 examples/quick-start/try-llm.py\n</code></pre></p> <p>Here is a screenshot of what it looks like:</p> <p></p>"},{"location":"quick-start/llm-interaction/#next-steps","title":"Next steps","text":"<p>You might be thinking:  \"It is tedious to keep track of the LLM conversation history and set up a  loop. Does Langroid provide any abstractions to make this easier?\"</p> <p>We're glad you asked! And this leads to the notion of an <code>Agent</code>.  The next section will show you how to use the <code>ChatAgent</code> class  to set up a simple chat Agent in a couple of lines of code.</p>"},{"location":"quick-start/multi-agent-task-delegation/","title":"Multi-Agent collaboration via Task Delegation","text":""},{"location":"quick-start/multi-agent-task-delegation/#why-multiple-agents","title":"Why multiple agents?","text":"<p>Let's say we want to develop a complex LLM-based application, for example an application that reads a legal contract, extracts structured information, cross-checks it against some taxonomoy, gets some human input, and produces clear summaries. In theory it may be possible to solve this in a monolithic architecture using an LLM API and a vector-store. But this approach quickly runs into problems -- you would need to maintain multiple LLM conversation histories and states, multiple vector-store instances, and coordinate all of the interactions between them.</p> <p>Langroid's <code>ChatAgent</code> and <code>Task</code> abstractions provide a natural and intuitive way to decompose a solution approach into multiple tasks, each requiring different skills and capabilities. Some of these tasks may need access to an LLM, others may need access to a vector-store, and yet others may need tools/plugins/function-calling capabilities, or any combination of these. It may also make sense to have some tasks that manage the overall solution process. From an architectural perspective, this type of modularity has numerous benefits:</p> <ul> <li>Reusability: We can reuse the same agent/task in other contexts,</li> <li>Scalability: We can scale up the solution by adding more agents/tasks,</li> <li>Flexibility: We can easily change the solution by adding/removing agents/tasks.</li> <li>Maintainability: We can maintain the solution by updating individual agents/tasks.</li> <li>Testability: We can test/debug individual agents/tasks in isolation.</li> <li>Composability: We can compose agents/tasks to create new agents/tasks.</li> <li>Extensibility: We can extend the solution by adding new agents/tasks.</li> <li>Interoperability: We can integrate the solution with other systems by   adding new agents/tasks.</li> <li>Security/Privacy: We can secure the solution by isolating sensitive agents/tasks.</li> <li>Performance: We can improve performance by isolating performance-critical agents/tasks.</li> </ul>"},{"location":"quick-start/multi-agent-task-delegation/#task-collaboration-via-sub-tasks","title":"Task collaboration via sub-tasks","text":"<p>Langroid currently provides a mechanism for hierarchical (i.e. tree-structured) task delegation: a <code>Task</code> object can add other <code>Task</code> objects as sub-tasks, as shown in this pattern:</p> <pre><code>from langroid import ChatAgent, ChatAgentConfig, Task\n\nmain_agent = ChatAgent(ChatAgentConfig(...))\nmain_task = Task(main_agent, ...)\n\nhelper_agent1 = ChatAgent(ChatAgentConfig(...))\nhelper_agent2 = ChatAgent(ChatAgentConfig(...))\nhelper_task1 = Task(agent1, ...)\nhelper_task2 = Task(agent2, ...)\n\nmain_task.add_sub_task([helper_task1, helper_task2])\n</code></pre> <p>What happens when we call <code>main_task.run()</code>? Recall from the previous section that <code>Task.run()</code> works by repeatedly calling <code>Task.step()</code> until <code>Task.done()</code> is True. When the <code>Task</code> object has no sub-tasks, <code>Task.step()</code> simply tries to get a valid response from the <code>Task</code>'s <code>ChatAgent</code>'s \"native\" responders, in this sequence: <pre><code>[self.agent_response, self.llm_response, self.user_response] #(1)!\n</code></pre></p> <ol> <li>This is the default sequence in Langroid, but it can be changed by    overriding <code>ChatAgent.entity_responders()</code></li> </ol> <p>When a <code>Task</code> object has subtasks, the sequence of responders tried by <code>Task.step()</code> consists of the above \"native\" responders, plus the sequence of <code>Task.run()</code> calls on the sub-tasks, in the order in which they were added to the <code>Task</code> object. For the example above, this means that <code>main_task.step()</code> will seek a valid response in this sequence:</p> <p><pre><code>[self.agent_response, self.llm_response, self.user_response, \n    helper_task1.run(), helper_task2.run()]\n</code></pre> Fortunately, as noted in the previous section, <code>Task.run()</code> has the same type signature as that of the <code>ChatAgent</code>'s \"native\" responders, so this works seamlessly. Of course, each of the sub-tasks can have its own sub-tasks, and so on, recursively. One way to think of this type of task delegation is that <code>main_task()</code> \"fails-over\" to <code>helper_task1()</code> and <code>helper_task2()</code> when it cannot respond to the current <code>pending_message</code> on its own.</p>"},{"location":"quick-start/multi-agent-task-delegation/#or-else-logic-vs-and-then-logic","title":"Or Else logic vs And Then logic","text":"<p>It is important to keep in mind how <code>step()</code> works: As each responder  in the sequence is tried, when there is a valid response, the  next call to <code>step()</code> restarts its search at the beginning of the sequence (with the only exception being that the human User is given a chance  to respond after each non-human response).  In this sense, the semantics of the responder sequence is similar to OR Else logic, as opposed to AND Then logic.</p> <p>If we want to have a sequence of sub-tasks that is more like AND Then logic, we can achieve this by recursively adding subtasks. In the above example suppose we wanted the <code>main_task</code>  to trigger <code>helper_task1</code> and <code>helper_task2</code> in sequence, then we could set it up like this:</p> <pre><code>helper_task1.add_sub_task(helper_task2) #(1)!\nmain_task.add_sub_task(helper_task1)\n</code></pre> <ol> <li>When adding a single sub-task, we do not need to wrap it in a list.</li> </ol>"},{"location":"quick-start/multi-agent-task-delegation/#next-steps","title":"Next steps","text":"<p>In the next section we will see how this mechanism  can be used to set up a simple collaboration between two agents.</p>"},{"location":"quick-start/setup/","title":"Setup","text":""},{"location":"quick-start/setup/#install","title":"Install","text":"<p>Ensure you are using Python 3.11. It is best to work in a virtual environment:</p> <p><pre><code># go to your repo root (which may be langroid-examples)\ncd &lt;your repo root&gt;\npython3 -m venv .venv\n. ./.venv/bin/activate\n</code></pre> To see how to use Langroid in your own repo, you can take a look at the <code>langroid-examples</code> repo, which can be a good starting point for your own repo. The <code>langroid-examples</code> repo already contains a <code>pyproject.toml</code> file so that you can  use <code>Poetry</code> to manage your virtual environment and dependencies.  For example you can do  <pre><code>poetry install # installs latest version of langroid\n</code></pre> Alternatively, use <code>pip</code> to install <code>langroid</code> into your virtual environment: <pre><code>pip install langroid\n</code></pre></p> <p>The core Langroid package lets you use OpenAI Embeddings models via their API. If you instead want to use the <code>sentence-transformers</code> embedding models from HuggingFace, install Langroid like this: <pre><code>pip install \"langroid[hf-embeddings]\"\n</code></pre> For many practical scenarios, you may need additional optional dependencies: - To use various document-parsers, install langroid with the <code>doc-chat</code> extra:     <pre><code>pip install \"langroid[doc-chat]\"\n</code></pre> - For \"chat with databases\", use the <code>db</code> extra:     <code>`bash     pip install \"langroid[db]\"</code> - You can specify multiple extras by separating them with commas, e.g.:     <pre><code>pip install \"langroid[doc-chat,db]\"\n</code></pre> - To simply install all optional dependencies, use the <code>all</code> extra (but note that this will result in longer load/startup times and a larger install size):     <pre><code>pip install \"langroid[all]\"\n</code></pre></p> Optional Installs for using SQL Chat with a PostgreSQL DB <p>If you are using <code>SQLChatAgent</code> (e.g. the script <code>examples/data-qa/sql-chat/sql_chat.py</code>, with a postgres db, you will need to:</p> <ul> <li>Install PostgreSQL dev libraries for your platform, e.g.<ul> <li><code>sudo apt-get install libpq-dev</code> on Ubuntu,</li> <li><code>brew install postgresql</code> on Mac, etc.</li> </ul> </li> <li>Install langroid with the postgres extra, e.g. <code>pip install langroid[postgres]</code>   or <code>poetry add langroid[postgres]</code> or <code>poetry install -E postgres</code>.   If this gives you an error, try <code>pip install psycopg2-binary</code> in your virtualenv.</li> </ul> <p>Work in a nice terminal, such as Iterm2, rather than a notebook</p> <p>All of the examples we will go through are command-line applications. For the best experience we recommend you work in a nice terminal that supports  colored outputs, such as Iterm2.    </p> <p>OpenAI GPT-4/GPT-4o is required</p> <p>The various LLM prompts and instructions in Langroid  have been tested to work well with GPT-4 (and to some extent GPT-4o). Switching to other LLMs (local/open and proprietary) is easy (see guides mentioned below), and may suffice for some applications, but in general you may see inferior results unless you adjust the prompts and/or the multi-agent setup.</p>"},{"location":"quick-start/setup/#set-up-tokenskeys","title":"Set up tokens/keys","text":"<p>To get started, all you need is an OpenAI API Key. If you don't have one, see this OpenAI Page. (Note that while this is the simplest way to get started, Langroid works with practically any LLM, not just those from OpenAI. See the guides to using Open/Local LLMs, and other non-OpenAI proprietary LLMs.)</p> <p>In the root of the repo, copy the <code>.env-template</code> file to a new file <code>.env</code>: <pre><code>cp .env-template .env\n</code></pre> Then insert your OpenAI API Key. Your <code>.env</code> file should look like this: <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\n</code></pre></p> <p>Alternatively, you can set this as an environment variable in your shell (you will need to do this every time you open a new shell): <pre><code>export OPENAI_API_KEY=your-key-here-without-quotes\n</code></pre></p> <p>All of the following environment variable settings are optional, and some are only needed to use specific features (as noted below).</p> <ul> <li>Qdrant Vector Store API Key, URL. This is only required if you want to use Qdrant cloud.   Langroid uses LanceDB as the default vector store in its <code>DocChatAgent</code> class (for RAG).   Alternatively Chroma is also currently supported.   We use the local-storage version of Chroma, so there is no need for an API key.</li> <li>Redis Password, host, port: This is optional, and only needed to cache LLM API responses   using Redis Cloud. Redis offers a free 30MB Redis account   which is more than sufficient to try out Langroid and even beyond.   If you don't set up these, Langroid will use a pure-python   Redis in-memory cache via the Fakeredis library.</li> <li>Momento Serverless Caching of LLM API responses (as an alternative to Redis).   To use Momento instead of Redis:<ul> <li>enter your Momento Token in the <code>.env</code> file, as the value of <code>MOMENTO_AUTH_TOKEN</code> (see example file below),</li> <li>in the <code>.env</code> file set <code>CACHE_TYPE=momento</code> (instead of <code>CACHE_TYPE=redis</code> which is the default).</li> </ul> </li> <li>GitHub Personal Access Token (required for apps that need to analyze git   repos; token-based API calls are less rate-limited). See this   GitHub page.</li> <li>Google Custom Search API Credentials: Only needed to enable an Agent to use the <code>GoogleSearchTool</code>.   To use Google Search as an LLM Tool/Plugin/function-call,   you'll need to set up   a Google API key,   then setup a Google Custom Search Engine (CSE) and get the CSE ID.   (Documentation for these can be challenging, we suggest asking GPT4 for a step-by-step guide.)   After obtaining these credentials, store them as values of   <code>GOOGLE_API_KEY</code> and <code>GOOGLE_CSE_ID</code> in your <code>.env</code> file.   Full documentation on using this (and other such \"stateless\" tools) is coming soon, but   in the meantime take a peek at the test   <code>tests/main/test_google_search_tool.py</code> to see how to use it.</li> </ul> <p>If you add all of these optional variables, your <code>.env</code> file should look like this: <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\nGITHUB_ACCESS_TOKEN=your-personal-access-token-no-quotes\nCACHE_TYPE=redis # or momento\nREDIS_PASSWORD=your-redis-password-no-quotes\nREDIS_HOST=your-redis-hostname-no-quotes\nREDIS_PORT=your-redis-port-no-quotes\nMOMENTO_AUTH_TOKEN=your-momento-token-no-quotes # instead of REDIS* variables\nQDRANT_API_KEY=your-key\nQDRANT_API_URL=https://your.url.here:6333 # note port number must be included\nGOOGLE_API_KEY=your-key\nGOOGLE_CSE_ID=your-cse-id\n</code></pre></p>"},{"location":"quick-start/setup/#microsoft-azure-openai-setupoptional","title":"Microsoft Azure OpenAI setup[Optional]","text":"<p>This section applies only if you are using Microsoft Azure OpenAI.</p> <p>When using Azure OpenAI, additional environment variables are required in the <code>.env</code> file. This page Microsoft Azure OpenAI provides more information, and you can set each environment variable as follows:</p> <ul> <li><code>AZURE_OPENAI_API_KEY</code>, from the value of <code>API_KEY</code></li> <li><code>AZURE_OPENAI_API_BASE</code> from the value of <code>ENDPOINT</code>, typically looks like <code>https://your.domain.azure.com</code>.</li> <li>For <code>AZURE_OPENAI_API_VERSION</code>, you can use the default value in <code>.env-template</code>, and latest version can be found here</li> <li><code>AZURE_OPENAI_DEPLOYMENT_NAME</code> is the name of the deployed model, which is defined by the user during the model setup</li> <li><code>AZURE_OPENAI_MODEL_NAME</code> Azure OpenAI allows specific model names when you select the model for your deployment. You need to put precisly the exact model name that was selected. For example, GPT-3.5 (should be <code>gpt-35-turbo-16k</code> or <code>gpt-35-turbo</code>) or GPT-4 (should be <code>gpt-4-32k</code> or <code>gpt-4</code>).</li> <li><code>AZURE_OPENAI_MODEL_VERSION</code> is required if <code>AZURE_OPENAI_MODEL_NAME = gpt=4</code>, which will assist Langroid to determine the cost of the model</li> </ul>"},{"location":"quick-start/setup/#next-steps","title":"Next steps","text":"<p>Now you should be ready to use Langroid! As a next step, you may want to see how you can use Langroid to interact  directly with the LLM (OpenAI GPT models only for now).</p>"},{"location":"quick-start/three-agent-chat-num-router/","title":"Three-Agent Collaboration, with message Routing","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>three-agent-chat-num-router.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>.</p> <p>Let's change the number game from the three agent chat example slightly. In that example, when the <code>even_agent</code>'s LLM receives an odd number, it responds with <code>DO-NOT-KNOW</code>, and similarly for the <code>odd_agent</code> when it receives an even number. The <code>step()</code> method of the <code>repeater_task</code> considers <code>DO-NOT-KNOW</code> to be an invalid response and continues to  look for a valid response from any remaining sub-tasks. Thus there was no need for the <code>processor_agent</code> to specify who should handle the current number.</p> <p>But what if there is a scenario where the <code>even_agent</code> and <code>odd_agent</code> might return a legit but \"wrong\" answer? In this section we add this twist -- when the <code>even_agent</code> receives an odd number, it responds with -10, and similarly for the <code>odd_agent</code> when it receives an even number. We tell the <code>processor_agent</code> to avoid getting a negative number.</p> <p>The goal we have set for the <code>processor_agent</code> implies that it  must specify the intended recipient of  the number it is sending.  We can enforce this using a special Langroid Tool,  <code>RecipientTool</code>. So when setting up the <code>processor_task</code> we include instructions to use this tool (whose name is <code>recipient_message</code>, the value of <code>RecipientTool.request</code>):</p> <pre><code>processor_agent = lr.ChatAgent(config)\nprocessor_task = lr.Task(\n    processor_agent,\n    name = \"Processor\",\n    system_message=\"\"\"\n        You will receive a list of numbers from me (the user).\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation.\n        You can take the help of two people to perform the \n        transformation.\n        If the number is even, send it to EvenHandler,\n        and if it is odd, send it to OddHandler.\n\n        IMPORTANT: send the numbers ONE AT A TIME\n\n        The handlers will transform the number and give you a new number.        \n        If you send it to the wrong person, you will receive a negative value.\n        Your aim is to never get a negative number, so you must \n        clearly specify who you are sending the number to, using the\n        `recipient_message` tool/function-call, where the `content` field\n        is the number you want to send, and the `recipient` field is the name\n        of the intended recipient, either \"EvenHandler\" or \"OddHandler\".        \n\n        Once all numbers in the given list have been transformed, \n        say DONE and show me the result. \n        Start by asking me for the list of numbers.\n    \"\"\",\n    llm_delegate=True,\n    single_round=False,\n)\n</code></pre> <p>To enable the <code>processor_agent</code> to use this tool, we must enable it: <pre><code>processor_agent.enable_message(lr.agent.tools.RecipientTool)\n</code></pre></p> <p>The rest of the code remains the same as in the previous section, i.e., we simply add the two handler tasks as sub-tasks of the <code>processor_task</code>, like this: <pre><code>processor_task.add_sub_task([even_task, odd_task])\n</code></pre></p> <p>One of the benefits of using the <code>RecipientTool</code> is that it contains  mechanisms to remind the LLM to specify a recipient for its message, when it forgets to do so (this does happen once in a while, even with GPT-4).</p> <p>Feel free to try the working example script <code>three-agent-chat-num-router.py</code> in the  <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num-router.py\n</code></pre> <p>Below is screenshot of what this might look like, using the OpenAI function-calling  mechanism with the <code>recipient_message</code> tool:</p> <p></p> <p>And here is what it looks like using Langroid's built-in tools mechanism (use the <code>-t</code> option when running the script):</p> <p></p> <p>And here is what it looks like using </p>"},{"location":"quick-start/three-agent-chat-num-router/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid with external documents.</p>"},{"location":"quick-start/three-agent-chat-num/","title":"Three-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>three-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>.</p> <p>Let us set up a simple numbers exercise between 3 agents. The <code>Processor</code> agent receives a list of numbers, and its goal is to  apply a transformation to each number \\(n\\). However it does not know how to apply these transformations, and takes the help of two other agents to do so. Given a number \\(n\\),</p> <ul> <li>The <code>EvenHandler</code> returns \\(n/2\\) if n is even, otherwise says <code>DO-NOT-KNOW</code>.</li> <li>The <code>OddHandler</code> returns \\(3n+1\\) if n is odd, otherwise says <code>DO-NOT-KNOW</code>.</li> </ul> <p>As before we first create a common <code>ChatAgentConfig</code> to use for all agents:</p> <pre><code>config = lr.ChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    vecdb=None,\n)\n</code></pre> <p>Next, set up the <code>processor_agent</code>, along with instructions for the task: <pre><code>processor_agent = lr.ChatAgent(config)\nprocessor_task = lr.Task(\n    processor_agent,\n    name = \"Processor\",\n    system_message=\"\"\"\n        You will receive a list of numbers from the user.\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation,\n        so the user will help you. \n        You can simply send the user each number FROM THE GIVEN LIST\n        and the user will return the result \n        with the appropriate transformation applied.\n        IMPORTANT: only send one number at a time, concisely, say nothing else.\n        Once you have accomplished your goal, say DONE and show the result.\n        Start by asking the user for the list of numbers.\n        \"\"\",\n    llm_delegate=True, #(1)!\n    single_round=False, #(2)!\n)\n</code></pre></p> <ol> <li>Setting the <code>llm_delegate</code> option to <code>True</code> means that the <code>processor_task</code> is     delegated to the LLM (as opposed to the User),      in the sense that the LLM is the one \"seeking\" a response to the latest      number. Specifically, this means that in the <code>processor_task.step()</code>      when a sub-task returns <code>DO-NOT-KNOW</code>,     it is not considered a valid response, and the search for a valid response      continues to the next sub-task if any.</li> <li><code>single_round=False</code> means that the <code>processor_task</code> should not terminate after      a valid response from a responder.</li> </ol> <p>Set up the other two agents and tasks:</p> <pre><code>NO_ANSWER = lr.utils.constants.NO_ANSWER\n\neven_agent = lr.ChatAgent(config)\neven_task = lr.Task(\n    even_agent,\n    name = \"EvenHandler\",\n    system_message=f\"\"\"\n    You will be given a number. \n    If it is even, divide by 2 and say the result, nothing else.\n    If it is odd, say {NO_ANSWER}\n    \"\"\",\n    single_round=True,  # task done after 1 step() with valid response\n)\n\nodd_agent = lr.ChatAgent(config)\nodd_task = lr.Task(\n    odd_agent,\n    name = \"OddHandler\",\n    system_message=f\"\"\"\n    You will be given a number n. \n    If it is odd, return (n*3+1), say nothing else. \n    If it is even, say {NO_ANSWER}\n    \"\"\",\n    single_round=True,  # task done after 1 step() with valid response\n)\n</code></pre> <p>Now add the <code>even_task</code> and <code>odd_task</code> as subtasks of the <code>processor_task</code>,  and then run it as before:</p> <pre><code>processor_task.add_sub_task([even_task, odd_task])\nprocessor_task.run()\n</code></pre> <p>Feel free to try the working example script <code>three-agent-chat-num.py</code> <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num.py\n</code></pre> <p>Here's a screenshot of what it looks like: </p>"},{"location":"quick-start/three-agent-chat-num/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid to equip a <code>ChatAgent</code> with tools or function-calling.</p>"},{"location":"quick-start/two-agent-chat-num/","title":"Two-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/two-agent-chat-num.py</code>.</p> <p>To illustrate these ideas, let's look at a toy example<sup>1</sup> where  a <code>Student</code> agent receives a list of numbers to add. We set up this agent with an instruction that they do not know how to add, and they can ask for help adding pairs of numbers. To add pairs of numbers, we set up an <code>Adder</code> agent.</p> <p>First define a common <code>ChatAgentConfig</code> to use for both agents: <pre><code>config = lr.ChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    vecdb = None, #(1)!\n)\n</code></pre></p> <ol> <li>We don't need access to external docs so we set <code>vecdb=None</code> to avoid     the overhead of loading a vector-store.</li> </ol> <p>Next, set up the student agent and the corresponding task:</p> <pre><code>student_agent = lr.ChatAgent(config)\nstudent_task = lr.Task(\n    student_agent,\n    name = \"Student\",\n    system_message=\"\"\"\n        You will receive a list of numbers from me (the User),\n        and your goal is to calculate their sum.\n        However you do not know how to add numbers.\n        I can help you add numbers, two at a time, since\n        I only know how to add pairs of numbers.\n        Send me a pair of numbers to add, one at a time, \n        and I will tell you their sum.\n        For each question, simply ask me the sum in math notation, \n        e.g., simply say \"1 + 2\", etc, and say nothing else.\n        Once you have added all the numbers in the list, \n        say DONE and give me the final sum. \n        Start by asking me for the list of numbers.\n    \"\"\",\n    llm_delegate = True, #(1)!\n    single_round=False,  # (2)! \n)\n</code></pre> <ol> <li>Whenever we \"flip roles\" and assign the LLM the role of generating questions,     we set <code>llm_delegate=True</code>. In effect this ensures that the LLM \"decides\" when    the task is done.</li> <li>This setting means the task is not a single-round task, i.e. it is not done    after one <code>step()</code> with a valid response.</li> </ol> <p>Next, set up the adder agent and task:</p> <pre><code>adder_agent = lr.ChatAgent(config)\nadder_task = lr.Task(\n    adder_agent,\n    name = \"Adder\", #(1)!\n    system_message=\"\"\"\n        You are an expert on addition of numbers. \n        When given numbers to add, simply return their sum, say nothing else\n        \"\"\",\n    single_round=True,  # task done after 1 step() with valid response (2)!\n)\n</code></pre> <ol> <li>The Task name is used when displaying the conversation in the console.</li> <li>We set <code>single_round=True</code> to ensure that the expert task is done after     one step() with a valid response. </li> </ol> <p>Finally, we add the <code>adder_task</code> as a sub-task of the <code>student_task</code>,  and run the <code>student_task</code>:</p> <pre><code>student_task.add_sub_task(adder_task) #(1)!\nstudent_task.run()\n</code></pre> <ol> <li>When adding just one sub-task, we don't need to use a list.</li> </ol> <p>For a full working example, see the  <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo. You can run this using: <pre><code>python3 examples/quick-start/two-agent-chat-num.py\n</code></pre></p> <p>Here is an example of the conversation that results:</p> <p></p>"},{"location":"quick-start/two-agent-chat-num/#logs-of-multi-agent-interactions","title":"Logs of multi-agent interactions","text":"<p>For advanced users</p> <p>This section is for advanced users who want more visibility into the internals of multi-agent interactions.</p> <p>When running a multi-agent chat, e.g. using <code>task.run()</code>, two types of logs are generated: - plain-text logs in <code>logs/&lt;task_name&gt;.log</code> - tsv logs in <code>logs/&lt;task_name&gt;.tsv</code></p> <p>It is important to realize that the logs show every iteration  of the loop in <code>Task.step()</code>, i.e. every attempt at responding to the current pending message, even those that are not allowed. The ones marked with an asterisk (*) are the ones that are considered valid responses for a given <code>step()</code> (which is a \"turn\" in the conversation).</p> <p>The plain text logs have color-coding ANSI chars to make them easier to read by doing <code>less &lt;log_file&gt;</code>. The format is (subject to change): <pre><code>(TaskName) Responder SenderEntity (EntityName) (=&gt; Recipient) TOOL Content\n</code></pre></p> <p>The structure of the <code>tsv</code> logs is similar. A great way to view these is to install and use the excellent <code>visidata</code> (https://www.visidata.org/) tool: <pre><code>vd logs/&lt;task_name&gt;.tsv\n</code></pre></p>"},{"location":"quick-start/two-agent-chat-num/#next-steps","title":"Next steps","text":"<p>As a next step, look at how to set up a collaboration among three agents for a simple numbers game.</p> <ol> <li> <p>Toy numerical examples are perfect to illustrate the ideas without   incurring too much token cost from LLM API calls.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"langroid","text":"<p>langroid/init.py </p> <p>Main langroid package</p>"},{"location":"reference/#langroid.Agent","title":"<code>Agent(config=AgentConfig())</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An Agent is an abstraction that encapsulates mainly two components:</p> <ul> <li>a language model (LLM)</li> <li>a vector store (vecdb)</li> </ul> <p>plus associated components such as a parser, and variables that hold information about any tool/function-calling messages that have been defined.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig = AgentConfig()):\n    self.config = config\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.interactive: bool | None = None\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    if config.parsing is not None and self.config.llm is not None:\n        # token_encoding_model is used to obtain the tokenizer,\n        # so in case it's an OpenAI model, we ensure that the tokenizer\n        # corresponding to the model is used.\n        if isinstance(self.llm, OpenAIGPT) and self.llm.is_openai_chat_model():\n            config.parsing.token_encoding_model = self.llm.config.chat_model\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n    if config.add_to_registry:\n        ObjectRegistry.register_object(self)\n\n    self.callbacks = SimpleNamespace(\n        start_llm_stream=lambda: noop_fn,\n        cancel_llm_stream=noop_fn,\n        finish_llm_stream=noop_fn,\n        show_llm_response=noop_fn,\n        show_agent_response=noop_fn,\n        get_user_response=None,\n        get_last_step=noop_fn,\n        set_parent_agent=noop_fn,\n        show_error_message=noop_fn,\n        show_start_response=noop_fn,\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.indent","title":"<code>indent: str</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/#langroid.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/#langroid.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/#langroid.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/#langroid.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/#langroid.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_examples(random=True)  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/#langroid.Agent.create_agent_response","title":"<code>create_agent_response(content=None)</code>","text":"<p>Template for agent_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_agent_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for agent_response.\"\"\"\n    return self._response_template(Entity.AGENT, content)\n</code></pre>"},{"location":"reference/#langroid.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n    if results is None:\n        return None\n    if isinstance(results, ChatDocument):\n        # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n        results.metadata.tool_ids = (\n            [] if isinstance(msg, str) else msg.metadata.tool_ids\n        )\n        return results\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {escape(results)}\")\n        maybe_json = len(extract_top_level_json(results)) &gt; 0\n        self.callbacks.show_agent_response(\n            content=results,\n            language=\"json\" if maybe_json else \"text\",\n        )\n    sender_name = self.config.name\n    if isinstance(msg, ChatDocument) and msg.function_call is not None:\n        # if result was from handling an LLM `function_call`,\n        # set sender_name to \"request\", i.e. name of the function_call\n        sender_name = msg.function_call.name\n\n    return ChatDocument(\n        content=results,\n        metadata=ChatDocMetaData(\n            source=Entity.AGENT,\n            sender=Entity.AGENT,\n            sender_name=sender_name,\n            # preserve trail of tool_ids for OpenAI Assistant fn-calls\n            tool_ids=[] if isinstance(msg, str) else msg.metadata.tool_ids,\n        ),\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.create_user_response","title":"<code>create_user_response(content=None)</code>","text":"<p>Template for user_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_user_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for user_response.\"\"\"\n    return self._response_template(Entity.USER, content)\n</code></pre>"},{"location":"reference/#langroid.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n\n    # When msg explicitly addressed to user, this means an actual human response\n    # is being sought.\n    need_human_response = (\n        isinstance(msg, ChatDocument) and msg.metadata.recipient == Entity.USER\n    )\n\n    interactive = self.interactive or settings.interactive\n\n    if not interactive and not need_human_response:\n        return None\n    elif self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if self.callbacks.get_user_response is not None:\n            # ask user with empty prompt: no need for prompt\n            # since user has seen the conversation so far.\n            # But non-empty prompt can be useful when Agent\n            # uses a tool that requires user input, or in other scenarios.\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}Human \"\n                \"(respond or q, x to exit current level, \"\n                f\"or hit enter to continue)\\n{self.indent}\",\n            ).strip()\n\n    tool_ids = []\n    if msg is not None and isinstance(msg, ChatDocument):\n        tool_ids = msg.metadata.tool_ids\n    # only return non-None result if user_msg not empty\n    if not user_msg:\n        return None\n    else:\n        if user_msg.startswith(\"SYSTEM\"):\n            user_msg = user_msg[6:].strip()\n            source = Entity.SYSTEM\n            sender = Entity.SYSTEM\n        else:\n            source = Entity.USER\n            sender = Entity.USER\n        return ChatDocument(\n            content=user_msg,\n            metadata=ChatDocMetaData(\n                source=source,\n                sender=sender,\n                # preserve trail of tool_ids for OpenAI Assistant fn-calls\n                tool_ids=tool_ids,\n            ),\n        )\n</code></pre>"},{"location":"reference/#langroid.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if message is not None and len(self.get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/#langroid.Agent.create_llm_response","title":"<code>create_llm_response(content=None)</code>","text":"<p>Template for llm_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_llm_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for llm_response.\"\"\"\n    return self._response_template(Entity.LLM, content)\n</code></pre>"},{"location":"reference/#langroid.Agent.llm_response_async","title":"<code>llm_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    output_len = self.config.llm.max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM. \n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + escape(response.message))\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = [] if isinstance(msg, str) else msg.metadata.tool_ids\n    return cdoc\n</code></pre>"},{"location":"reference/#langroid.Agent.llm_response","title":"<code>llm_response(msg=None)</code>","text":"<p>LLM response to a prompt. Args:     msg (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        msg (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM. \n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + escape(response.message))\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = [] if isinstance(msg, str) else msg.metadata.tool_ids\n    return cdoc\n</code></pre>"},{"location":"reference/#langroid.Agent.has_tool_message_attempt","title":"<code>has_tool_message_attempt(msg)</code>","text":"<p>Check whether msg contains a Tool/fn-call attempt (by the LLM)</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_tool_message_attempt(self, msg: str | ChatDocument | None) -&gt; bool:\n    \"\"\"Check whether msg contains a Tool/fn-call attempt (by the LLM)\"\"\"\n    if msg is None:\n        return False\n    try:\n        tools = self.get_tool_messages(msg)\n        return len(tools) &gt; 0\n    except ValidationError:\n        # there is a tool/fn-call attempt but had a validation error,\n        # so we still consider this a tool message \"attempt\"\n        return True\n    return False\n</code></pre>"},{"location":"reference/#langroid.Agent.get_json_tool_messages","title":"<code>get_json_tool_messages(input_str)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    json_substrings = extract_top_level_json(input_str)\n    if len(json_substrings) == 0:\n        return []\n    results = [self._get_one_tool_message(j) for j in json_substrings]\n    return [r for r in results if r is not None]\n</code></pre>"},{"location":"reference/#langroid.Agent.tool_validation_error","title":"<code>tool_validation_error(ve)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     tool (ToolMessage): The tool message that failed validation     ve (ValidationError): The exception raised</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(self, ve: ValidationError) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        tool (ToolMessage): The tool message that failed validation\n        ve (ValidationError): The exception raised\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    tool_name = cast(ToolMessage, ve.model).default_value(\"request\")\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc']}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the \n    TOOL or function_call named '{tool_name}': \n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | ChatDocument</code> <p>Optional[Str]: The result of the handler method in string form so it can</p> <code>None | str | ChatDocument</code> <p>be sent back to the LLM, or None if <code>msg</code> was not successfully</p> <code>None | str | ChatDocument</code> <p>handled by a method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(self, msg: str | ChatDocument) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        Optional[Str]: The result of the handler method in string form so it can\n        be sent back to the LLM, or None if `msg` was not successfully\n        handled by a method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        return self.handle_message_fallback(msg)\n\n    results = [self.handle_tool_message(t) for t in tools]\n\n    results_list = [r for r in results if r is not None]\n    if len(results_list) == 0:\n        return None  # self.handle_message_fallback(msg)\n    # there was a non-None result\n    chat_doc_results = [r for r in results_list if isinstance(r, ChatDocument)]\n    if len(chat_doc_results) &gt; 1:\n        logger.warning(\n            \"\"\"There were multiple ChatDocument results from tools,\n            which is unexpected. The first one will be returned, and the others\n            will be ignored.\n            \"\"\"\n        )\n    if len(chat_doc_results) &gt; 0:\n        return chat_doc_results[0]\n\n    str_doc_results = [r for r in results_list if isinstance(r, str)]\n    final = \"\\n\".join(str_doc_results)\n    return final\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method to handle possible \"tool\" msg if no other method applies or if an error is thrown. This method can be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     str: The result of the handler method in string form so it can         be sent back to the LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Fallback method to handle possible \"tool\" msg if no other method applies\n    or if an error is thrown.\n    This method can be overridden by subclasses.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        str: The result of the handler method in string form so it can\n            be sent back to the LLM.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_tool_message","title":"<code>handle_tool_message(tool)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(self, tool: ToolMessage) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    handler_method = getattr(self, tool_name, None)\n    if handler_method is None:\n        return None\n\n    try:\n        result = handler_method(tool)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return result  # type: ignore\n</code></pre>"},{"location":"reference/#langroid.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields).the usage memebr It updates the cost after checking the cache and updates the tokens (prompts and completion) if the response stream is True, because OpenAI doesn't returns these fields.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields).the usage memebr\n    It updates the cost after checking the cache and updates the\n    tokens (prompts and completion) if the response stream is True, because OpenAI\n    doesn't returns these fields.\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/#langroid.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/#langroid.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/#langroid.StatusCode","title":"<code>StatusCode</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Codes meant to be returned by task.run(). Some are not used yet.</p>"},{"location":"reference/#langroid.ChatDocument","title":"<code>ChatDocument(**data)</code>","text":"<p>             Bases: <code>Document</code></p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def __init__(self, **data: Any):\n    super().__init__(**data)\n    ObjectRegistry.register_object(self)\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.delete_id","title":"<code>delete_id(id)</code>  <code>staticmethod</code>","text":"<p>Remove ChatDocument with given id from ObjectRegistry, and all its descendants.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef delete_id(id: str) -&gt; None:\n    \"\"\"Remove ChatDocument with given id from ObjectRegistry,\n    and all its descendants.\n    \"\"\"\n    chat_doc = ChatDocument.from_id(id)\n    # first delete all descendants\n    while chat_doc is not None:\n        next_chat_doc = chat_doc.child\n        ObjectRegistry.remove(chat_doc.id())\n        chat_doc = next_chat_doc\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.get_json_tools","title":"<code>get_json_tools()</code>","text":"<p>Get names of attempted JSON tool usages in the content     of the message. Returns:     List[str]: list of JSON tool names</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_json_tools(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted JSON tool usages in the content\n        of the message.\n    Returns:\n        List[str]: list of JSON tool names\n    \"\"\"\n    jsons = extract_top_level_json(self.content)\n    tools = []\n    for j in jsons:\n        json_data = json.loads(j)\n        tool = json_data.get(\"request\")\n        if tool is not None:\n            tools.append(str(tool))\n    return tools\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n    if self.function_call is not None:\n        tool_type = \"FUNC\"\n        tool = self.function_call.name\n    elif self.get_json_tools() != []:\n        tool_type = \"TOOL\"\n        tool = self.get_json_tools()[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    if tool_type == \"FUNC\":\n        content += str(self.function_call)\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.pop_tool_ids","title":"<code>pop_tool_ids()</code>","text":"<p>Pop the last tool_id from the stack of tool_ids.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def pop_tool_ids(self) -&gt; None:\n    \"\"\"\n    Pop the last tool_id from the stack of tool_ids.\n    \"\"\"\n    if len(self.metadata.tool_ids) &gt; 0:\n        self.metadata.tool_ids.pop()\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.from_LLMResponse","title":"<code>from_LLMResponse(response, displayed=False)</code>  <code>staticmethod</code>","text":"<p>Convert LLMResponse to ChatDocument. Args:     response (LLMResponse): LLMResponse to convert.     displayed (bool): Whether this response was displayed to the user. Returns:     ChatDocument: ChatDocument representation of this LLMResponse.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMResponse(\n    response: LLMResponse,\n    displayed: bool = False,\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMResponse to ChatDocument.\n    Args:\n        response (LLMResponse): LLMResponse to convert.\n        displayed (bool): Whether this response was displayed to the user.\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMResponse.\n    \"\"\"\n    recipient, message = response.get_recipient_and_message()\n    message = message.strip()\n    if message in [\"''\", '\"\"']:\n        message = \"\"\n    if response.function_call is not None:\n        # Sometimes an OpenAI LLM (esp gpt-4o) may generate a function-call\n        # with odditities:\n        # (a) the `name` is set, as well as `arugments.request` is set,\n        #  and in langroid we use the `request` value as the `name`.\n        #  In this case we override the `name` with the `request` value.\n        # (b) the `name` looks like \"functions blah\" or just \"functions\"\n        #   In this case we strip the \"functions\" part.\n        fc = response.function_call\n        fc.name = fc.name.replace(\"functions\", \"\").strip()\n        if fc.arguments is not None:\n            request = fc.arguments.get(\"request\")\n            if request is not None and request != \"\":\n                fc.name = request\n                fc.arguments.pop(\"request\")\n    return ChatDocument(\n        content=message,\n        function_call=response.function_call,\n        metadata=ChatDocMetaData(\n            source=Entity.LLM,\n            sender=Entity.LLM,\n            usage=response.usage,\n            displayed=displayed,\n            cached=response.cached,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message)</code>  <code>staticmethod</code>","text":"<p>Convert to LLMMessage for use with LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <p>Returns:     LLMMessage: LLMMessage representation of this str or ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(message: Union[str, \"ChatDocument\"]) -&gt; LLMMessage:\n    \"\"\"\n    Convert to LLMMessage for use with LLM.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n    Returns:\n        LLMMessage: LLMMessage representation of this str or ChatDocument.\n\n    \"\"\"\n    sender_name = None\n    sender_role = Role.USER\n    fun_call = None\n    tool_id = \"\"\n    chat_document_id: str = \"\"\n    if isinstance(message, ChatDocument):\n        content = message.content\n        fun_call = message.function_call\n        if message.metadata.sender == Entity.USER and fun_call is not None:\n            # This may happen when a (parent agent's) LLM generates a\n            # a Function-call, and it ends up being sent to the current task's\n            # LLM (possibly because the function-call is mis-named or has other\n            # issues and couldn't be handled by handler methods).\n            # But a function-call can only be generated by an entity with\n            # Role.ASSISTANT, so we instead put the content of the function-call\n            # in the content of the message.\n            content += \" \" + str(fun_call)\n            fun_call = None\n        sender_name = message.metadata.sender_name\n        tool_ids = message.metadata.tool_ids\n        tool_id = tool_ids[-1] if len(tool_ids) &gt; 0 else \"\"\n        chat_document_id = message.id()\n        if message.metadata.sender == Entity.SYSTEM:\n            sender_role = Role.SYSTEM\n        if (\n            message.metadata.parent is not None\n            and message.metadata.parent.function_call is not None\n        ):\n            sender_role = Role.FUNCTION\n            sender_name = message.metadata.parent.function_call.name\n        elif message.metadata.sender == Entity.LLM:\n            sender_role = Role.ASSISTANT\n    else:\n        # LLM can only respond to text content, so extract it\n        content = message\n\n    return LLMMessage(\n        role=sender_role,\n        tool_id=tool_id,\n        content=content,\n        function_call=fun_call,\n        name=sender_name,\n        chat_document_id=chat_document_id,\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p> <code>result</code> <code>str</code> <p>example of result of agent method.</p>"},{"location":"reference/#langroid.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with JSON formatting instructions. Each example can be either: - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or - a tuple (description, ToolMessage instance), where the description is     a natural language \"thought\" that leads to the tool usage,     e.g. (\"I want to find the square of 5\",  SquareTool(num=5))     In some scenarios, including such a description can significantly     enhance reliability of tool use. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\" | Tuple[str, \"ToolMessage\"]]:\n    \"\"\"\n    Examples to use in few-shot demos with JSON formatting instructions.\n    Each example can be either:\n    - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or\n    - a tuple (description, ToolMessage instance), where the description is\n        a natural language \"thought\" that leads to the tool usage,\n        e.g. (\"I want to find the square of 5\",  SquareTool(num=5))\n        In some scenarios, including such a description can significantly\n        enhance reliability of tool use.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.usage_examples","title":"<code>usage_examples(random=False)</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing examples of how to use the tool-message.</p> <p>Parameters:</p> Name Type Description Default <code>random</code> <code>bool</code> <p>whether to pick a random example from the list of examples. Set to <code>true</code> when using this to illustrate a dialog between LLM and user. (if false, use ALL examples)</p> <code>False</code> <p>Returns:     str: examples of how to use the tool/function-call</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_examples(cls, random: bool = False) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing examples of how to use the tool-message.\n\n    Args:\n        random (bool): whether to pick a random example from the list of examples.\n            Set to `true` when using this to illustrate a dialog between LLM and\n            user.\n            (if false, use ALL examples)\n    Returns:\n        str: examples of how to use the tool/function-call\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    if random:\n        examples = [choice(cls.examples())]\n    else:\n        examples = cls.examples()\n    examples_jsons = [\n        (\n            f\"EXAMPLE {i}: (THOUGHT: {ex[0]}) =&gt; \\n{ex[1].json_example()}\"\n            if isinstance(ex, tuple)\n            else f\"EXAMPLE {i}:\\n {ex.json_example()}\"\n        )\n        for i, ex in enumerate(examples, 1)\n    ]\n    return \"\\n\\n\".join(examples_jsons)\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.json_instructions","title":"<code>json_instructions(tool=False)</code>  <code>classmethod</code>","text":"<p>Default Instructions to the LLM showing how to use the tool/function-call. Works for GPT4 but override this for weaker LLMs if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>bool</code> <p>instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM) (or else it would be for OpenAI Function calls)</p> <code>False</code> <p>Returns:     str: instructions on how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef json_instructions(cls, tool: bool = False) -&gt; str:\n    \"\"\"\n    Default Instructions to the LLM showing how to use the tool/function-call.\n    Works for GPT4 but override this for weaker LLMs if needed.\n\n    Args:\n        tool: instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM)\n            (or else it would be for OpenAI Function calls)\n    Returns:\n        str: instructions on how to use the message\n    \"\"\"\n    # TODO: when we attempt to use a \"simpler schema\"\n    # (i.e. all nested fields explicit without definitions),\n    # we seem to get worse results, so we turn it off for now\n    param_dict = (\n        # cls.simple_schema() if tool else\n        cls.llm_function_schema(request=True).parameters\n    )\n    examples_str = \"\"\n    if cls.examples():\n        examples_str = \"EXAMPLES:\\n\" + cls.usage_examples()\n    return textwrap.dedent(\n        f\"\"\"\n        TOOL: {cls.default_value(\"request\")}\n        PURPOSE: {cls.default_value(\"purpose\")} \n        JSON FORMAT: {\n            json.dumps(param_dict, indent=4)\n        }\n        {examples_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.json_group_instructions","title":"<code>json_group_instructions()</code>  <code>staticmethod</code>","text":"<p>Template for instructions for a group of tools. Works with GPT4 but override this for weaker LLMs if needed.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@staticmethod\ndef json_group_instructions() -&gt; str:\n    \"\"\"Template for instructions for a group of tools.\n    Works with GPT4 but override this for weaker LLMs if needed.\n    \"\"\"\n    return textwrap.dedent(\n        \"\"\"\n        === ALL AVAILABLE TOOLS and THEIR JSON FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {json_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above JSON format.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False, defaults=True)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <code>defaults</code> <code>bool</code> <p>whether to include fields with default values in the schema,     in the \"properties\" section.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(\n    cls,\n    request: bool = False,\n    defaults: bool = True,\n) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n        defaults: whether to include fields with default values in the schema,\n                in the \"properties\" section.\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = cls.schema()\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = (\n        [\"result\", \"purpose\"] if request else [\"request\", \"result\", \"purpose\"]\n    )\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes and (defaults or details.get(\"default\") is None)\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\"),\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.simple_schema","title":"<code>simple_schema()</code>  <code>classmethod</code>","text":"<p>Return a simplified schema for the message, with only the request and required fields. Returns:     Dict[str, Any]: simplified schema</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef simple_schema(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a simplified schema for the message, with only the request and\n    required fields.\n    Returns:\n        Dict[str, Any]: simplified schema\n    \"\"\"\n    schema = generate_simple_schema(cls, exclude=[\"result\", \"purpose\"])\n    return schema\n</code></pre>"},{"location":"reference/#langroid.ChatAgent","title":"<code>ChatAgent(config=ChatAgentConfig(), task=None)</code>","text":"<p>             Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: ChatAgentConfig = ChatAgentConfig(),\n    task: Optional[List[LLMMessage]] = None,\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    self.config._set_fn_or_tools(self._fn_call_available())\n    self.message_history: List[LLMMessage] = []\n    self.tool_instructions_added: bool = False\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_json_tool_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.task_messages","title":"<code>task_messages: List[LLMMessage]</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/#langroid.ChatAgent.from_id","title":"<code>from_id(id)</code>  <code>staticmethod</code>","text":"<p>Get an agent from its ID Args:     agent_id (str): ID of the agent Returns:     ChatAgent: The agent with the given ID</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@staticmethod\ndef from_id(id: str) -&gt; \"ChatAgent\":\n    \"\"\"\n    Get an agent from its ID\n    Args:\n        agent_id (str): ID of the agent\n    Returns:\n        ChatAgent: The agent with the given ID\n    \"\"\"\n    return cast(ChatAgent, Agent.from_id(id))\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.clone","title":"<code>clone(i=0)</code>","text":"<p>Create i'th clone of this agent, ensuring tool use/handling is cloned. Important: We assume all member variables are in the init method here and in the Agent class. TODO: We are attempting to clone an agent after its state has been changed in possibly many ways. Below is an imperfect solution. Caution advised. Revisit later.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clone(self, i: int = 0) -&gt; \"ChatAgent\":\n    \"\"\"Create i'th clone of this agent, ensuring tool use/handling is cloned.\n    Important: We assume all member variables are in the __init__ method here\n    and in the Agent class.\n    TODO: We are attempting to clone an agent after its state has been\n    changed in possibly many ways. Below is an imperfect solution. Caution advised.\n    Revisit later.\n    \"\"\"\n    agent_cls = type(self)\n    config_copy = copy.deepcopy(self.config)\n    config_copy.name = f\"{config_copy.name}-{i}\"\n    new_agent = agent_cls(config_copy)\n    new_agent.system_tool_instructions = self.system_tool_instructions\n    new_agent.system_json_tool_instructions = self.system_json_tool_instructions\n    new_agent.llm_tools_map = self.llm_tools_map\n    new_agent.llm_functions_map = self.llm_functions_map\n    new_agent.llm_functions_handled = self.llm_functions_handled\n    new_agent.llm_functions_usable = self.llm_functions_usable\n    new_agent.llm_function_force = self.llm_function_force\n    # Caution - we are copying the vector-db, maybe we don't always want this?\n    new_agent.vecdb = self.vecdb\n    new_agent.id = ObjectRegistry.new_id()\n    if self.config.add_to_registry:\n        ObjectRegistry.register_object(new_agent)\n    return new_agent\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.clear_history","title":"<code>clear_history(start=-2)</code>","text":"<p>Clear the message history, starting at the index <code>start</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2) -&gt; None:\n    \"\"\"\n    Clear the message history, starting at the index `start`\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n    \"\"\"\n    if start &lt; 0:\n        n = len(self.message_history)\n        start = max(0, n + start)\n    dropped = self.message_history[start:]\n    for msg in dropped:\n        # clear out the chat document from the ObjectRegistry\n        ChatDocument.delete_id(msg.chat_document_id)\n    self.message_history = self.message_history[:start]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.json_format_rules","title":"<code>json_format_rules()</code>","text":"<p>Specification of JSON formatting rules, based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def json_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of JSON formatting rules, based on the currently enabled\n    usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"You can ask questions in natural language.\"\n    json_instructions = \"\\n\\n\".join(\n        [\n            msg_cls.json_instructions(tool=self.config.use_tools)\n            for _, msg_cls in enumerate(enabled_classes)\n            if msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ]\n    )\n    # if any of the enabled classes has json_group_instructions, then use that,\n    # else fall back to ToolMessage.json_group_instructions\n    for msg_cls in enabled_classes:\n        if hasattr(msg_cls, \"json_group_instructions\") and callable(\n            getattr(msg_cls, \"json_group_instructions\")\n        ):\n            return msg_cls.json_group_instructions().format(\n                json_instructions=json_instructions\n            )\n    return ToolMessage.json_group_instructions().format(\n        json_instructions=json_instructions\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if (\n            hasattr(msg_cls, \"instructions\")\n            and inspect.ismethod(msg_cls.instructions)\n            and msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ):\n            # example will be shown in json_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_examples())\n            if example != \"\":\n                example = \"EXAMPLES:\\n\" + example\n            class_instructions = msg_cls.instructions()\n            guidance = (\n                \"\"\n                if class_instructions == \"\"\n                else (\"GUIDANCE: \" + class_instructions)\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    n_role_msgs = len([m for m in self.message_history if m.role == role])\n    if n_role_msgs == 0:\n        return None\n    idx = self.nth_message_idx_with_role(role, n_role_msgs)\n    return self.message_history[idx]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.nth_message_idx_with_role","title":"<code>nth_message_idx_with_role(role, n)</code>","text":"<p>Index of <code>n</code>th message in message_history, with specified role. (n is assumed to be 1-based, i.e. 1 is the first message with that role). Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def nth_message_idx_with_role(self, role: Role, n: int) -&gt; int:\n    \"\"\"Index of `n`th message in message_history, with specified role.\n    (n is assumed to be 1-based, i.e. 1 is the first message with that role).\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n\n    if len(indices_with_role) &lt; n:\n        return -1\n    return indices_with_role[n - 1]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): new message to replace with     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): new message to replace with\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to enable, for USE, or HANDLING, or both. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> <code>require_defaults</code> <p>whether to include fields that have default values, in the \"properties\" section of the JSON format instructions. (Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.)</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class to enable,\n            for USE, or HANDLING, or both.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n        require_defaults: whether to include fields that have default values,\n            in the \"properties\" section of the JSON format instructions.\n            (Normally the OpenAI completion API ignores these fields,\n            but the Assistant fn-calling seems to pay attn to these,\n            and if we don't want this, we should set this to False.)\n    \"\"\"\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        if require_recipient:\n            message_class = message_class.require_recipient()\n        request = message_class.default_value(\"request\")\n        llm_function = message_class.llm_function_schema(defaults=include_defaults)\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            self.llm_tools_usable.add(t)\n            self.llm_functions_usable.add(t)\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    # Set tool instructions and JSON format instructions\n    if self.config.use_tools:\n        self.system_json_tool_instructions = self.json_format_rules()\n    self.system_tool_instructions = self.tool_instructions()\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.__fields__[\"request\"].default\n    to_remove = [r for r in self.llm_tools_usable if r != request]\n    for r in to_remove:\n        self.llm_tools_usable.discard(r)\n        self.llm_functions_usable.discard(r)\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = self.llm_response_messages(hist, output_len)\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    hist, output_len = self._prep_llm_messages(message)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm_response_messages_async(hist, output_len)\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.init_message_history","title":"<code>init_message_history()</code>","text":"<p>Initialize the message history with the system message and user message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_message_history(self) -&gt; None:\n    \"\"\"\n    Initialize the message history with the system message and user message\n    \"\"\"\n    self.message_history = [self._create_system_and_tools_message()]\n    if self.user_message:\n        self.message_history.append(\n            LLMMessage(role=Role.USER, content=self.user_message)\n        )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None)</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            # (Why? b/c the intent of showing a spinner is to \"show progress\",\n            # and we don't need to do that when streaming, since\n            # streaming output already shows progress.)\n            cm = status(\n                \"LLM responding to messages...\",\n                log_if_quiet=False,\n            )\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions, fun_call = self._function_args()\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            functions=functions,\n            function_call=fun_call,\n        )\n    if self.llm.get_stream():\n        self.callbacks.finish_llm_stream(\n            content=str(response),\n            is_tool=self.has_tool_message_attempt(\n                ChatDocument.from_LLMResponse(response, displayed=True),\n            ),\n        )\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)\n    return chat_doc\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    functions: Optional[List[LLMFunctionSpec]] = None\n    fun_call: str | Dict[str, str] = \"none\"\n    if self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\n        functions = [self.llm_functions_map[f] for f in self.llm_functions_usable]\n        fun_call = (\n            \"auto\" if self.llm_function_force is None else self.llm_function_force\n        )\n    assert self.llm is not None\n\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        functions=functions,\n        function_call=fun_call,\n    )\n    if self.llm.get_stream():\n        self.callbacks.finish_llm_stream(\n            content=str(response),\n            is_tool=self.has_tool_message_attempt(\n                ChatDocument.from_LLMResponse(response, displayed=True),\n            ),\n        )\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)\n    return chat_doc\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str): user message\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/#langroid.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>             Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent Attributes:     system_message: system message to include in message sequence          (typically defines role and task of agent).          Used only if <code>task</code> is not specified in the constructor.     user_message: user message to include in message sequence.          Used only if <code>task</code> is not specified in the constructor.     use_tools: whether to use our own ToolMessages mechanism     use_functions_api: whether to use functions native to the LLM API             (e.g. OpenAI's <code>function_call</code> mechanism)</p>"},{"location":"reference/#langroid.Task","title":"<code>Task(agent=None, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=True, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False, allow_null_result=False, max_stalled_steps=5, done_if_no_response=[], done_if_response=[], config=TaskConfig(), **kwargs)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>Whether to delegate \"control\" to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve, and decides when a task is done. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity). See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by \"controller\" (i.e. LLM if <code>llm_delegate</code> is true, otherwise USER) and subsequent response by non-controller [When a tool is involved, this will not give intended results. See <code>done_if_response</code>, <code>done_if_no_response</code> below]. termination]. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\". See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true, resets the agent's message history at every run.</p> <code>True</code> <code>default_human_response</code> <code>str | None</code> <p>default response from user; useful for testing, to avoid interactive input from user. [Instead of this, setting <code>interactive</code> usually suffices]</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\" Note: When interactive = False, the one exception is when the user is explicitly addressed, via \"@user\" or using RecipientTool, in which case the system will wait for a user response. In other words, use <code>interactive=False</code> when you want a \"largely non-interactive\" run, with the exception of explicit user addressing.</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, when interactive=True, only user can quit the root task (Ignored when interactive=False).</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> <code>allow_null_result</code> <code>bool</code> <p>If true, create dummy NO_ANSWER response when no valid response is found in a step. Optional, default is False. Note: In non-interactive mode, when this is set to True, you can have a situation where an LLM generates (non-tool) text, and no other responders have valid responses, and a \"Null result\" is inserted as a dummy response from the User entity, so the LLM will now respond to this Null result, and this will continue until the LLM emits a DONE signal (if instructed to do so), otherwise langroid detects a potential infinite loop after a certain number of such steps (= <code>TaskConfig.inf_loop_wait_factor</code>) and will raise an InfiniteLoopException.</p> <code>False</code> <code>max_stalled_steps</code> <code>int</code> <p>task considered done after this many consecutive steps with no progress. Default is 3.</p> <code>5</code> <code>done_if_no_response</code> <code>List[Responder]</code> <p>consider task done if NULL response from any of these responders. Default is empty list.</p> <code>[]</code> <code>done_if_response</code> <code>List[Responder]</code> <p>consider task done if NON-NULL response from any of these responders. Default is empty list.</p> <code>[]</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Optional[Agent] = None,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = True,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n    allow_null_result: bool = False,\n    max_stalled_steps: int = 5,\n    done_if_no_response: List[Responder] = [],\n    done_if_response: List[Responder] = [],\n    config: TaskConfig = TaskConfig(),\n    **kwargs: Any,  # catch-all for any legacy params, for backwards compatibility\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool):\n            Whether to delegate \"control\" to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve, and decides when a task is done.\n            The \"controlling entity\" is either the LLM or the USER.\n            (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        single_round (bool):\n            If true, task runs until one message by \"controller\"\n            (i.e. LLM if `llm_delegate` is true, otherwise USER)\n            and subsequent response by non-controller [When a tool is involved,\n            this will not give intended results. See `done_if_response`,\n            `done_if_no_response` below].\n            termination]. If false, runs for the specified number of turns in\n            `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true, resets the agent's message history *at every run*.\n        default_human_response (str|None): default response from user; useful for\n            testing, to avoid interactive input from user.\n            [Instead of this, setting `interactive` usually suffices]\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n            Note: When interactive = False, the one exception is when the user\n            is explicitly addressed, via \"@user\" or using RecipientTool, in which\n            case the system will wait for a user response. In other words, use\n            `interactive=False` when you want a \"largely non-interactive\"\n            run, with the exception of explicit user addressing.\n        only_user_quits_root (bool): if true, when interactive=True, only user can\n            quit the root task (Ignored when interactive=False).\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n        allow_null_result (bool):\n            If true, create dummy NO_ANSWER response when no valid response is found\n            in a step.\n            Optional, default is False.\n            *Note:* In non-interactive mode, when this is set to True,\n            you can have a situation where an LLM generates (non-tool) text,\n            and no other responders have valid responses, and a \"Null result\"\n            is inserted as a dummy response from the User entity, so the LLM\n            will now respond to this Null result, and this will continue\n            until the LLM emits a DONE signal (if instructed to do so),\n            otherwise langroid detects a potential infinite loop after\n            a certain number of such steps (= `TaskConfig.inf_loop_wait_factor`)\n            and will raise an InfiniteLoopException.\n        max_stalled_steps (int): task considered done after this many consecutive\n            steps with no progress. Default is 3.\n        done_if_no_response (List[Responder]): consider task done if NULL\n            response from any of these responders. Default is empty list.\n        done_if_response (List[Responder]): consider task done if NON-NULL\n            response from any of these responders. Default is empty list.\n    \"\"\"\n    if agent is None:\n        agent = ChatAgent()\n    self.callbacks = SimpleNamespace(\n        show_subtask_response=noop_fn,\n        set_parent_agent=noop_fn,\n    )\n    self.config = config\n    # how to behave as a sub-task; can be overriden by `add_sub_task()`\n    self.config_sub_task = copy.deepcopy(config)\n    # counts of distinct pending messages in history,\n    # to help detect (exact) infinite loops\n    self.message_counter: Counter[str] = Counter()\n    self._init_message_counter()\n\n    self.history: Deque[str] = deque(\n        maxlen=self.config.inf_loop_cycle_len * self.config.inf_loop_wait_factor\n    )\n    # copy the agent's config, so that we don't modify the original agent's config,\n    # which may be shared by other agents.\n    try:\n        config_copy = copy.deepcopy(agent.config)\n        agent.config = config_copy\n    except Exception:\n        logger.warning(\n            \"\"\"\n            Failed to deep-copy Agent config during task creation, \n            proceeding with original config. Be aware that changes to \n            the config may affect other agents using the same config.\n            \"\"\"\n        )\n    self.restart = restart\n    agent = cast(ChatAgent, agent)\n    self.agent: ChatAgent = agent\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        self.agent.clear_history(0)\n        self.agent.clear_dialog()\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            self.agent.set_system_message(system_message)\n        if user_message:\n            self.agent.set_user_message(user_message)\n    self.max_cost: float = 0\n    self.max_tokens: int = 0\n    self.session_id: str = \"\"\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.color_log: bool = False if settings.notebook else True\n\n    self.n_stalled_steps = 0  # how many consecutive steps with no progress?\n    # how many 2-step-apart alternations of no_answer step-result have we had,\n    # i.e. x1, N/A, x2, N/A, x3, N/A ...\n    self.n_no_answer_alternations = 0\n    self._no_answer_step: int = -5\n    self._step_idx = -1  # current step index\n    self.max_stalled_steps = max_stalled_steps\n    self.done_if_response = [r.value for r in done_if_response]\n    self.done_if_no_response = [r.value for r in done_if_no_response]\n    self.is_done = False  # is task done (based on response)?\n    self.is_pass_thru = False  # is current response a pass-thru?\n    if name:\n        # task name overrides name in agent config\n        agent.config.name = name\n    self.name = name or agent.config.name\n    self.value: str = self.name\n\n    self.default_human_response = default_human_response\n    if default_human_response is not None:\n        # only override agent's default_human_response if it is explicitly set\n        self.agent.default_human_response = default_human_response\n    self.interactive = interactive\n    self.agent.interactive = interactive\n    self.only_user_quits_root = only_user_quits_root\n    self.message_history_idx = -1\n\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n    self.allow_null_result = allow_null_result\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    if llm_delegate:\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM (as the Controller) asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        if self.single_round:\n            # 0: User (as Controller) asks,\n            # 1: LLM replies.\n            self.turns = 1\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/#langroid.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n    agent: ChatAgent = self.agent.clone(i)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=self.restart,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        erase_substeps=self.erase_substeps,\n        allow_null_result=self.allow_null_result,\n        max_stalled_steps=self.max_stalled_steps,\n        done_if_no_response=[Entity(s) for s in self.done_if_no_response],\n        done_if_response=[Entity(s) for s in self.done_if_response],\n        config=self.config,\n    )\n</code></pre>"},{"location":"reference/#langroid.Task.kill_session","title":"<code>kill_session(session_id='')</code>  <code>classmethod</code>","text":"<p>Kill the session with the given session_id.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>@classmethod\ndef kill_session(cls, session_id: str = \"\") -&gt; None:\n    \"\"\"\n    Kill the session with the given session_id.\n    \"\"\"\n    session_id_kill_key = f\"{session_id}:kill\"\n    cls.cache().store(session_id_kill_key, \"1\")\n</code></pre>"},{"location":"reference/#langroid.Task.kill","title":"<code>kill()</code>","text":"<p>Kill the task run associated with the current session.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def kill(self) -&gt; None:\n    \"\"\"\n    Kill the task run associated with the current session.\n    \"\"\"\n    self._cache_session_store(\"kill\", \"1\")\n</code></pre>"},{"location":"reference/#langroid.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response (unless a sub-task is explicitly addressed).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]</code> <p>A task, or list of tasks, or a tuple of task and task config, or a list of tuples of task and task config. These tasks are added as sub-tasks of the current task. The task configs (if any) dictate how the tasks are run when invoked as sub-tasks of other tasks. This allows users to specify behavior applicable only in the context of a particular task-subtask combination.</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(\n    self,\n    task: (\n        Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response (unless a sub-task is explicitly addressed).\n\n    Args:\n        task: A task, or list of tasks, or a tuple of task and task config,\n            or a list of tuples of task and task config.\n            These tasks are added as sub-tasks of the current task.\n            The task configs (if any) dictate how the tasks are run when\n            invoked as sub-tasks of other tasks. This allows users to specify\n            behavior applicable only in the context of a particular task-subtask\n            combination.\n    \"\"\"\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n\n    if isinstance(task, tuple):\n        task, config = task\n    else:\n        config = TaskConfig()\n    task.config_sub_task = config\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/#langroid.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    elif msg is None and len(self.agent.message_history) &gt; 1:\n        # if agent has a history beyond system msg, set the\n        # pending message to the ChatDocument linked from\n        # last message in the history\n        last_agent_msg = self.agent.message_history[-1]\n        self.pending_message = ChatDocument.from_id(last_agent_msg.chat_document_id)\n        if self.pending_message is not None:\n            self.pending_sender = self.pending_message.metadata.sender\n    else:\n        if isinstance(msg, ChatDocument):\n            # carefully deep-copy: fresh metadata.id, register\n            # as new obj in registry\n            self.pending_message = ChatDocument.deepcopy(msg)\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n            # update parent, child, agent pointers\n            if msg is not None:\n                msg.metadata.child_id = self.pending_message.metadata.id\n                self.pending_message.metadata.parent_id = msg.metadata.id\n            self.pending_message.metadata.agent_id = self.agent.id\n\n    self._show_pending_message_if_debug()\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    else:\n        self.logger = RichFileLogger(\n            str(Path(self.config.logs_dir) / f\"{self.name}.log\"),\n            color=self.color_log,\n        )\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    else:\n        self.tsv_logger = setup_file_logger(\n            \"tsv_logger\",\n            str(Path(self.config.logs_dir) / f\"{self.name}.tsv\"),\n        )\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    self.log_message(Entity.USER, self.pending_message)\n    return self.pending_message\n</code></pre>"},{"location":"reference/#langroid.Task.reset_all_sub_tasks","title":"<code>reset_all_sub_tasks()</code>","text":"<p>Recursively reset message history of own agent and all sub-tasks</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def reset_all_sub_tasks(self) -&gt; None:\n    \"\"\"Recursively reset message history of own agent and all sub-tasks\"\"\"\n    self.agent.clear_history(0)\n    self.agent.clear_dialog()\n    for t in self.sub_tasks:\n        t.reset_all_sub_tasks()\n</code></pre>"},{"location":"reference/#langroid.Task.run","title":"<code>run(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='')</code>","text":"<p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n    if (self.restart and caller is None) or (\n        self.config_sub_task.restart_as_subtask and caller is not None\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    assert (\n        msg is None or isinstance(msg, str) or isinstance(msg, ChatDocument)\n    ), f\"msg arg in Task.run() must be None, str, or ChatDocument, not {type(msg)}\"\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        self.step()\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/#langroid.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='')</code>  <code>async</code>","text":"<p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>initial user-role message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User). Note that <code>msg</code>, if passed, is treated as message with role <code>user</code>; a \"system\" role message should not be passed here.</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <code>max_cost</code> <code>float</code> <p>max cost allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>max tokens allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>session_id</code> <code>str</code> <p>session id for the task</p> <code>''</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (str|ChatDocument): initial *user-role* message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User). Note that `msg`, if passed, is treated as\n            message with role `user`; a \"system\" role message should not be\n            passed here.\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n        max_cost (float): max cost allowed for the task (default 0 -&gt; no limit)\n        max_tokens (int): max tokens allowed for the task (default 0 -&gt; no limit)\n        session_id (str): session id for the task\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if (\n        self.restart\n        and caller is None\n        or self.config_sub_task.restart_as_subtask\n        and caller is not None\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=True,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        await self.step_async()\n        await asyncio.sleep(0.01)  # temp yield to avoid blocking\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/#langroid.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details. TODO: Except for the self.response() calls, this fn should be identical to <code>step_async()</code>. Consider refactoring to avoid duplication.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    TODO: Except for the self.response() calls, this fn should be identical to\n    `step_async()`. Consider refactoring to avoid duplication.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # When in interactive mode,\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:  # did not find a valid response\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/#langroid.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/#langroid.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        result = e.run(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n            max_cost=self.max_cost,\n            max_tokens=self.max_tokens,\n        )\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\" if result is None else str(ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n    return self._process_result_routing(result)\n</code></pre>"},{"location":"reference/#langroid.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        result = await e.run_async(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n            max_cost=self.max_cost,\n            max_tokens=self.max_tokens,\n        )\n        result_str = str(ChatDocument.to_LLMMessage(result))\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n    return self._process_result_routing(result)\n</code></pre>"},{"location":"reference/#langroid.Task.result","title":"<code>result(status=None)</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Note the result of a task is returned as if it is from the User entity.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusCode</code> <p>status of the task when it ended</p> <code>None</code> <p>Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self, status: StatusCode | None = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n\n    Note the result of a task is returned as if it is from the User entity.\n\n    Args:\n        status (StatusCode): status of the task when it ended\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    if status in [StatusCode.STALLED, StatusCode.MAX_TURNS, StatusCode.INF_LOOP]:\n        # In these case we don't know (and don't want to try to guess)\n        # what the task result should be, so we return None\n        return None\n\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    if DONE in content:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    fun_call = result_msg.function_call if result_msg else None\n    tool_messages = result_msg.tool_messages if result_msg else []\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else \"\"\n    tool_ids = result_msg.metadata.tool_ids if result_msg else []\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    result_doc = ChatDocument(\n        content=content,\n        function_call=fun_call,\n        tool_messages=tool_messages,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            status=status or (result_msg.metadata.status if result_msg else None),\n            sender_name=self.name,\n            recipient=recipient,\n            tool_ids=tool_ids,\n            parent_id=result_msg.id() if result_msg else \"\",\n            agent_id=str(self.agent.id),\n        ),\n    )\n    if self.pending_message is not None:\n        self.pending_message.metadata.child_id = result_doc.id()\n\n    return result_doc\n</code></pre>"},{"location":"reference/#langroid.Task.done","title":"<code>done(result=None, r=None)</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Args:     result (ChatDocument|None): result from a responder     r (Responder|None): responder that produced the result         Not used here, but could be used by derived classes. Returns:     bool: True if task is done, False otherwise     StatusCode: status code indicating why task is done</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(\n    self, result: ChatDocument | None = None, r: Responder | None = None\n) -&gt; Tuple[bool, StatusCode]:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Args:\n        result (ChatDocument|None): result from a responder\n        r (Responder|None): responder that produced the result\n            Not used here, but could be used by derived classes.\n    Returns:\n        bool: True if task is done, False otherwise\n        StatusCode: status code indicating why task is done\n    \"\"\"\n    if self._is_kill():\n        return (True, StatusCode.KILL)\n    result = result or self.pending_message\n    user_quit = (\n        result is not None\n        and (result.content in USER_QUIT_STRINGS or DONE in result.content)\n        and result.metadata.sender == Entity.USER\n    )\n    if self._level == 0 and self.interactive and self.only_user_quits_root:\n        # for top-level task, in interactive mode, only user can quit out\n        return (user_quit, StatusCode.USER_QUIT if user_quit else StatusCode.OK)\n\n    if self.is_done:\n        return (True, StatusCode.DONE)\n\n    if self.n_stalled_steps &gt;= self.max_stalled_steps:\n        # we are stuck, so bail to avoid infinite loop\n        logger.warning(\n            f\"Task {self.name} stuck for {self.max_stalled_steps} steps; exiting.\"\n        )\n        return (True, StatusCode.STALLED)\n\n    if self.max_cost &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[1] &gt; self.max_cost:\n                logger.warning(\n                    f\"Task {self.name} cost exceeded {self.max_cost}; exiting.\"\n                )\n                return (True, StatusCode.MAX_COST)\n        except Exception:\n            pass\n\n    if self.max_tokens &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[0] &gt; self.max_tokens:\n                logger.warning(\n                    f\"Task {self.name} uses &gt; {self.max_tokens} tokens; exiting.\"\n                )\n                return (True, StatusCode.MAX_TOKENS)\n        except Exception:\n            pass\n    final = (\n        # no valid response from any entity/agent in current turn\n        result is None\n        # An entity decided task is done\n        or DONE in result.content\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and result.metadata.recipient == self.caller.name\n        )\n        or user_quit\n    )\n    return (final, StatusCode.OK)\n</code></pre>"},{"location":"reference/#langroid.Task.valid","title":"<code>valid(result, r)</code>","text":"<p>Is the result from a Responder (i.e. an entity or sub-task) such that we can stop searching for responses in this step?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(\n    self,\n    result: Optional[ChatDocument],\n    r: Responder,\n) -&gt; bool:\n    \"\"\"\n    Is the result from a Responder (i.e. an entity or sub-task)\n    such that we can stop searching for responses in this step?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n\n    # if task would be considered done given responder r's `result`,\n    # then consider the result valid.\n    if result is not None and self.done(result, r)[0]:\n        return True\n    return (\n        result is not None\n        and not self._is_empty_message(result)\n        # some weaker LLMs, including even GPT-4o, may say \"DO-NOT-KNOW.\"\n        # (with a punctuation at the end), so need to strip out punctuation\n        and re.sub(r\"[,.!?:]\", \"\", result.content.strip()) != NO_ANSWER\n    )\n</code></pre>"},{"location":"reference/#langroid.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    default_values = ChatDocLoggerFields().dict().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n</code></pre>"},{"location":"reference/#langroid.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/#langroid.TaskConfig","title":"<code>TaskConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration for a Task. This is a container for any params that we didn't include in the task <code>__init__</code> method. We may eventually move all the task init params to this class, analogous to how we have config classes for <code>Agent</code>, <code>ChatAgent</code>, <code>LanguageModel</code>, etc.</p> <p>Attributes:</p> Name Type Description <code>inf_loop_cycle_len</code> <code>int</code> <p>max exact-loop cycle length: 0 =&gt; no inf loop test</p> <code>inf_loop_dominance_factor</code> <code>float</code> <p>dominance factor for exact-loop detection</p> <code>inf_loop_wait_factor</code> <code>int</code> <p>wait this * cycle_len msgs before loop-check</p> <code>restart_subtask_run</code> <code>bool</code> <p>whether to restart every run of this task when run as a subtask.</p>"},{"location":"reference/#langroid.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p>"},{"location":"reference/#langroid.DocMetaData.dict_bool_int","title":"<code>dict_bool_int(*args, **kwargs)</code>","text":"<p>Special dict method to convert bool fields to int, to appease some downstream libraries,  e.g. Chroma which complains about bool fields in metadata.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>def dict_bool_int(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Special dict method to convert bool fields to int, to appease some\n    downstream libraries,  e.g. Chroma which complains about bool fields in\n    metadata.\n    \"\"\"\n    original_dict = super().dict(*args, **kwargs)\n\n    for key, value in original_dict.items():\n        if isinstance(value, bool):\n            original_dict[key] = 1 * value\n\n    return original_dict\n</code></pre>"},{"location":"reference/#langroid.Document","title":"<code>Document</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p>"},{"location":"reference/#langroid.Entity","title":"<code>Entity</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the different types of entities that can respond to the current message.</p>"},{"location":"reference/#langroid.LangroidImportError","title":"<code>LangroidImportError(package=None, extra=None, error='', *args)</code>","text":"<p>             Bases: <code>ImportError</code></p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The name of the package to import.</p> <code>None</code> <code>extra</code> <code>str</code> <p>The name of the extras package required for this import.</p> <code>None</code> <code>error</code> <code>str</code> <p>The error message to display. Depending on context, we can set this by capturing the ImportError message.</p> <code>''</code> Source code in <code>langroid/exceptions.py</code> <pre><code>def __init__(\n    self,\n    package: Optional[str] = None,\n    extra: Optional[str] = None,\n    error: str = \"\",\n    *args: object,\n) -&gt; None:\n    \"\"\"\n    Generate helpful warning when attempting to import package or module.\n\n    Args:\n        package (str): The name of the package to import.\n        extra (str): The name of the extras package required for this import.\n        error (str): The error message to display. Depending on context, we\n            can set this by capturing the ImportError message.\n\n    \"\"\"\n    if error == \"\" and package is not None:\n        error = f\"{package} is not installed by default with Langroid.\\n\"\n\n    if extra:\n        install_help = f\"\"\"\n            If you want to use it, please install langroid \n            with the `{extra}` extra, for example:\n\n            If you are using pip:\n            pip install \"langroid[{extra}]\"\n\n            For multiple extras, you can separate them with commas:\n            pip install \"langroid[{extra},another-extra]\"\n\n            If you are using Poetry:\n            poetry add langroid --extras \"{extra}\"\n\n            For multiple extras with Poetry, list them with spaces:\n            poetry add langroid --extras \"{extra} another-extra\"\n\n            If you are working within the langroid dev env (which uses Poetry),\n            you can do:\n            poetry install -E \"{extra}\" \n            or if you want to include multiple extras:\n            poetry install -E \"{extra} another-extra\"\n            \"\"\"\n    else:\n        install_help = \"\"\"\n            If you want to use it, please install it in the same\n            virtual environment as langroid.\n            \"\"\"\n    msg = error + install_help\n\n    super().__init__(msg, *args)\n</code></pre>"},{"location":"reference/#langroid.run_batch_tasks","title":"<code>run_batch_tasks(task, items, input_map=lambda x: str(x), output_map=lambda x: x, sequential=True, batch_size=None, turns=-1, max_cost=0.0, max_tokens=0)</code>","text":"<p>Run copies of <code>task</code> async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     task (Task): task to run     items (list[T]): list of items to process     input_map (Callable[[T], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], U]): function to map result         to final result     sequential (bool): whether to run sequentially         (e.g. some APIs such as ooba don't support concurrent requests)     batch_size (Optional[int]): The number of tasks to run at a time,         if None, unbatched     turns (int): number of turns to run, -1 for infinite     max_cost: float: maximum cost to run the task (default 0.0 for unlimited)     max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)</p> <p>Returns:</p> Type Description <code>List[U]</code> <p>list[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_tasks(\n    task: Task,\n    items: list[T],\n    input_map: Callable[[T], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], U] = lambda x: x,  # type: ignore\n    sequential: bool = True,\n    batch_size: Optional[int] = None,\n    turns: int = -1,\n    max_cost: float = 0.0,\n    max_tokens: int = 0,\n) -&gt; List[U]:\n    \"\"\"\n    Run copies of `task` async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        task (Task): task to run\n        items (list[T]): list of items to process\n        input_map (Callable[[T], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], U]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        batch_size (Optional[int]): The number of tasks to run at a time,\n            if None, unbatched\n        turns (int): number of turns to run, -1 for infinite\n        max_cost: float: maximum cost to run the task (default 0.0 for unlimited)\n        max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)\n\n    Returns:\n        list[Any]: list of final results\n    \"\"\"\n    message = f\"[bold green]Running {len(items)} copies of {task.name}...\"\n    return run_batch_task_gen(\n        lambda i: task.clone(i),\n        items,\n        input_map,\n        output_map,\n        sequential,\n        batch_size,\n        turns,\n        message,\n        max_cost=max_cost,\n        max_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>langroid<ul> <li>agent<ul> <li>base</li> <li>batch</li> <li>callbacks<ul> <li>chainlit</li> </ul> </li> <li>chat_agent</li> <li>chat_document</li> <li>openai_assistant</li> <li>special<ul> <li>doc_chat_agent</li> <li>lance_doc_chat_agent</li> <li>lance_rag<ul> <li>critic_agent</li> <li>lance_rag_task</li> <li>query_planner_agent</li> </ul> </li> <li>lance_tools</li> <li>neo4j<ul> <li>csv_kg_chat</li> <li>neo4j_chat_agent</li> <li>utils<ul> <li>system_message</li> </ul> </li> </ul> </li> <li>relevance_extractor_agent</li> <li>retriever_agent</li> <li>sql<ul> <li>sql_chat_agent</li> <li>utils<ul> <li>description_extractors</li> <li>populate_metadata</li> <li>system_message</li> <li>tools</li> </ul> </li> </ul> </li> <li>table_chat_agent</li> </ul> </li> <li>task</li> <li>tool_message</li> <li>tools<ul> <li>duckduckgo_search_tool</li> <li>google_search_tool</li> <li>metaphor_search_tool</li> <li>recipient_tool</li> <li>retrieval_tool</li> <li>rewind_tool</li> <li>segment_extract_tool</li> </ul> </li> </ul> </li> <li>cachedb<ul> <li>base</li> <li>momento_cachedb</li> <li>redis_cachedb</li> </ul> </li> <li>embedding_models<ul> <li>base</li> <li>models</li> <li>protoc<ul> <li>embeddings_pb2</li> <li>embeddings_pb2_grpc</li> </ul> </li> <li>remote_embeds</li> </ul> </li> <li>exceptions</li> <li>language_models<ul> <li>azure_openai</li> <li>base</li> <li>config</li> <li>mock_lm</li> <li>openai_gpt</li> <li>prompt_formatter<ul> <li>base</li> <li>hf_formatter</li> <li>llama2_formatter</li> </ul> </li> <li>utils</li> </ul> </li> <li>mytypes</li> <li>parsing<ul> <li>agent_chats</li> <li>code_parser</li> <li>document_parser</li> <li>para_sentence_split</li> <li>parse_json</li> <li>parser</li> <li>repo_loader</li> <li>routing</li> <li>search</li> <li>spider</li> <li>table_loader</li> <li>url_loader</li> <li>urls</li> <li>utils</li> <li>web_search</li> </ul> </li> <li>prompts<ul> <li>dialog</li> <li>prompts_config</li> <li>templates</li> </ul> </li> <li>pydantic_v1<ul> <li>main</li> </ul> </li> <li>utils<ul> <li>algorithms<ul> <li>graph</li> </ul> </li> <li>configuration</li> <li>constants</li> <li>globals</li> <li>logging</li> <li>object_registry</li> <li>output<ul> <li>citations</li> <li>printing</li> <li>status</li> </ul> </li> <li>pandas_utils</li> <li>pydantic_utils</li> <li>system</li> </ul> </li> <li>vector_store<ul> <li>base</li> <li>chromadb</li> <li>lancedb</li> <li>meilisearch</li> <li>momento</li> <li>qdrantdb</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/exceptions/","title":"exceptions","text":"<p>langroid/exceptions.py </p>"},{"location":"reference/exceptions/#langroid.exceptions.LangroidImportError","title":"<code>LangroidImportError(package=None, extra=None, error='', *args)</code>","text":"<p>             Bases: <code>ImportError</code></p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The name of the package to import.</p> <code>None</code> <code>extra</code> <code>str</code> <p>The name of the extras package required for this import.</p> <code>None</code> <code>error</code> <code>str</code> <p>The error message to display. Depending on context, we can set this by capturing the ImportError message.</p> <code>''</code> Source code in <code>langroid/exceptions.py</code> <pre><code>def __init__(\n    self,\n    package: Optional[str] = None,\n    extra: Optional[str] = None,\n    error: str = \"\",\n    *args: object,\n) -&gt; None:\n    \"\"\"\n    Generate helpful warning when attempting to import package or module.\n\n    Args:\n        package (str): The name of the package to import.\n        extra (str): The name of the extras package required for this import.\n        error (str): The error message to display. Depending on context, we\n            can set this by capturing the ImportError message.\n\n    \"\"\"\n    if error == \"\" and package is not None:\n        error = f\"{package} is not installed by default with Langroid.\\n\"\n\n    if extra:\n        install_help = f\"\"\"\n            If you want to use it, please install langroid \n            with the `{extra}` extra, for example:\n\n            If you are using pip:\n            pip install \"langroid[{extra}]\"\n\n            For multiple extras, you can separate them with commas:\n            pip install \"langroid[{extra},another-extra]\"\n\n            If you are using Poetry:\n            poetry add langroid --extras \"{extra}\"\n\n            For multiple extras with Poetry, list them with spaces:\n            poetry add langroid --extras \"{extra} another-extra\"\n\n            If you are working within the langroid dev env (which uses Poetry),\n            you can do:\n            poetry install -E \"{extra}\" \n            or if you want to include multiple extras:\n            poetry install -E \"{extra} another-extra\"\n            \"\"\"\n    else:\n        install_help = \"\"\"\n            If you want to use it, please install it in the same\n            virtual environment as langroid.\n            \"\"\"\n    msg = error + install_help\n\n    super().__init__(msg, *args)\n</code></pre>"},{"location":"reference/mytypes/","title":"mytypes","text":"<p>langroid/mytypes.py </p>"},{"location":"reference/mytypes/#langroid.mytypes.Entity","title":"<code>Entity</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the different types of entities that can respond to the current message.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData.dict_bool_int","title":"<code>dict_bool_int(*args, **kwargs)</code>","text":"<p>Special dict method to convert bool fields to int, to appease some downstream libraries,  e.g. Chroma which complains about bool fields in metadata.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>def dict_bool_int(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Special dict method to convert bool fields to int, to appease some\n    downstream libraries,  e.g. Chroma which complains about bool fields in\n    metadata.\n    \"\"\"\n    original_dict = super().dict(*args, **kwargs)\n\n    for key, value in original_dict.items():\n        if isinstance(value, bool):\n            original_dict[key] = 1 * value\n\n    return original_dict\n</code></pre>"},{"location":"reference/mytypes/#langroid.mytypes.Document","title":"<code>Document</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p>"},{"location":"reference/agent/","title":"agent","text":"<p>langroid/agent/init.py </p>"},{"location":"reference/agent/#langroid.agent.Agent","title":"<code>Agent(config=AgentConfig())</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An Agent is an abstraction that encapsulates mainly two components:</p> <ul> <li>a language model (LLM)</li> <li>a vector store (vecdb)</li> </ul> <p>plus associated components such as a parser, and variables that hold information about any tool/function-calling messages that have been defined.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig = AgentConfig()):\n    self.config = config\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.interactive: bool | None = None\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    if config.parsing is not None and self.config.llm is not None:\n        # token_encoding_model is used to obtain the tokenizer,\n        # so in case it's an OpenAI model, we ensure that the tokenizer\n        # corresponding to the model is used.\n        if isinstance(self.llm, OpenAIGPT) and self.llm.is_openai_chat_model():\n            config.parsing.token_encoding_model = self.llm.config.chat_model\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n    if config.add_to_registry:\n        ObjectRegistry.register_object(self)\n\n    self.callbacks = SimpleNamespace(\n        start_llm_stream=lambda: noop_fn,\n        cancel_llm_stream=noop_fn,\n        finish_llm_stream=noop_fn,\n        show_llm_response=noop_fn,\n        show_agent_response=noop_fn,\n        get_user_response=None,\n        get_last_step=noop_fn,\n        set_parent_agent=noop_fn,\n        show_error_message=noop_fn,\n        show_start_response=noop_fn,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.indent","title":"<code>indent: str</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/#langroid.agent.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_examples(random=True)  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.create_agent_response","title":"<code>create_agent_response(content=None)</code>","text":"<p>Template for agent_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_agent_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for agent_response.\"\"\"\n    return self._response_template(Entity.AGENT, content)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n    if results is None:\n        return None\n    if isinstance(results, ChatDocument):\n        # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n        results.metadata.tool_ids = (\n            [] if isinstance(msg, str) else msg.metadata.tool_ids\n        )\n        return results\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {escape(results)}\")\n        maybe_json = len(extract_top_level_json(results)) &gt; 0\n        self.callbacks.show_agent_response(\n            content=results,\n            language=\"json\" if maybe_json else \"text\",\n        )\n    sender_name = self.config.name\n    if isinstance(msg, ChatDocument) and msg.function_call is not None:\n        # if result was from handling an LLM `function_call`,\n        # set sender_name to \"request\", i.e. name of the function_call\n        sender_name = msg.function_call.name\n\n    return ChatDocument(\n        content=results,\n        metadata=ChatDocMetaData(\n            source=Entity.AGENT,\n            sender=Entity.AGENT,\n            sender_name=sender_name,\n            # preserve trail of tool_ids for OpenAI Assistant fn-calls\n            tool_ids=[] if isinstance(msg, str) else msg.metadata.tool_ids,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.create_user_response","title":"<code>create_user_response(content=None)</code>","text":"<p>Template for user_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_user_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for user_response.\"\"\"\n    return self._response_template(Entity.USER, content)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n\n    # When msg explicitly addressed to user, this means an actual human response\n    # is being sought.\n    need_human_response = (\n        isinstance(msg, ChatDocument) and msg.metadata.recipient == Entity.USER\n    )\n\n    interactive = self.interactive or settings.interactive\n\n    if not interactive and not need_human_response:\n        return None\n    elif self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if self.callbacks.get_user_response is not None:\n            # ask user with empty prompt: no need for prompt\n            # since user has seen the conversation so far.\n            # But non-empty prompt can be useful when Agent\n            # uses a tool that requires user input, or in other scenarios.\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}Human \"\n                \"(respond or q, x to exit current level, \"\n                f\"or hit enter to continue)\\n{self.indent}\",\n            ).strip()\n\n    tool_ids = []\n    if msg is not None and isinstance(msg, ChatDocument):\n        tool_ids = msg.metadata.tool_ids\n    # only return non-None result if user_msg not empty\n    if not user_msg:\n        return None\n    else:\n        if user_msg.startswith(\"SYSTEM\"):\n            user_msg = user_msg[6:].strip()\n            source = Entity.SYSTEM\n            sender = Entity.SYSTEM\n        else:\n            source = Entity.USER\n            sender = Entity.USER\n        return ChatDocument(\n            content=user_msg,\n            metadata=ChatDocMetaData(\n                source=source,\n                sender=sender,\n                # preserve trail of tool_ids for OpenAI Assistant fn-calls\n                tool_ids=tool_ids,\n            ),\n        )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if message is not None and len(self.get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.create_llm_response","title":"<code>create_llm_response(content=None)</code>","text":"<p>Template for llm_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_llm_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for llm_response.\"\"\"\n    return self._response_template(Entity.LLM, content)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.llm_response_async","title":"<code>llm_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    output_len = self.config.llm.max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM. \n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + escape(response.message))\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = [] if isinstance(msg, str) else msg.metadata.tool_ids\n    return cdoc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.llm_response","title":"<code>llm_response(msg=None)</code>","text":"<p>LLM response to a prompt. Args:     msg (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        msg (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM. \n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + escape(response.message))\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = [] if isinstance(msg, str) else msg.metadata.tool_ids\n    return cdoc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.has_tool_message_attempt","title":"<code>has_tool_message_attempt(msg)</code>","text":"<p>Check whether msg contains a Tool/fn-call attempt (by the LLM)</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_tool_message_attempt(self, msg: str | ChatDocument | None) -&gt; bool:\n    \"\"\"Check whether msg contains a Tool/fn-call attempt (by the LLM)\"\"\"\n    if msg is None:\n        return False\n    try:\n        tools = self.get_tool_messages(msg)\n        return len(tools) &gt; 0\n    except ValidationError:\n        # there is a tool/fn-call attempt but had a validation error,\n        # so we still consider this a tool message \"attempt\"\n        return True\n    return False\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.get_json_tool_messages","title":"<code>get_json_tool_messages(input_str)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    json_substrings = extract_top_level_json(input_str)\n    if len(json_substrings) == 0:\n        return []\n    results = [self._get_one_tool_message(j) for j in json_substrings]\n    return [r for r in results if r is not None]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.tool_validation_error","title":"<code>tool_validation_error(ve)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     tool (ToolMessage): The tool message that failed validation     ve (ValidationError): The exception raised</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(self, ve: ValidationError) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        tool (ToolMessage): The tool message that failed validation\n        ve (ValidationError): The exception raised\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    tool_name = cast(ToolMessage, ve.model).default_value(\"request\")\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc']}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the \n    TOOL or function_call named '{tool_name}': \n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | ChatDocument</code> <p>Optional[Str]: The result of the handler method in string form so it can</p> <code>None | str | ChatDocument</code> <p>be sent back to the LLM, or None if <code>msg</code> was not successfully</p> <code>None | str | ChatDocument</code> <p>handled by a method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(self, msg: str | ChatDocument) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        Optional[Str]: The result of the handler method in string form so it can\n        be sent back to the LLM, or None if `msg` was not successfully\n        handled by a method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        return self.handle_message_fallback(msg)\n\n    results = [self.handle_tool_message(t) for t in tools]\n\n    results_list = [r for r in results if r is not None]\n    if len(results_list) == 0:\n        return None  # self.handle_message_fallback(msg)\n    # there was a non-None result\n    chat_doc_results = [r for r in results_list if isinstance(r, ChatDocument)]\n    if len(chat_doc_results) &gt; 1:\n        logger.warning(\n            \"\"\"There were multiple ChatDocument results from tools,\n            which is unexpected. The first one will be returned, and the others\n            will be ignored.\n            \"\"\"\n        )\n    if len(chat_doc_results) &gt; 0:\n        return chat_doc_results[0]\n\n    str_doc_results = [r for r in results_list if isinstance(r, str)]\n    final = \"\\n\".join(str_doc_results)\n    return final\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method to handle possible \"tool\" msg if no other method applies or if an error is thrown. This method can be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     str: The result of the handler method in string form so it can         be sent back to the LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Fallback method to handle possible \"tool\" msg if no other method applies\n    or if an error is thrown.\n    This method can be overridden by subclasses.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        str: The result of the handler method in string form so it can\n            be sent back to the LLM.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_tool_message","title":"<code>handle_tool_message(tool)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(self, tool: ToolMessage) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    handler_method = getattr(self, tool_name, None)\n    if handler_method is None:\n        return None\n\n    try:\n        result = handler_method(tool)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return result  # type: ignore\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields).the usage memebr It updates the cost after checking the cache and updates the tokens (prompts and completion) if the response stream is True, because OpenAI doesn't returns these fields.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields).the usage memebr\n    It updates the cost after checking the cache and updates the\n    tokens (prompts and completion) if the response stream is True, because OpenAI\n    doesn't returns these fields.\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/agent/#langroid.agent.ChatDocument","title":"<code>ChatDocument(**data)</code>","text":"<p>             Bases: <code>Document</code></p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def __init__(self, **data: Any):\n    super().__init__(**data)\n    ObjectRegistry.register_object(self)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.delete_id","title":"<code>delete_id(id)</code>  <code>staticmethod</code>","text":"<p>Remove ChatDocument with given id from ObjectRegistry, and all its descendants.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef delete_id(id: str) -&gt; None:\n    \"\"\"Remove ChatDocument with given id from ObjectRegistry,\n    and all its descendants.\n    \"\"\"\n    chat_doc = ChatDocument.from_id(id)\n    # first delete all descendants\n    while chat_doc is not None:\n        next_chat_doc = chat_doc.child\n        ObjectRegistry.remove(chat_doc.id())\n        chat_doc = next_chat_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.get_json_tools","title":"<code>get_json_tools()</code>","text":"<p>Get names of attempted JSON tool usages in the content     of the message. Returns:     List[str]: list of JSON tool names</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_json_tools(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted JSON tool usages in the content\n        of the message.\n    Returns:\n        List[str]: list of JSON tool names\n    \"\"\"\n    jsons = extract_top_level_json(self.content)\n    tools = []\n    for j in jsons:\n        json_data = json.loads(j)\n        tool = json_data.get(\"request\")\n        if tool is not None:\n            tools.append(str(tool))\n    return tools\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n    if self.function_call is not None:\n        tool_type = \"FUNC\"\n        tool = self.function_call.name\n    elif self.get_json_tools() != []:\n        tool_type = \"TOOL\"\n        tool = self.get_json_tools()[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    if tool_type == \"FUNC\":\n        content += str(self.function_call)\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.pop_tool_ids","title":"<code>pop_tool_ids()</code>","text":"<p>Pop the last tool_id from the stack of tool_ids.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def pop_tool_ids(self) -&gt; None:\n    \"\"\"\n    Pop the last tool_id from the stack of tool_ids.\n    \"\"\"\n    if len(self.metadata.tool_ids) &gt; 0:\n        self.metadata.tool_ids.pop()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.from_LLMResponse","title":"<code>from_LLMResponse(response, displayed=False)</code>  <code>staticmethod</code>","text":"<p>Convert LLMResponse to ChatDocument. Args:     response (LLMResponse): LLMResponse to convert.     displayed (bool): Whether this response was displayed to the user. Returns:     ChatDocument: ChatDocument representation of this LLMResponse.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMResponse(\n    response: LLMResponse,\n    displayed: bool = False,\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMResponse to ChatDocument.\n    Args:\n        response (LLMResponse): LLMResponse to convert.\n        displayed (bool): Whether this response was displayed to the user.\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMResponse.\n    \"\"\"\n    recipient, message = response.get_recipient_and_message()\n    message = message.strip()\n    if message in [\"''\", '\"\"']:\n        message = \"\"\n    if response.function_call is not None:\n        # Sometimes an OpenAI LLM (esp gpt-4o) may generate a function-call\n        # with odditities:\n        # (a) the `name` is set, as well as `arugments.request` is set,\n        #  and in langroid we use the `request` value as the `name`.\n        #  In this case we override the `name` with the `request` value.\n        # (b) the `name` looks like \"functions blah\" or just \"functions\"\n        #   In this case we strip the \"functions\" part.\n        fc = response.function_call\n        fc.name = fc.name.replace(\"functions\", \"\").strip()\n        if fc.arguments is not None:\n            request = fc.arguments.get(\"request\")\n            if request is not None and request != \"\":\n                fc.name = request\n                fc.arguments.pop(\"request\")\n    return ChatDocument(\n        content=message,\n        function_call=response.function_call,\n        metadata=ChatDocMetaData(\n            source=Entity.LLM,\n            sender=Entity.LLM,\n            usage=response.usage,\n            displayed=displayed,\n            cached=response.cached,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message)</code>  <code>staticmethod</code>","text":"<p>Convert to LLMMessage for use with LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <p>Returns:     LLMMessage: LLMMessage representation of this str or ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(message: Union[str, \"ChatDocument\"]) -&gt; LLMMessage:\n    \"\"\"\n    Convert to LLMMessage for use with LLM.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n    Returns:\n        LLMMessage: LLMMessage representation of this str or ChatDocument.\n\n    \"\"\"\n    sender_name = None\n    sender_role = Role.USER\n    fun_call = None\n    tool_id = \"\"\n    chat_document_id: str = \"\"\n    if isinstance(message, ChatDocument):\n        content = message.content\n        fun_call = message.function_call\n        if message.metadata.sender == Entity.USER and fun_call is not None:\n            # This may happen when a (parent agent's) LLM generates a\n            # a Function-call, and it ends up being sent to the current task's\n            # LLM (possibly because the function-call is mis-named or has other\n            # issues and couldn't be handled by handler methods).\n            # But a function-call can only be generated by an entity with\n            # Role.ASSISTANT, so we instead put the content of the function-call\n            # in the content of the message.\n            content += \" \" + str(fun_call)\n            fun_call = None\n        sender_name = message.metadata.sender_name\n        tool_ids = message.metadata.tool_ids\n        tool_id = tool_ids[-1] if len(tool_ids) &gt; 0 else \"\"\n        chat_document_id = message.id()\n        if message.metadata.sender == Entity.SYSTEM:\n            sender_role = Role.SYSTEM\n        if (\n            message.metadata.parent is not None\n            and message.metadata.parent.function_call is not None\n        ):\n            sender_role = Role.FUNCTION\n            sender_name = message.metadata.parent.function_call.name\n        elif message.metadata.sender == Entity.LLM:\n            sender_role = Role.ASSISTANT\n    else:\n        # LLM can only respond to text content, so extract it\n        content = message\n\n    return LLMMessage(\n        role=sender_role,\n        tool_id=tool_id,\n        content=content,\n        function_call=fun_call,\n        name=sender_name,\n        chat_document_id=chat_document_id,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>             Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent Attributes:     system_message: system message to include in message sequence          (typically defines role and task of agent).          Used only if <code>task</code> is not specified in the constructor.     user_message: user message to include in message sequence.          Used only if <code>task</code> is not specified in the constructor.     use_tools: whether to use our own ToolMessages mechanism     use_functions_api: whether to use functions native to the LLM API             (e.g. OpenAI's <code>function_call</code> mechanism)</p>"},{"location":"reference/agent/#langroid.agent.ChatAgent","title":"<code>ChatAgent(config=ChatAgentConfig(), task=None)</code>","text":"<p>             Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: ChatAgentConfig = ChatAgentConfig(),\n    task: Optional[List[LLMMessage]] = None,\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    self.config._set_fn_or_tools(self._fn_call_available())\n    self.message_history: List[LLMMessage] = []\n    self.tool_instructions_added: bool = False\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_json_tool_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.task_messages","title":"<code>task_messages: List[LLMMessage]</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/agent/#langroid.agent.ChatAgent.from_id","title":"<code>from_id(id)</code>  <code>staticmethod</code>","text":"<p>Get an agent from its ID Args:     agent_id (str): ID of the agent Returns:     ChatAgent: The agent with the given ID</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@staticmethod\ndef from_id(id: str) -&gt; \"ChatAgent\":\n    \"\"\"\n    Get an agent from its ID\n    Args:\n        agent_id (str): ID of the agent\n    Returns:\n        ChatAgent: The agent with the given ID\n    \"\"\"\n    return cast(ChatAgent, Agent.from_id(id))\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.clone","title":"<code>clone(i=0)</code>","text":"<p>Create i'th clone of this agent, ensuring tool use/handling is cloned. Important: We assume all member variables are in the init method here and in the Agent class. TODO: We are attempting to clone an agent after its state has been changed in possibly many ways. Below is an imperfect solution. Caution advised. Revisit later.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clone(self, i: int = 0) -&gt; \"ChatAgent\":\n    \"\"\"Create i'th clone of this agent, ensuring tool use/handling is cloned.\n    Important: We assume all member variables are in the __init__ method here\n    and in the Agent class.\n    TODO: We are attempting to clone an agent after its state has been\n    changed in possibly many ways. Below is an imperfect solution. Caution advised.\n    Revisit later.\n    \"\"\"\n    agent_cls = type(self)\n    config_copy = copy.deepcopy(self.config)\n    config_copy.name = f\"{config_copy.name}-{i}\"\n    new_agent = agent_cls(config_copy)\n    new_agent.system_tool_instructions = self.system_tool_instructions\n    new_agent.system_json_tool_instructions = self.system_json_tool_instructions\n    new_agent.llm_tools_map = self.llm_tools_map\n    new_agent.llm_functions_map = self.llm_functions_map\n    new_agent.llm_functions_handled = self.llm_functions_handled\n    new_agent.llm_functions_usable = self.llm_functions_usable\n    new_agent.llm_function_force = self.llm_function_force\n    # Caution - we are copying the vector-db, maybe we don't always want this?\n    new_agent.vecdb = self.vecdb\n    new_agent.id = ObjectRegistry.new_id()\n    if self.config.add_to_registry:\n        ObjectRegistry.register_object(new_agent)\n    return new_agent\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2)</code>","text":"<p>Clear the message history, starting at the index <code>start</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2) -&gt; None:\n    \"\"\"\n    Clear the message history, starting at the index `start`\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n    \"\"\"\n    if start &lt; 0:\n        n = len(self.message_history)\n        start = max(0, n + start)\n    dropped = self.message_history[start:]\n    for msg in dropped:\n        # clear out the chat document from the ObjectRegistry\n        ChatDocument.delete_id(msg.chat_document_id)\n    self.message_history = self.message_history[:start]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.json_format_rules","title":"<code>json_format_rules()</code>","text":"<p>Specification of JSON formatting rules, based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def json_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of JSON formatting rules, based on the currently enabled\n    usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"You can ask questions in natural language.\"\n    json_instructions = \"\\n\\n\".join(\n        [\n            msg_cls.json_instructions(tool=self.config.use_tools)\n            for _, msg_cls in enumerate(enabled_classes)\n            if msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ]\n    )\n    # if any of the enabled classes has json_group_instructions, then use that,\n    # else fall back to ToolMessage.json_group_instructions\n    for msg_cls in enabled_classes:\n        if hasattr(msg_cls, \"json_group_instructions\") and callable(\n            getattr(msg_cls, \"json_group_instructions\")\n        ):\n            return msg_cls.json_group_instructions().format(\n                json_instructions=json_instructions\n            )\n    return ToolMessage.json_group_instructions().format(\n        json_instructions=json_instructions\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if (\n            hasattr(msg_cls, \"instructions\")\n            and inspect.ismethod(msg_cls.instructions)\n            and msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ):\n            # example will be shown in json_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_examples())\n            if example != \"\":\n                example = \"EXAMPLES:\\n\" + example\n            class_instructions = msg_cls.instructions()\n            guidance = (\n                \"\"\n                if class_instructions == \"\"\n                else (\"GUIDANCE: \" + class_instructions)\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    n_role_msgs = len([m for m in self.message_history if m.role == role])\n    if n_role_msgs == 0:\n        return None\n    idx = self.nth_message_idx_with_role(role, n_role_msgs)\n    return self.message_history[idx]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.nth_message_idx_with_role","title":"<code>nth_message_idx_with_role(role, n)</code>","text":"<p>Index of <code>n</code>th message in message_history, with specified role. (n is assumed to be 1-based, i.e. 1 is the first message with that role). Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def nth_message_idx_with_role(self, role: Role, n: int) -&gt; int:\n    \"\"\"Index of `n`th message in message_history, with specified role.\n    (n is assumed to be 1-based, i.e. 1 is the first message with that role).\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n\n    if len(indices_with_role) &lt; n:\n        return -1\n    return indices_with_role[n - 1]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): new message to replace with     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): new message to replace with\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to enable, for USE, or HANDLING, or both. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> <code>require_defaults</code> <p>whether to include fields that have default values, in the \"properties\" section of the JSON format instructions. (Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.)</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class to enable,\n            for USE, or HANDLING, or both.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n        require_defaults: whether to include fields that have default values,\n            in the \"properties\" section of the JSON format instructions.\n            (Normally the OpenAI completion API ignores these fields,\n            but the Assistant fn-calling seems to pay attn to these,\n            and if we don't want this, we should set this to False.)\n    \"\"\"\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        if require_recipient:\n            message_class = message_class.require_recipient()\n        request = message_class.default_value(\"request\")\n        llm_function = message_class.llm_function_schema(defaults=include_defaults)\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            self.llm_tools_usable.add(t)\n            self.llm_functions_usable.add(t)\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    # Set tool instructions and JSON format instructions\n    if self.config.use_tools:\n        self.system_json_tool_instructions = self.json_format_rules()\n    self.system_tool_instructions = self.tool_instructions()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.__fields__[\"request\"].default\n    to_remove = [r for r in self.llm_tools_usable if r != request]\n    for r in to_remove:\n        self.llm_tools_usable.discard(r)\n        self.llm_functions_usable.discard(r)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = self.llm_response_messages(hist, output_len)\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    hist, output_len = self._prep_llm_messages(message)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm_response_messages_async(hist, output_len)\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.init_message_history","title":"<code>init_message_history()</code>","text":"<p>Initialize the message history with the system message and user message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_message_history(self) -&gt; None:\n    \"\"\"\n    Initialize the message history with the system message and user message\n    \"\"\"\n    self.message_history = [self._create_system_and_tools_message()]\n    if self.user_message:\n        self.message_history.append(\n            LLMMessage(role=Role.USER, content=self.user_message)\n        )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None)</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            # (Why? b/c the intent of showing a spinner is to \"show progress\",\n            # and we don't need to do that when streaming, since\n            # streaming output already shows progress.)\n            cm = status(\n                \"LLM responding to messages...\",\n                log_if_quiet=False,\n            )\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions, fun_call = self._function_args()\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            functions=functions,\n            function_call=fun_call,\n        )\n    if self.llm.get_stream():\n        self.callbacks.finish_llm_stream(\n            content=str(response),\n            is_tool=self.has_tool_message_attempt(\n                ChatDocument.from_LLMResponse(response, displayed=True),\n            ),\n        )\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    functions: Optional[List[LLMFunctionSpec]] = None\n    fun_call: str | Dict[str, str] = \"none\"\n    if self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\n        functions = [self.llm_functions_map[f] for f in self.llm_functions_usable]\n        fun_call = (\n            \"auto\" if self.llm_function_force is None else self.llm_function_force\n        )\n    assert self.llm is not None\n\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        functions=functions,\n        function_call=fun_call,\n    )\n    if self.llm.get_stream():\n        self.callbacks.finish_llm_stream(\n            content=str(response),\n            is_tool=self.has_tool_message_attempt(\n                ChatDocument.from_LLMResponse(response, displayed=True),\n            ),\n        )\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str): user message\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p> <code>result</code> <code>str</code> <p>example of result of agent method.</p>"},{"location":"reference/agent/#langroid.agent.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with JSON formatting instructions. Each example can be either: - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or - a tuple (description, ToolMessage instance), where the description is     a natural language \"thought\" that leads to the tool usage,     e.g. (\"I want to find the square of 5\",  SquareTool(num=5))     In some scenarios, including such a description can significantly     enhance reliability of tool use. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\" | Tuple[str, \"ToolMessage\"]]:\n    \"\"\"\n    Examples to use in few-shot demos with JSON formatting instructions.\n    Each example can be either:\n    - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or\n    - a tuple (description, ToolMessage instance), where the description is\n        a natural language \"thought\" that leads to the tool usage,\n        e.g. (\"I want to find the square of 5\",  SquareTool(num=5))\n        In some scenarios, including such a description can significantly\n        enhance reliability of tool use.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.usage_examples","title":"<code>usage_examples(random=False)</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing examples of how to use the tool-message.</p> <p>Parameters:</p> Name Type Description Default <code>random</code> <code>bool</code> <p>whether to pick a random example from the list of examples. Set to <code>true</code> when using this to illustrate a dialog between LLM and user. (if false, use ALL examples)</p> <code>False</code> <p>Returns:     str: examples of how to use the tool/function-call</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_examples(cls, random: bool = False) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing examples of how to use the tool-message.\n\n    Args:\n        random (bool): whether to pick a random example from the list of examples.\n            Set to `true` when using this to illustrate a dialog between LLM and\n            user.\n            (if false, use ALL examples)\n    Returns:\n        str: examples of how to use the tool/function-call\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    if random:\n        examples = [choice(cls.examples())]\n    else:\n        examples = cls.examples()\n    examples_jsons = [\n        (\n            f\"EXAMPLE {i}: (THOUGHT: {ex[0]}) =&gt; \\n{ex[1].json_example()}\"\n            if isinstance(ex, tuple)\n            else f\"EXAMPLE {i}:\\n {ex.json_example()}\"\n        )\n        for i, ex in enumerate(examples, 1)\n    ]\n    return \"\\n\\n\".join(examples_jsons)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.json_instructions","title":"<code>json_instructions(tool=False)</code>  <code>classmethod</code>","text":"<p>Default Instructions to the LLM showing how to use the tool/function-call. Works for GPT4 but override this for weaker LLMs if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>bool</code> <p>instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM) (or else it would be for OpenAI Function calls)</p> <code>False</code> <p>Returns:     str: instructions on how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef json_instructions(cls, tool: bool = False) -&gt; str:\n    \"\"\"\n    Default Instructions to the LLM showing how to use the tool/function-call.\n    Works for GPT4 but override this for weaker LLMs if needed.\n\n    Args:\n        tool: instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM)\n            (or else it would be for OpenAI Function calls)\n    Returns:\n        str: instructions on how to use the message\n    \"\"\"\n    # TODO: when we attempt to use a \"simpler schema\"\n    # (i.e. all nested fields explicit without definitions),\n    # we seem to get worse results, so we turn it off for now\n    param_dict = (\n        # cls.simple_schema() if tool else\n        cls.llm_function_schema(request=True).parameters\n    )\n    examples_str = \"\"\n    if cls.examples():\n        examples_str = \"EXAMPLES:\\n\" + cls.usage_examples()\n    return textwrap.dedent(\n        f\"\"\"\n        TOOL: {cls.default_value(\"request\")}\n        PURPOSE: {cls.default_value(\"purpose\")} \n        JSON FORMAT: {\n            json.dumps(param_dict, indent=4)\n        }\n        {examples_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.json_group_instructions","title":"<code>json_group_instructions()</code>  <code>staticmethod</code>","text":"<p>Template for instructions for a group of tools. Works with GPT4 but override this for weaker LLMs if needed.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@staticmethod\ndef json_group_instructions() -&gt; str:\n    \"\"\"Template for instructions for a group of tools.\n    Works with GPT4 but override this for weaker LLMs if needed.\n    \"\"\"\n    return textwrap.dedent(\n        \"\"\"\n        === ALL AVAILABLE TOOLS and THEIR JSON FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {json_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above JSON format.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False, defaults=True)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <code>defaults</code> <code>bool</code> <p>whether to include fields with default values in the schema,     in the \"properties\" section.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(\n    cls,\n    request: bool = False,\n    defaults: bool = True,\n) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n        defaults: whether to include fields with default values in the schema,\n                in the \"properties\" section.\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = cls.schema()\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = (\n        [\"result\", \"purpose\"] if request else [\"request\", \"result\", \"purpose\"]\n    )\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes and (defaults or details.get(\"default\") is None)\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\"),\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.simple_schema","title":"<code>simple_schema()</code>  <code>classmethod</code>","text":"<p>Return a simplified schema for the message, with only the request and required fields. Returns:     Dict[str, Any]: simplified schema</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef simple_schema(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a simplified schema for the message, with only the request and\n    required fields.\n    Returns:\n        Dict[str, Any]: simplified schema\n    \"\"\"\n    schema = generate_simple_schema(cls, exclude=[\"result\", \"purpose\"])\n    return schema\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task","title":"<code>Task(agent=None, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=True, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False, allow_null_result=False, max_stalled_steps=5, done_if_no_response=[], done_if_response=[], config=TaskConfig(), **kwargs)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>Whether to delegate \"control\" to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve, and decides when a task is done. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity). See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by \"controller\" (i.e. LLM if <code>llm_delegate</code> is true, otherwise USER) and subsequent response by non-controller [When a tool is involved, this will not give intended results. See <code>done_if_response</code>, <code>done_if_no_response</code> below]. termination]. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\". See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true, resets the agent's message history at every run.</p> <code>True</code> <code>default_human_response</code> <code>str | None</code> <p>default response from user; useful for testing, to avoid interactive input from user. [Instead of this, setting <code>interactive</code> usually suffices]</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\" Note: When interactive = False, the one exception is when the user is explicitly addressed, via \"@user\" or using RecipientTool, in which case the system will wait for a user response. In other words, use <code>interactive=False</code> when you want a \"largely non-interactive\" run, with the exception of explicit user addressing.</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, when interactive=True, only user can quit the root task (Ignored when interactive=False).</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> <code>allow_null_result</code> <code>bool</code> <p>If true, create dummy NO_ANSWER response when no valid response is found in a step. Optional, default is False. Note: In non-interactive mode, when this is set to True, you can have a situation where an LLM generates (non-tool) text, and no other responders have valid responses, and a \"Null result\" is inserted as a dummy response from the User entity, so the LLM will now respond to this Null result, and this will continue until the LLM emits a DONE signal (if instructed to do so), otherwise langroid detects a potential infinite loop after a certain number of such steps (= <code>TaskConfig.inf_loop_wait_factor</code>) and will raise an InfiniteLoopException.</p> <code>False</code> <code>max_stalled_steps</code> <code>int</code> <p>task considered done after this many consecutive steps with no progress. Default is 3.</p> <code>5</code> <code>done_if_no_response</code> <code>List[Responder]</code> <p>consider task done if NULL response from any of these responders. Default is empty list.</p> <code>[]</code> <code>done_if_response</code> <code>List[Responder]</code> <p>consider task done if NON-NULL response from any of these responders. Default is empty list.</p> <code>[]</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Optional[Agent] = None,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = True,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n    allow_null_result: bool = False,\n    max_stalled_steps: int = 5,\n    done_if_no_response: List[Responder] = [],\n    done_if_response: List[Responder] = [],\n    config: TaskConfig = TaskConfig(),\n    **kwargs: Any,  # catch-all for any legacy params, for backwards compatibility\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool):\n            Whether to delegate \"control\" to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve, and decides when a task is done.\n            The \"controlling entity\" is either the LLM or the USER.\n            (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        single_round (bool):\n            If true, task runs until one message by \"controller\"\n            (i.e. LLM if `llm_delegate` is true, otherwise USER)\n            and subsequent response by non-controller [When a tool is involved,\n            this will not give intended results. See `done_if_response`,\n            `done_if_no_response` below].\n            termination]. If false, runs for the specified number of turns in\n            `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true, resets the agent's message history *at every run*.\n        default_human_response (str|None): default response from user; useful for\n            testing, to avoid interactive input from user.\n            [Instead of this, setting `interactive` usually suffices]\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n            Note: When interactive = False, the one exception is when the user\n            is explicitly addressed, via \"@user\" or using RecipientTool, in which\n            case the system will wait for a user response. In other words, use\n            `interactive=False` when you want a \"largely non-interactive\"\n            run, with the exception of explicit user addressing.\n        only_user_quits_root (bool): if true, when interactive=True, only user can\n            quit the root task (Ignored when interactive=False).\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n        allow_null_result (bool):\n            If true, create dummy NO_ANSWER response when no valid response is found\n            in a step.\n            Optional, default is False.\n            *Note:* In non-interactive mode, when this is set to True,\n            you can have a situation where an LLM generates (non-tool) text,\n            and no other responders have valid responses, and a \"Null result\"\n            is inserted as a dummy response from the User entity, so the LLM\n            will now respond to this Null result, and this will continue\n            until the LLM emits a DONE signal (if instructed to do so),\n            otherwise langroid detects a potential infinite loop after\n            a certain number of such steps (= `TaskConfig.inf_loop_wait_factor`)\n            and will raise an InfiniteLoopException.\n        max_stalled_steps (int): task considered done after this many consecutive\n            steps with no progress. Default is 3.\n        done_if_no_response (List[Responder]): consider task done if NULL\n            response from any of these responders. Default is empty list.\n        done_if_response (List[Responder]): consider task done if NON-NULL\n            response from any of these responders. Default is empty list.\n    \"\"\"\n    if agent is None:\n        agent = ChatAgent()\n    self.callbacks = SimpleNamespace(\n        show_subtask_response=noop_fn,\n        set_parent_agent=noop_fn,\n    )\n    self.config = config\n    # how to behave as a sub-task; can be overriden by `add_sub_task()`\n    self.config_sub_task = copy.deepcopy(config)\n    # counts of distinct pending messages in history,\n    # to help detect (exact) infinite loops\n    self.message_counter: Counter[str] = Counter()\n    self._init_message_counter()\n\n    self.history: Deque[str] = deque(\n        maxlen=self.config.inf_loop_cycle_len * self.config.inf_loop_wait_factor\n    )\n    # copy the agent's config, so that we don't modify the original agent's config,\n    # which may be shared by other agents.\n    try:\n        config_copy = copy.deepcopy(agent.config)\n        agent.config = config_copy\n    except Exception:\n        logger.warning(\n            \"\"\"\n            Failed to deep-copy Agent config during task creation, \n            proceeding with original config. Be aware that changes to \n            the config may affect other agents using the same config.\n            \"\"\"\n        )\n    self.restart = restart\n    agent = cast(ChatAgent, agent)\n    self.agent: ChatAgent = agent\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        self.agent.clear_history(0)\n        self.agent.clear_dialog()\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            self.agent.set_system_message(system_message)\n        if user_message:\n            self.agent.set_user_message(user_message)\n    self.max_cost: float = 0\n    self.max_tokens: int = 0\n    self.session_id: str = \"\"\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.color_log: bool = False if settings.notebook else True\n\n    self.n_stalled_steps = 0  # how many consecutive steps with no progress?\n    # how many 2-step-apart alternations of no_answer step-result have we had,\n    # i.e. x1, N/A, x2, N/A, x3, N/A ...\n    self.n_no_answer_alternations = 0\n    self._no_answer_step: int = -5\n    self._step_idx = -1  # current step index\n    self.max_stalled_steps = max_stalled_steps\n    self.done_if_response = [r.value for r in done_if_response]\n    self.done_if_no_response = [r.value for r in done_if_no_response]\n    self.is_done = False  # is task done (based on response)?\n    self.is_pass_thru = False  # is current response a pass-thru?\n    if name:\n        # task name overrides name in agent config\n        agent.config.name = name\n    self.name = name or agent.config.name\n    self.value: str = self.name\n\n    self.default_human_response = default_human_response\n    if default_human_response is not None:\n        # only override agent's default_human_response if it is explicitly set\n        self.agent.default_human_response = default_human_response\n    self.interactive = interactive\n    self.agent.interactive = interactive\n    self.only_user_quits_root = only_user_quits_root\n    self.message_history_idx = -1\n\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n    self.allow_null_result = allow_null_result\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    if llm_delegate:\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM (as the Controller) asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        if self.single_round:\n            # 0: User (as Controller) asks,\n            # 1: LLM replies.\n            self.turns = 1\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n    agent: ChatAgent = self.agent.clone(i)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=self.restart,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        erase_substeps=self.erase_substeps,\n        allow_null_result=self.allow_null_result,\n        max_stalled_steps=self.max_stalled_steps,\n        done_if_no_response=[Entity(s) for s in self.done_if_no_response],\n        done_if_response=[Entity(s) for s in self.done_if_response],\n        config=self.config,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.kill_session","title":"<code>kill_session(session_id='')</code>  <code>classmethod</code>","text":"<p>Kill the session with the given session_id.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>@classmethod\ndef kill_session(cls, session_id: str = \"\") -&gt; None:\n    \"\"\"\n    Kill the session with the given session_id.\n    \"\"\"\n    session_id_kill_key = f\"{session_id}:kill\"\n    cls.cache().store(session_id_kill_key, \"1\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.kill","title":"<code>kill()</code>","text":"<p>Kill the task run associated with the current session.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def kill(self) -&gt; None:\n    \"\"\"\n    Kill the task run associated with the current session.\n    \"\"\"\n    self._cache_session_store(\"kill\", \"1\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response (unless a sub-task is explicitly addressed).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]</code> <p>A task, or list of tasks, or a tuple of task and task config, or a list of tuples of task and task config. These tasks are added as sub-tasks of the current task. The task configs (if any) dictate how the tasks are run when invoked as sub-tasks of other tasks. This allows users to specify behavior applicable only in the context of a particular task-subtask combination.</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(\n    self,\n    task: (\n        Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response (unless a sub-task is explicitly addressed).\n\n    Args:\n        task: A task, or list of tasks, or a tuple of task and task config,\n            or a list of tuples of task and task config.\n            These tasks are added as sub-tasks of the current task.\n            The task configs (if any) dictate how the tasks are run when\n            invoked as sub-tasks of other tasks. This allows users to specify\n            behavior applicable only in the context of a particular task-subtask\n            combination.\n    \"\"\"\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n\n    if isinstance(task, tuple):\n        task, config = task\n    else:\n        config = TaskConfig()\n    task.config_sub_task = config\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    elif msg is None and len(self.agent.message_history) &gt; 1:\n        # if agent has a history beyond system msg, set the\n        # pending message to the ChatDocument linked from\n        # last message in the history\n        last_agent_msg = self.agent.message_history[-1]\n        self.pending_message = ChatDocument.from_id(last_agent_msg.chat_document_id)\n        if self.pending_message is not None:\n            self.pending_sender = self.pending_message.metadata.sender\n    else:\n        if isinstance(msg, ChatDocument):\n            # carefully deep-copy: fresh metadata.id, register\n            # as new obj in registry\n            self.pending_message = ChatDocument.deepcopy(msg)\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n            # update parent, child, agent pointers\n            if msg is not None:\n                msg.metadata.child_id = self.pending_message.metadata.id\n                self.pending_message.metadata.parent_id = msg.metadata.id\n            self.pending_message.metadata.agent_id = self.agent.id\n\n    self._show_pending_message_if_debug()\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    else:\n        self.logger = RichFileLogger(\n            str(Path(self.config.logs_dir) / f\"{self.name}.log\"),\n            color=self.color_log,\n        )\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    else:\n        self.tsv_logger = setup_file_logger(\n            \"tsv_logger\",\n            str(Path(self.config.logs_dir) / f\"{self.name}.tsv\"),\n        )\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    self.log_message(Entity.USER, self.pending_message)\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.reset_all_sub_tasks","title":"<code>reset_all_sub_tasks()</code>","text":"<p>Recursively reset message history of own agent and all sub-tasks</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def reset_all_sub_tasks(self) -&gt; None:\n    \"\"\"Recursively reset message history of own agent and all sub-tasks\"\"\"\n    self.agent.clear_history(0)\n    self.agent.clear_dialog()\n    for t in self.sub_tasks:\n        t.reset_all_sub_tasks()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.run","title":"<code>run(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='')</code>","text":"<p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n    if (self.restart and caller is None) or (\n        self.config_sub_task.restart_as_subtask and caller is not None\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    assert (\n        msg is None or isinstance(msg, str) or isinstance(msg, ChatDocument)\n    ), f\"msg arg in Task.run() must be None, str, or ChatDocument, not {type(msg)}\"\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        self.step()\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='')</code>  <code>async</code>","text":"<p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>initial user-role message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User). Note that <code>msg</code>, if passed, is treated as message with role <code>user</code>; a \"system\" role message should not be passed here.</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <code>max_cost</code> <code>float</code> <p>max cost allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>max tokens allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>session_id</code> <code>str</code> <p>session id for the task</p> <code>''</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (str|ChatDocument): initial *user-role* message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User). Note that `msg`, if passed, is treated as\n            message with role `user`; a \"system\" role message should not be\n            passed here.\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n        max_cost (float): max cost allowed for the task (default 0 -&gt; no limit)\n        max_tokens (int): max tokens allowed for the task (default 0 -&gt; no limit)\n        session_id (str): session id for the task\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if (\n        self.restart\n        and caller is None\n        or self.config_sub_task.restart_as_subtask\n        and caller is not None\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=True,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        await self.step_async()\n        await asyncio.sleep(0.01)  # temp yield to avoid blocking\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details. TODO: Except for the self.response() calls, this fn should be identical to <code>step_async()</code>. Consider refactoring to avoid duplication.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    TODO: Except for the self.response() calls, this fn should be identical to\n    `step_async()`. Consider refactoring to avoid duplication.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # When in interactive mode,\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:  # did not find a valid response\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        result = e.run(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n            max_cost=self.max_cost,\n            max_tokens=self.max_tokens,\n        )\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\" if result is None else str(ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n    return self._process_result_routing(result)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        result = await e.run_async(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n            max_cost=self.max_cost,\n            max_tokens=self.max_tokens,\n        )\n        result_str = str(ChatDocument.to_LLMMessage(result))\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n    return self._process_result_routing(result)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.result","title":"<code>result(status=None)</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Note the result of a task is returned as if it is from the User entity.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusCode</code> <p>status of the task when it ended</p> <code>None</code> <p>Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self, status: StatusCode | None = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n\n    Note the result of a task is returned as if it is from the User entity.\n\n    Args:\n        status (StatusCode): status of the task when it ended\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    if status in [StatusCode.STALLED, StatusCode.MAX_TURNS, StatusCode.INF_LOOP]:\n        # In these case we don't know (and don't want to try to guess)\n        # what the task result should be, so we return None\n        return None\n\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    if DONE in content:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    fun_call = result_msg.function_call if result_msg else None\n    tool_messages = result_msg.tool_messages if result_msg else []\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else \"\"\n    tool_ids = result_msg.metadata.tool_ids if result_msg else []\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    result_doc = ChatDocument(\n        content=content,\n        function_call=fun_call,\n        tool_messages=tool_messages,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            status=status or (result_msg.metadata.status if result_msg else None),\n            sender_name=self.name,\n            recipient=recipient,\n            tool_ids=tool_ids,\n            parent_id=result_msg.id() if result_msg else \"\",\n            agent_id=str(self.agent.id),\n        ),\n    )\n    if self.pending_message is not None:\n        self.pending_message.metadata.child_id = result_doc.id()\n\n    return result_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.done","title":"<code>done(result=None, r=None)</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Args:     result (ChatDocument|None): result from a responder     r (Responder|None): responder that produced the result         Not used here, but could be used by derived classes. Returns:     bool: True if task is done, False otherwise     StatusCode: status code indicating why task is done</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(\n    self, result: ChatDocument | None = None, r: Responder | None = None\n) -&gt; Tuple[bool, StatusCode]:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Args:\n        result (ChatDocument|None): result from a responder\n        r (Responder|None): responder that produced the result\n            Not used here, but could be used by derived classes.\n    Returns:\n        bool: True if task is done, False otherwise\n        StatusCode: status code indicating why task is done\n    \"\"\"\n    if self._is_kill():\n        return (True, StatusCode.KILL)\n    result = result or self.pending_message\n    user_quit = (\n        result is not None\n        and (result.content in USER_QUIT_STRINGS or DONE in result.content)\n        and result.metadata.sender == Entity.USER\n    )\n    if self._level == 0 and self.interactive and self.only_user_quits_root:\n        # for top-level task, in interactive mode, only user can quit out\n        return (user_quit, StatusCode.USER_QUIT if user_quit else StatusCode.OK)\n\n    if self.is_done:\n        return (True, StatusCode.DONE)\n\n    if self.n_stalled_steps &gt;= self.max_stalled_steps:\n        # we are stuck, so bail to avoid infinite loop\n        logger.warning(\n            f\"Task {self.name} stuck for {self.max_stalled_steps} steps; exiting.\"\n        )\n        return (True, StatusCode.STALLED)\n\n    if self.max_cost &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[1] &gt; self.max_cost:\n                logger.warning(\n                    f\"Task {self.name} cost exceeded {self.max_cost}; exiting.\"\n                )\n                return (True, StatusCode.MAX_COST)\n        except Exception:\n            pass\n\n    if self.max_tokens &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[0] &gt; self.max_tokens:\n                logger.warning(\n                    f\"Task {self.name} uses &gt; {self.max_tokens} tokens; exiting.\"\n                )\n                return (True, StatusCode.MAX_TOKENS)\n        except Exception:\n            pass\n    final = (\n        # no valid response from any entity/agent in current turn\n        result is None\n        # An entity decided task is done\n        or DONE in result.content\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and result.metadata.recipient == self.caller.name\n        )\n        or user_quit\n    )\n    return (final, StatusCode.OK)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.valid","title":"<code>valid(result, r)</code>","text":"<p>Is the result from a Responder (i.e. an entity or sub-task) such that we can stop searching for responses in this step?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(\n    self,\n    result: Optional[ChatDocument],\n    r: Responder,\n) -&gt; bool:\n    \"\"\"\n    Is the result from a Responder (i.e. an entity or sub-task)\n    such that we can stop searching for responses in this step?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n\n    # if task would be considered done given responder r's `result`,\n    # then consider the result valid.\n    if result is not None and self.done(result, r)[0]:\n        return True\n    return (\n        result is not None\n        and not self._is_empty_message(result)\n        # some weaker LLMs, including even GPT-4o, may say \"DO-NOT-KNOW.\"\n        # (with a punctuation at the end), so need to strip out punctuation\n        and re.sub(r\"[,.!?:]\", \"\", result.content.strip()) != NO_ANSWER\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    default_values = ChatDocLoggerFields().dict().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/agent/base/","title":"base","text":"<p>langroid/agent/base.py </p>"},{"location":"reference/agent/base/#langroid.agent.base.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent","title":"<code>Agent(config=AgentConfig())</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An Agent is an abstraction that encapsulates mainly two components:</p> <ul> <li>a language model (LLM)</li> <li>a vector store (vecdb)</li> </ul> <p>plus associated components such as a parser, and variables that hold information about any tool/function-calling messages that have been defined.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig = AgentConfig()):\n    self.config = config\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.interactive: bool | None = None\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    if config.parsing is not None and self.config.llm is not None:\n        # token_encoding_model is used to obtain the tokenizer,\n        # so in case it's an OpenAI model, we ensure that the tokenizer\n        # corresponding to the model is used.\n        if isinstance(self.llm, OpenAIGPT) and self.llm.is_openai_chat_model():\n            config.parsing.token_encoding_model = self.llm.config.chat_model\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n    if config.add_to_registry:\n        ObjectRegistry.register_object(self)\n\n    self.callbacks = SimpleNamespace(\n        start_llm_stream=lambda: noop_fn,\n        cancel_llm_stream=noop_fn,\n        finish_llm_stream=noop_fn,\n        show_llm_response=noop_fn,\n        show_agent_response=noop_fn,\n        get_user_response=None,\n        get_last_step=noop_fn,\n        set_parent_agent=noop_fn,\n        show_error_message=noop_fn,\n        show_start_response=noop_fn,\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.indent","title":"<code>indent: str</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_examples(random=True)  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.create_agent_response","title":"<code>create_agent_response(content=None)</code>","text":"<p>Template for agent_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_agent_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for agent_response.\"\"\"\n    return self._response_template(Entity.AGENT, content)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n    if results is None:\n        return None\n    if isinstance(results, ChatDocument):\n        # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n        results.metadata.tool_ids = (\n            [] if isinstance(msg, str) else msg.metadata.tool_ids\n        )\n        return results\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {escape(results)}\")\n        maybe_json = len(extract_top_level_json(results)) &gt; 0\n        self.callbacks.show_agent_response(\n            content=results,\n            language=\"json\" if maybe_json else \"text\",\n        )\n    sender_name = self.config.name\n    if isinstance(msg, ChatDocument) and msg.function_call is not None:\n        # if result was from handling an LLM `function_call`,\n        # set sender_name to \"request\", i.e. name of the function_call\n        sender_name = msg.function_call.name\n\n    return ChatDocument(\n        content=results,\n        metadata=ChatDocMetaData(\n            source=Entity.AGENT,\n            sender=Entity.AGENT,\n            sender_name=sender_name,\n            # preserve trail of tool_ids for OpenAI Assistant fn-calls\n            tool_ids=[] if isinstance(msg, str) else msg.metadata.tool_ids,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.create_user_response","title":"<code>create_user_response(content=None)</code>","text":"<p>Template for user_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_user_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for user_response.\"\"\"\n    return self._response_template(Entity.USER, content)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n\n    # When msg explicitly addressed to user, this means an actual human response\n    # is being sought.\n    need_human_response = (\n        isinstance(msg, ChatDocument) and msg.metadata.recipient == Entity.USER\n    )\n\n    interactive = self.interactive or settings.interactive\n\n    if not interactive and not need_human_response:\n        return None\n    elif self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if self.callbacks.get_user_response is not None:\n            # ask user with empty prompt: no need for prompt\n            # since user has seen the conversation so far.\n            # But non-empty prompt can be useful when Agent\n            # uses a tool that requires user input, or in other scenarios.\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}Human \"\n                \"(respond or q, x to exit current level, \"\n                f\"or hit enter to continue)\\n{self.indent}\",\n            ).strip()\n\n    tool_ids = []\n    if msg is not None and isinstance(msg, ChatDocument):\n        tool_ids = msg.metadata.tool_ids\n    # only return non-None result if user_msg not empty\n    if not user_msg:\n        return None\n    else:\n        if user_msg.startswith(\"SYSTEM\"):\n            user_msg = user_msg[6:].strip()\n            source = Entity.SYSTEM\n            sender = Entity.SYSTEM\n        else:\n            source = Entity.USER\n            sender = Entity.USER\n        return ChatDocument(\n            content=user_msg,\n            metadata=ChatDocMetaData(\n                source=source,\n                sender=sender,\n                # preserve trail of tool_ids for OpenAI Assistant fn-calls\n                tool_ids=tool_ids,\n            ),\n        )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if message is not None and len(self.get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.create_llm_response","title":"<code>create_llm_response(content=None)</code>","text":"<p>Template for llm_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_llm_response(self, content: str | None = None) -&gt; ChatDocument:\n    \"\"\"Template for llm_response.\"\"\"\n    return self._response_template(Entity.LLM, content)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response_async","title":"<code>llm_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    output_len = self.config.llm.max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM. \n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + escape(response.message))\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = [] if isinstance(msg, str) else msg.metadata.tool_ids\n    return cdoc\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response","title":"<code>llm_response(msg=None)</code>","text":"<p>LLM response to a prompt. Args:     msg (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        msg (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM. \n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + escape(response.message))\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = [] if isinstance(msg, str) else msg.metadata.tool_ids\n    return cdoc\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.has_tool_message_attempt","title":"<code>has_tool_message_attempt(msg)</code>","text":"<p>Check whether msg contains a Tool/fn-call attempt (by the LLM)</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_tool_message_attempt(self, msg: str | ChatDocument | None) -&gt; bool:\n    \"\"\"Check whether msg contains a Tool/fn-call attempt (by the LLM)\"\"\"\n    if msg is None:\n        return False\n    try:\n        tools = self.get_tool_messages(msg)\n        return len(tools) &gt; 0\n    except ValidationError:\n        # there is a tool/fn-call attempt but had a validation error,\n        # so we still consider this a tool message \"attempt\"\n        return True\n    return False\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_json_tool_messages","title":"<code>get_json_tool_messages(input_str)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    json_substrings = extract_top_level_json(input_str)\n    if len(json_substrings) == 0:\n        return []\n    results = [self._get_one_tool_message(j) for j in json_substrings]\n    return [r for r in results if r is not None]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.tool_validation_error","title":"<code>tool_validation_error(ve)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     tool (ToolMessage): The tool message that failed validation     ve (ValidationError): The exception raised</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(self, ve: ValidationError) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        tool (ToolMessage): The tool message that failed validation\n        ve (ValidationError): The exception raised\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    tool_name = cast(ToolMessage, ve.model).default_value(\"request\")\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc']}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the \n    TOOL or function_call named '{tool_name}': \n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | ChatDocument</code> <p>Optional[Str]: The result of the handler method in string form so it can</p> <code>None | str | ChatDocument</code> <p>be sent back to the LLM, or None if <code>msg</code> was not successfully</p> <code>None | str | ChatDocument</code> <p>handled by a method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(self, msg: str | ChatDocument) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        Optional[Str]: The result of the handler method in string form so it can\n        be sent back to the LLM, or None if `msg` was not successfully\n        handled by a method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        return self.handle_message_fallback(msg)\n\n    results = [self.handle_tool_message(t) for t in tools]\n\n    results_list = [r for r in results if r is not None]\n    if len(results_list) == 0:\n        return None  # self.handle_message_fallback(msg)\n    # there was a non-None result\n    chat_doc_results = [r for r in results_list if isinstance(r, ChatDocument)]\n    if len(chat_doc_results) &gt; 1:\n        logger.warning(\n            \"\"\"There were multiple ChatDocument results from tools,\n            which is unexpected. The first one will be returned, and the others\n            will be ignored.\n            \"\"\"\n        )\n    if len(chat_doc_results) &gt; 0:\n        return chat_doc_results[0]\n\n    str_doc_results = [r for r in results_list if isinstance(r, str)]\n    final = \"\\n\".join(str_doc_results)\n    return final\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method to handle possible \"tool\" msg if no other method applies or if an error is thrown. This method can be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     str: The result of the handler method in string form so it can         be sent back to the LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Fallback method to handle possible \"tool\" msg if no other method applies\n    or if an error is thrown.\n    This method can be overridden by subclasses.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        str: The result of the handler method in string form so it can\n            be sent back to the LLM.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_tool_message","title":"<code>handle_tool_message(tool)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(self, tool: ToolMessage) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    handler_method = getattr(self, tool_name, None)\n    if handler_method is None:\n        return None\n\n    try:\n        result = handler_method(tool)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return result  # type: ignore\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields).the usage memebr It updates the cost after checking the cache and updates the tokens (prompts and completion) if the response stream is True, because OpenAI doesn't returns these fields.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields).the usage memebr\n    It updates the cost after checking the cache and updates the\n    tokens (prompts and completion) if the response stream is True, because OpenAI\n    doesn't returns these fields.\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/agent/batch/","title":"batch","text":"<p>langroid/agent/batch.py </p>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_task_gen","title":"<code>run_batch_task_gen(gen_task, items, input_map=lambda x: str(x), output_map=lambda x: x, sequential=True, batch_size=None, turns=-1, message=None, handle_exceptions=False, max_cost=0.0, max_tokens=0)</code>","text":"<p>Generate and run copies of a task async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     gen_task (Callable[[int], Task]): generates the tasks to run     items (list[T]): list of items to process     input_map (Callable[[T], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], U]): function to map result         to final result     sequential (bool): whether to run sequentially         (e.g. some APIs such as ooba don't support concurrent requests)     batch_size (Optional[int]): The number of tasks to run at a time,         if None, unbatched     turns (int): number of turns to run, -1 for infinite     message (Optional[str]): optionally overrides the console status messages     handle_exceptions: bool: Whether to replace exceptions with outputs of None     max_cost: float: maximum cost to run the task (default 0.0 for unlimited)     max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)</p> <p>Returns:</p> Type Description <code>list[U]</code> <p>list[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_task_gen(\n    gen_task: Callable[[int], Task],\n    items: list[T],\n    input_map: Callable[[T], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], U] = lambda x: x,  # type: ignore\n    sequential: bool = True,\n    batch_size: Optional[int] = None,\n    turns: int = -1,\n    message: Optional[str] = None,\n    handle_exceptions: bool = False,\n    max_cost: float = 0.0,\n    max_tokens: int = 0,\n) -&gt; list[U]:\n    \"\"\"\n    Generate and run copies of a task async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        gen_task (Callable[[int], Task]): generates the tasks to run\n        items (list[T]): list of items to process\n        input_map (Callable[[T], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], U]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        batch_size (Optional[int]): The number of tasks to run at a time,\n            if None, unbatched\n        turns (int): number of turns to run, -1 for infinite\n        message (Optional[str]): optionally overrides the console status messages\n        handle_exceptions: bool: Whether to replace exceptions with outputs of None\n        max_cost: float: maximum cost to run the task (default 0.0 for unlimited)\n        max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)\n\n\n    Returns:\n        list[Any]: list of final results\n    \"\"\"\n    inputs = [input_map(item) for item in items]\n\n    async def _do_task(input: str | ChatDocument, i: int) -&gt; Optional[ChatDocument]:\n        task_i = gen_task(i)\n        if task_i.agent.llm is not None:\n            task_i.agent.llm.set_stream(False)\n        task_i.agent.config.show_stats = False\n\n        result = await task_i.run_async(\n            input, turns=turns, max_cost=max_cost, max_tokens=max_tokens\n        )\n        return result\n\n    async def _do_all(\n        inputs: Iterable[str | ChatDocument], start_idx: int = 0\n    ) -&gt; list[U]:\n        results: list[Optional[ChatDocument]] = []\n        if sequential:\n            for i, input in enumerate(inputs):\n                try:\n                    result = await _do_task(input, i + start_idx)\n                except BaseException as e:\n                    if handle_exceptions:\n                        result = None\n                    else:\n                        raise e\n                results.append(result)\n        else:\n            results_with_exceptions = await asyncio.gather(\n                *(_do_task(input, i + start_idx) for i, input in enumerate(inputs)),\n                return_exceptions=handle_exceptions,\n            )\n\n            results = [\n                r if not isinstance(r, BaseException) else None\n                for r in results_with_exceptions\n            ]\n\n        return list(map(output_map, results))\n\n    results: List[U] = []\n    if batch_size is None:\n        msg = message or f\"[bold green]Running {len(items)} tasks:\"\n\n        with status(msg), SuppressLoggerWarnings():\n            results = asyncio.run(_do_all(inputs))\n    else:\n        batches = batched(inputs, batch_size)\n\n        for batch in batches:\n            start_idx = len(results)\n            complete_str = f\", {start_idx} complete\" if start_idx &gt; 0 else \"\"\n            msg = message or f\"[bold green]Running {len(items)} tasks{complete_str}:\"\n\n            with status(msg), SuppressLoggerWarnings():\n                results.extend(asyncio.run(_do_all(batch, start_idx=start_idx)))\n\n    return results\n</code></pre>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_tasks","title":"<code>run_batch_tasks(task, items, input_map=lambda x: str(x), output_map=lambda x: x, sequential=True, batch_size=None, turns=-1, max_cost=0.0, max_tokens=0)</code>","text":"<p>Run copies of <code>task</code> async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     task (Task): task to run     items (list[T]): list of items to process     input_map (Callable[[T], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], U]): function to map result         to final result     sequential (bool): whether to run sequentially         (e.g. some APIs such as ooba don't support concurrent requests)     batch_size (Optional[int]): The number of tasks to run at a time,         if None, unbatched     turns (int): number of turns to run, -1 for infinite     max_cost: float: maximum cost to run the task (default 0.0 for unlimited)     max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)</p> <p>Returns:</p> Type Description <code>List[U]</code> <p>list[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_tasks(\n    task: Task,\n    items: list[T],\n    input_map: Callable[[T], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], U] = lambda x: x,  # type: ignore\n    sequential: bool = True,\n    batch_size: Optional[int] = None,\n    turns: int = -1,\n    max_cost: float = 0.0,\n    max_tokens: int = 0,\n) -&gt; List[U]:\n    \"\"\"\n    Run copies of `task` async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        task (Task): task to run\n        items (list[T]): list of items to process\n        input_map (Callable[[T], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], U]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        batch_size (Optional[int]): The number of tasks to run at a time,\n            if None, unbatched\n        turns (int): number of turns to run, -1 for infinite\n        max_cost: float: maximum cost to run the task (default 0.0 for unlimited)\n        max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)\n\n    Returns:\n        list[Any]: list of final results\n    \"\"\"\n    message = f\"[bold green]Running {len(items)} copies of {task.name}...\"\n    return run_batch_task_gen(\n        lambda i: task.clone(i),\n        items,\n        input_map,\n        output_map,\n        sequential,\n        batch_size,\n        turns,\n        message,\n        max_cost=max_cost,\n        max_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_agent_method","title":"<code>run_batch_agent_method(agent, method, items, input_map=lambda x: str(x), output_map=lambda x: x, sequential=True)</code>","text":"<p>Run the <code>method</code> on copies of <code>agent</code>, async/concurrently one per item in <code>items</code> list. ASSUMPTION: The <code>method</code> is an async method and has signature:     method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None So this would typically be used for the agent's \"responder\" methods, e.g. <code>llm_response_async</code> or <code>agent_responder_async</code>.</p> <p>For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent whose method to run</p> required <code>method</code> <code>str</code> <p>Async method to run on copies of <code>agent</code>. The method is assumed to have signature: <code>method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None</code></p> required <code>input_map</code> <code>Callable[[Any], str | ChatDocument]</code> <p>function to map item to initial message to process</p> <code>lambda x: str(x)</code> <code>output_map</code> <code>Callable[[ChatDocument | str], Any]</code> <p>function to map result to final result</p> <code>lambda x: x</code> <code>sequential</code> <code>bool</code> <p>whether to run sequentially (e.g. some APIs such as ooba don't support concurrent requests)</p> <code>True</code> <p>Returns:     List[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_agent_method(\n    agent: Agent,\n    method: Callable[\n        [str | ChatDocument | None], Coroutine[Any, Any, ChatDocument | None]\n    ],\n    items: List[Any],\n    input_map: Callable[[Any], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], Any] = lambda x: x,\n    sequential: bool = True,\n) -&gt; List[Any]:\n    \"\"\"\n    Run the `method` on copies of `agent`, async/concurrently one per\n    item in `items` list.\n    ASSUMPTION: The `method` is an async method and has signature:\n        method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None\n    So this would typically be used for the agent's \"responder\" methods,\n    e.g. `llm_response_async` or `agent_responder_async`.\n\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n\n    Args:\n        agent (Agent): agent whose method to run\n        method (str): Async method to run on copies of `agent`.\n            The method is assumed to have signature:\n            `method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None`\n        input_map (Callable[[Any], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], Any]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n    Returns:\n        List[Any]: list of final results\n    \"\"\"\n    # Check if the method is async\n    method_name = method.__name__\n    if not inspect.iscoroutinefunction(method):\n        raise ValueError(f\"The method {method_name} is not async.\")\n\n    inputs = [input_map(item) for item in items]\n    agent_cfg = copy.deepcopy(agent.config)\n    assert agent_cfg.llm is not None, \"agent must have llm config\"\n    agent_cfg.llm.stream = False\n    agent_cfg.show_stats = False\n    agent_cls = type(agent)\n    agent_name = agent_cfg.name\n\n    async def _do_task(input: str | ChatDocument, i: int) -&gt; Any:\n        agent_cfg.name = f\"{agent_cfg.name}-{i}\"\n        agent_i = agent_cls(agent_cfg)\n        method_i = getattr(agent_i, method_name, None)\n        if method_i is None:\n            raise ValueError(f\"Agent {agent_name} has no method {method_name}\")\n        result = await method_i(input)\n        return output_map(result)\n\n    async def _do_all() -&gt; List[Any]:\n        if sequential:\n            results = []\n            for i, input in enumerate(inputs):\n                result = await _do_task(input, i)\n                results.append(result)\n            return results\n        with quiet_mode(), SuppressLoggerWarnings():\n            return await asyncio.gather(\n                *(_do_task(input, i) for i, input in enumerate(inputs))\n            )\n\n    n = len(items)\n    with status(f\"[bold green]Running {n} copies of {agent_name}...\"):\n        results = asyncio.run(_do_all())\n\n    return results\n</code></pre>"},{"location":"reference/agent/chat_agent/","title":"chat_agent","text":"<p>langroid/agent/chat_agent.py </p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>             Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent Attributes:     system_message: system message to include in message sequence          (typically defines role and task of agent).          Used only if <code>task</code> is not specified in the constructor.     user_message: user message to include in message sequence.          Used only if <code>task</code> is not specified in the constructor.     use_tools: whether to use our own ToolMessages mechanism     use_functions_api: whether to use functions native to the LLM API             (e.g. OpenAI's <code>function_call</code> mechanism)</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent","title":"<code>ChatAgent(config=ChatAgentConfig(), task=None)</code>","text":"<p>             Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: ChatAgentConfig = ChatAgentConfig(),\n    task: Optional[List[LLMMessage]] = None,\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    self.config._set_fn_or_tools(self._fn_call_available())\n    self.message_history: List[LLMMessage] = []\n    self.tool_instructions_added: bool = False\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_json_tool_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.task_messages","title":"<code>task_messages: List[LLMMessage]</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.from_id","title":"<code>from_id(id)</code>  <code>staticmethod</code>","text":"<p>Get an agent from its ID Args:     agent_id (str): ID of the agent Returns:     ChatAgent: The agent with the given ID</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@staticmethod\ndef from_id(id: str) -&gt; \"ChatAgent\":\n    \"\"\"\n    Get an agent from its ID\n    Args:\n        agent_id (str): ID of the agent\n    Returns:\n        ChatAgent: The agent with the given ID\n    \"\"\"\n    return cast(ChatAgent, Agent.from_id(id))\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clone","title":"<code>clone(i=0)</code>","text":"<p>Create i'th clone of this agent, ensuring tool use/handling is cloned. Important: We assume all member variables are in the init method here and in the Agent class. TODO: We are attempting to clone an agent after its state has been changed in possibly many ways. Below is an imperfect solution. Caution advised. Revisit later.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clone(self, i: int = 0) -&gt; \"ChatAgent\":\n    \"\"\"Create i'th clone of this agent, ensuring tool use/handling is cloned.\n    Important: We assume all member variables are in the __init__ method here\n    and in the Agent class.\n    TODO: We are attempting to clone an agent after its state has been\n    changed in possibly many ways. Below is an imperfect solution. Caution advised.\n    Revisit later.\n    \"\"\"\n    agent_cls = type(self)\n    config_copy = copy.deepcopy(self.config)\n    config_copy.name = f\"{config_copy.name}-{i}\"\n    new_agent = agent_cls(config_copy)\n    new_agent.system_tool_instructions = self.system_tool_instructions\n    new_agent.system_json_tool_instructions = self.system_json_tool_instructions\n    new_agent.llm_tools_map = self.llm_tools_map\n    new_agent.llm_functions_map = self.llm_functions_map\n    new_agent.llm_functions_handled = self.llm_functions_handled\n    new_agent.llm_functions_usable = self.llm_functions_usable\n    new_agent.llm_function_force = self.llm_function_force\n    # Caution - we are copying the vector-db, maybe we don't always want this?\n    new_agent.vecdb = self.vecdb\n    new_agent.id = ObjectRegistry.new_id()\n    if self.config.add_to_registry:\n        ObjectRegistry.register_object(new_agent)\n    return new_agent\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2)</code>","text":"<p>Clear the message history, starting at the index <code>start</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2) -&gt; None:\n    \"\"\"\n    Clear the message history, starting at the index `start`\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n    \"\"\"\n    if start &lt; 0:\n        n = len(self.message_history)\n        start = max(0, n + start)\n    dropped = self.message_history[start:]\n    for msg in dropped:\n        # clear out the chat document from the ObjectRegistry\n        ChatDocument.delete_id(msg.chat_document_id)\n    self.message_history = self.message_history[:start]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.json_format_rules","title":"<code>json_format_rules()</code>","text":"<p>Specification of JSON formatting rules, based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def json_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of JSON formatting rules, based on the currently enabled\n    usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"You can ask questions in natural language.\"\n    json_instructions = \"\\n\\n\".join(\n        [\n            msg_cls.json_instructions(tool=self.config.use_tools)\n            for _, msg_cls in enumerate(enabled_classes)\n            if msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ]\n    )\n    # if any of the enabled classes has json_group_instructions, then use that,\n    # else fall back to ToolMessage.json_group_instructions\n    for msg_cls in enabled_classes:\n        if hasattr(msg_cls, \"json_group_instructions\") and callable(\n            getattr(msg_cls, \"json_group_instructions\")\n        ):\n            return msg_cls.json_group_instructions().format(\n                json_instructions=json_instructions\n            )\n    return ToolMessage.json_group_instructions().format(\n        json_instructions=json_instructions\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if (\n            hasattr(msg_cls, \"instructions\")\n            and inspect.ismethod(msg_cls.instructions)\n            and msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ):\n            # example will be shown in json_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_examples())\n            if example != \"\":\n                example = \"EXAMPLES:\\n\" + example\n            class_instructions = msg_cls.instructions()\n            guidance = (\n                \"\"\n                if class_instructions == \"\"\n                else (\"GUIDANCE: \" + class_instructions)\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    n_role_msgs = len([m for m in self.message_history if m.role == role])\n    if n_role_msgs == 0:\n        return None\n    idx = self.nth_message_idx_with_role(role, n_role_msgs)\n    return self.message_history[idx]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.nth_message_idx_with_role","title":"<code>nth_message_idx_with_role(role, n)</code>","text":"<p>Index of <code>n</code>th message in message_history, with specified role. (n is assumed to be 1-based, i.e. 1 is the first message with that role). Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def nth_message_idx_with_role(self, role: Role, n: int) -&gt; int:\n    \"\"\"Index of `n`th message in message_history, with specified role.\n    (n is assumed to be 1-based, i.e. 1 is the first message with that role).\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n\n    if len(indices_with_role) &lt; n:\n        return -1\n    return indices_with_role[n - 1]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): new message to replace with     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): new message to replace with\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to enable, for USE, or HANDLING, or both. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> <code>require_defaults</code> <p>whether to include fields that have default values, in the \"properties\" section of the JSON format instructions. (Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.)</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class to enable,\n            for USE, or HANDLING, or both.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n        require_defaults: whether to include fields that have default values,\n            in the \"properties\" section of the JSON format instructions.\n            (Normally the OpenAI completion API ignores these fields,\n            but the Assistant fn-calling seems to pay attn to these,\n            and if we don't want this, we should set this to False.)\n    \"\"\"\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        if require_recipient:\n            message_class = message_class.require_recipient()\n        request = message_class.default_value(\"request\")\n        llm_function = message_class.llm_function_schema(defaults=include_defaults)\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            self.llm_tools_usable.add(t)\n            self.llm_functions_usable.add(t)\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    # Set tool instructions and JSON format instructions\n    if self.config.use_tools:\n        self.system_json_tool_instructions = self.json_format_rules()\n    self.system_tool_instructions = self.tool_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.__fields__[\"request\"].default\n    to_remove = [r for r in self.llm_tools_usable if r != request]\n    for r in to_remove:\n        self.llm_tools_usable.discard(r)\n        self.llm_functions_usable.discard(r)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = self.llm_response_messages(hist, output_len)\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    hist, output_len = self._prep_llm_messages(message)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm_response_messages_async(hist, output_len)\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.init_message_history","title":"<code>init_message_history()</code>","text":"<p>Initialize the message history with the system message and user message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_message_history(self) -&gt; None:\n    \"\"\"\n    Initialize the message history with the system message and user message\n    \"\"\"\n    self.message_history = [self._create_system_and_tools_message()]\n    if self.user_message:\n        self.message_history.append(\n            LLMMessage(role=Role.USER, content=self.user_message)\n        )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None)</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            # (Why? b/c the intent of showing a spinner is to \"show progress\",\n            # and we don't need to do that when streaming, since\n            # streaming output already shows progress.)\n            cm = status(\n                \"LLM responding to messages...\",\n                log_if_quiet=False,\n            )\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions, fun_call = self._function_args()\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            functions=functions,\n            function_call=fun_call,\n        )\n    if self.llm.get_stream():\n        self.callbacks.finish_llm_stream(\n            content=str(response),\n            is_tool=self.has_tool_message_attempt(\n                ChatDocument.from_LLMResponse(response, displayed=True),\n            ),\n        )\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    functions: Optional[List[LLMFunctionSpec]] = None\n    fun_call: str | Dict[str, str] = \"none\"\n    if self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\n        functions = [self.llm_functions_map[f] for f in self.llm_functions_usable]\n        fun_call = (\n            \"auto\" if self.llm_function_force is None else self.llm_function_force\n        )\n    assert self.llm is not None\n\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        functions=functions,\n        function_call=fun_call,\n    )\n    if self.llm.get_stream():\n        self.callbacks.finish_llm_stream(\n            content=str(response),\n            is_tool=self.has_tool_message_attempt(\n                ChatDocument.from_LLMResponse(response, displayed=True),\n            ),\n        )\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str): user message\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    self.message_history.pop() if len(self.message_history) &gt; n_msgs else None\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/chat_document/","title":"chat_document","text":"<p>langroid/agent/chat_document.py </p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.StatusCode","title":"<code>StatusCode</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Codes meant to be returned by task.run(). Some are not used yet.</p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument","title":"<code>ChatDocument(**data)</code>","text":"<p>             Bases: <code>Document</code></p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def __init__(self, **data: Any):\n    super().__init__(**data)\n    ObjectRegistry.register_object(self)\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.delete_id","title":"<code>delete_id(id)</code>  <code>staticmethod</code>","text":"<p>Remove ChatDocument with given id from ObjectRegistry, and all its descendants.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef delete_id(id: str) -&gt; None:\n    \"\"\"Remove ChatDocument with given id from ObjectRegistry,\n    and all its descendants.\n    \"\"\"\n    chat_doc = ChatDocument.from_id(id)\n    # first delete all descendants\n    while chat_doc is not None:\n        next_chat_doc = chat_doc.child\n        ObjectRegistry.remove(chat_doc.id())\n        chat_doc = next_chat_doc\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.get_json_tools","title":"<code>get_json_tools()</code>","text":"<p>Get names of attempted JSON tool usages in the content     of the message. Returns:     List[str]: list of JSON tool names</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_json_tools(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted JSON tool usages in the content\n        of the message.\n    Returns:\n        List[str]: list of JSON tool names\n    \"\"\"\n    jsons = extract_top_level_json(self.content)\n    tools = []\n    for j in jsons:\n        json_data = json.loads(j)\n        tool = json_data.get(\"request\")\n        if tool is not None:\n            tools.append(str(tool))\n    return tools\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n    if self.function_call is not None:\n        tool_type = \"FUNC\"\n        tool = self.function_call.name\n    elif self.get_json_tools() != []:\n        tool_type = \"TOOL\"\n        tool = self.get_json_tools()[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    if tool_type == \"FUNC\":\n        content += str(self.function_call)\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.pop_tool_ids","title":"<code>pop_tool_ids()</code>","text":"<p>Pop the last tool_id from the stack of tool_ids.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def pop_tool_ids(self) -&gt; None:\n    \"\"\"\n    Pop the last tool_id from the stack of tool_ids.\n    \"\"\"\n    if len(self.metadata.tool_ids) &gt; 0:\n        self.metadata.tool_ids.pop()\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.from_LLMResponse","title":"<code>from_LLMResponse(response, displayed=False)</code>  <code>staticmethod</code>","text":"<p>Convert LLMResponse to ChatDocument. Args:     response (LLMResponse): LLMResponse to convert.     displayed (bool): Whether this response was displayed to the user. Returns:     ChatDocument: ChatDocument representation of this LLMResponse.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMResponse(\n    response: LLMResponse,\n    displayed: bool = False,\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMResponse to ChatDocument.\n    Args:\n        response (LLMResponse): LLMResponse to convert.\n        displayed (bool): Whether this response was displayed to the user.\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMResponse.\n    \"\"\"\n    recipient, message = response.get_recipient_and_message()\n    message = message.strip()\n    if message in [\"''\", '\"\"']:\n        message = \"\"\n    if response.function_call is not None:\n        # Sometimes an OpenAI LLM (esp gpt-4o) may generate a function-call\n        # with odditities:\n        # (a) the `name` is set, as well as `arugments.request` is set,\n        #  and in langroid we use the `request` value as the `name`.\n        #  In this case we override the `name` with the `request` value.\n        # (b) the `name` looks like \"functions blah\" or just \"functions\"\n        #   In this case we strip the \"functions\" part.\n        fc = response.function_call\n        fc.name = fc.name.replace(\"functions\", \"\").strip()\n        if fc.arguments is not None:\n            request = fc.arguments.get(\"request\")\n            if request is not None and request != \"\":\n                fc.name = request\n                fc.arguments.pop(\"request\")\n    return ChatDocument(\n        content=message,\n        function_call=response.function_call,\n        metadata=ChatDocMetaData(\n            source=Entity.LLM,\n            sender=Entity.LLM,\n            usage=response.usage,\n            displayed=displayed,\n            cached=response.cached,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message)</code>  <code>staticmethod</code>","text":"<p>Convert to LLMMessage for use with LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <p>Returns:     LLMMessage: LLMMessage representation of this str or ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(message: Union[str, \"ChatDocument\"]) -&gt; LLMMessage:\n    \"\"\"\n    Convert to LLMMessage for use with LLM.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n    Returns:\n        LLMMessage: LLMMessage representation of this str or ChatDocument.\n\n    \"\"\"\n    sender_name = None\n    sender_role = Role.USER\n    fun_call = None\n    tool_id = \"\"\n    chat_document_id: str = \"\"\n    if isinstance(message, ChatDocument):\n        content = message.content\n        fun_call = message.function_call\n        if message.metadata.sender == Entity.USER and fun_call is not None:\n            # This may happen when a (parent agent's) LLM generates a\n            # a Function-call, and it ends up being sent to the current task's\n            # LLM (possibly because the function-call is mis-named or has other\n            # issues and couldn't be handled by handler methods).\n            # But a function-call can only be generated by an entity with\n            # Role.ASSISTANT, so we instead put the content of the function-call\n            # in the content of the message.\n            content += \" \" + str(fun_call)\n            fun_call = None\n        sender_name = message.metadata.sender_name\n        tool_ids = message.metadata.tool_ids\n        tool_id = tool_ids[-1] if len(tool_ids) &gt; 0 else \"\"\n        chat_document_id = message.id()\n        if message.metadata.sender == Entity.SYSTEM:\n            sender_role = Role.SYSTEM\n        if (\n            message.metadata.parent is not None\n            and message.metadata.parent.function_call is not None\n        ):\n            sender_role = Role.FUNCTION\n            sender_name = message.metadata.parent.function_call.name\n        elif message.metadata.sender == Entity.LLM:\n            sender_role = Role.ASSISTANT\n    else:\n        # LLM can only respond to text content, so extract it\n        content = message\n\n    return LLMMessage(\n        role=sender_role,\n        tool_id=tool_id,\n        content=content,\n        function_call=fun_call,\n        name=sender_name,\n        chat_document_id=chat_document_id,\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/","title":"openai_assistant","text":"<p>langroid/agent/openai_assistant.py </p>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant","title":"<code>OpenAIAssistant(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>A ChatAgent powered by OpenAI Assistant API: mainly, in <code>llm_response</code> method, we avoid maintaining conversation state, and instead let the Assistant API do it for us. Also handles persistent storage of Assistant and Threads: stores their ids (for given user, org) in a cache, and reuses them based on config.use_cached_assistant and config.use_cached_thread.</p> <p>This class can be used as a drop-in replacement for ChatAgent.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def __init__(self, config: OpenAIAssistantConfig):\n    super().__init__(config)\n    self.config: OpenAIAssistantConfig = config\n    self.llm: OpenAIGPT = OpenAIGPT(self.config.llm)\n    if not isinstance(self.llm.client, openai.OpenAI):\n        raise ValueError(\"Client must be OpenAI\")\n    # handles for various entities and methods\n    self.client: openai.OpenAI = self.llm.client\n    self.runs = self.client.beta.threads.runs\n    self.threads = self.client.beta.threads\n    self.thread_messages = self.client.beta.threads.messages\n    self.assistants = self.client.beta.assistants\n    # which tool_ids are awaiting output submissions\n    self.pending_tool_ids: List[str] = []\n    self.cached_tool_ids: List[str] = []\n\n    self.thread: Thread | None = None\n    self.assistant: Assistant | None = None\n    self.run: Run | None = None\n\n    self._maybe_create_assistant(self.config.assistant_id)\n    self._maybe_create_thread(self.config.thread_id)\n    self._cache_store()\n\n    self.add_assistant_files(self.config.files)\n    self.add_assistant_tools(self.config.tools)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.add_assistant_files","title":"<code>add_assistant_files(files)</code>","text":"<p>Add file_ids to assistant</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def add_assistant_files(self, files: List[str]) -&gt; None:\n    \"\"\"Add file_ids to assistant\"\"\"\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    self.files = [\n        self.client.files.create(file=open(f, \"rb\"), purpose=\"assistants\")\n        for f in files\n    ]\n    self.config.files = list(set(self.config.files + files))\n    self.assistant = self.assistants.update(\n        self.assistant.id,\n        tool_resources=ToolResources(\n            code_interpreter=ToolResourcesCodeInterpreter(\n                file_ids=[f.id for f in self.files],\n            ),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.add_assistant_tools","title":"<code>add_assistant_tools(tools)</code>","text":"<p>Add tools to assistant</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def add_assistant_tools(self, tools: List[AssistantTool]) -&gt; None:\n    \"\"\"Add tools to assistant\"\"\"\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    all_tool_dicts = [t.dct() for t in self.config.tools]\n    for t in tools:\n        if t.dct() not in all_tool_dicts:\n            self.config.tools.append(t)\n    self.assistant = self.assistants.update(\n        self.assistant.id,\n        tools=[tool.dct() for tool in self.config.tools],  # type: ignore\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Override ChatAgent's method: extract the function-related args. See that method for details. But specifically about the <code>include_defaults</code> arg: Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"Override ChatAgent's method: extract the function-related args.\n    See that method for details. But specifically about the `include_defaults` arg:\n    Normally the OpenAI completion API ignores these fields, but the Assistant\n    fn-calling seems to pay attn to these, and if we don't want this,\n    we should set this to False.\n    \"\"\"\n    super().enable_message(\n        message_class,\n        use=use,\n        handle=handle,\n        force=force,\n        require_recipient=require_recipient,\n        include_defaults=include_defaults,\n    )\n    if message_class is None or not use:\n        # no specific msg class, or\n        # we are not enabling USAGE/GENERATION of this tool/fn,\n        # then there's no need to attach the fn to the assistant\n        # (HANDLING the fn will still work via self.agent_response)\n        return\n    if self.config.use_tools:\n        sys_msg = self._create_system_and_tools_message()\n        self.set_system_message(sys_msg.content)\n    if not self.config.use_functions_api:\n        return\n    functions, _ = self._function_args()\n    if functions is None:\n        return\n    # add the functions to the assistant:\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    tools = self.assistant.tools\n    tools.extend(\n        [\n            {\n                \"type\": \"function\",  # type: ignore\n                \"function\": f.dict(),\n            }\n            for f in functions\n        ]\n    )\n    self.assistant = self.assistants.update(\n        self.assistant.id,\n        tools=tools,  # type: ignore\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.thread_msg_to_llm_msg","title":"<code>thread_msg_to_llm_msg(msg)</code>  <code>staticmethod</code>","text":"<p>Convert a Message to an LLMMessage</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>@staticmethod\ndef thread_msg_to_llm_msg(msg: Message) -&gt; LLMMessage:\n    \"\"\"\n    Convert a Message to an LLMMessage\n    \"\"\"\n    return LLMMessage(\n        content=msg.content[0].text.value,  # type: ignore\n        role=Role(msg.role),\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.set_system_message","title":"<code>set_system_message(msg)</code>","text":"<p>Override ChatAgent's method. The Task may use this method to set the system message of the chat assistant.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def set_system_message(self, msg: str) -&gt; None:\n    \"\"\"\n    Override ChatAgent's method.\n    The Task may use this method to set the system message\n    of the chat assistant.\n    \"\"\"\n    super().set_system_message(msg)\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    self.assistant = self.assistants.update(self.assistant.id, instructions=msg)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.process_citations","title":"<code>process_citations(thread_msg)</code>","text":"<p>Process citations in the thread message. Modifies the thread message in-place.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def process_citations(self, thread_msg: Message) -&gt; None:\n    \"\"\"\n    Process citations in the thread message.\n    Modifies the thread message in-place.\n    \"\"\"\n    # could there be multiple content items?\n    # TODO content could be MessageContentImageFile; handle that later\n    annotated_content = thread_msg.content[0].text  # type: ignore\n    annotations = annotated_content.annotations\n    citations = []\n    # Iterate over the annotations and add footnotes\n    for index, annotation in enumerate(annotations):\n        # Replace the text with a footnote\n        annotated_content.value = annotated_content.value.replace(\n            annotation.text, f\" [{index}]\"\n        )\n        # Gather citations based on annotation attributes\n        if file_citation := getattr(annotation, \"file_citation\", None):\n            try:\n                cited_file = self.client.files.retrieve(file_citation.file_id)\n            except Exception:\n                logger.warning(\n                    f\"\"\"\n                    Could not retrieve cited file with id {file_citation.file_id}, \n                    ignoring. \n                    \"\"\"\n                )\n                continue\n            citations.append(\n                f\"[{index}] '{file_citation.quote}',-- from {cited_file.filename}\"\n            )\n        elif file_path := getattr(annotation, \"file_path\", None):\n            cited_file = self.client.files.retrieve(file_path.file_id)\n            citations.append(\n                f\"[{index}] Click &lt;here&gt; to download {cited_file.filename}\"\n            )\n        # Note: File download functionality not implemented above for brevity\n    sep = \"\\n\" if len(citations) &gt; 0 else \"\"\n    annotated_content.value += sep + \"\\n\".join(citations)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Override ChatAgent's method: this is the main LLM response method. In the ChatAgent, this updates <code>self.message_history</code> and then calls <code>self.llm_response_messages</code>, but since we are relying on the Assistant API to maintain conversation state, this method is simpler: Simply start a run on the message-thread, and wait for it to complete.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str | ChatDocument]</code> <p>message to respond to (if absent, the LLM response will be based on the instructions in the system_message). Defaults to None.</p> <code>None</code> <p>Returns:     Optional[ChatDocument]: LLM response</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Override ChatAgent's method: this is the main LLM response method.\n    In the ChatAgent, this updates `self.message_history` and then calls\n    `self.llm_response_messages`, but since we are relying on the Assistant API\n    to maintain conversation state, this method is simpler: Simply start a run\n    on the message-thread, and wait for it to complete.\n\n    Args:\n        message (Optional[str | ChatDocument], optional): message to respond to\n            (if absent, the LLM response will be based on the\n            instructions in the system_message). Defaults to None.\n    Returns:\n        Optional[ChatDocument]: LLM response\n    \"\"\"\n    response = self._llm_response_preprocess(message)\n    cached = True\n    if response is None:\n        cached = False\n        response = self._run_result()\n    return self._llm_response_postprocess(response, cached=cached, message=message)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of llm_response.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of llm_response.\n    \"\"\"\n    response = self._llm_response_preprocess(message)\n    cached = True\n    if response is None:\n        cached = False\n        response = await self._run_result_async()\n    return self._llm_response_postprocess(response, cached=cached, message=message)\n</code></pre>"},{"location":"reference/agent/task/","title":"task","text":"<p>langroid/agent/task.py </p>"},{"location":"reference/agent/task/#langroid.agent.task.TaskConfig","title":"<code>TaskConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration for a Task. This is a container for any params that we didn't include in the task <code>__init__</code> method. We may eventually move all the task init params to this class, analogous to how we have config classes for <code>Agent</code>, <code>ChatAgent</code>, <code>LanguageModel</code>, etc.</p> <p>Attributes:</p> Name Type Description <code>inf_loop_cycle_len</code> <code>int</code> <p>max exact-loop cycle length: 0 =&gt; no inf loop test</p> <code>inf_loop_dominance_factor</code> <code>float</code> <p>dominance factor for exact-loop detection</p> <code>inf_loop_wait_factor</code> <code>int</code> <p>wait this * cycle_len msgs before loop-check</p> <code>restart_subtask_run</code> <code>bool</code> <p>whether to restart every run of this task when run as a subtask.</p>"},{"location":"reference/agent/task/#langroid.agent.task.Task","title":"<code>Task(agent=None, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=True, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False, allow_null_result=False, max_stalled_steps=5, done_if_no_response=[], done_if_response=[], config=TaskConfig(), **kwargs)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>Whether to delegate \"control\" to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve, and decides when a task is done. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity). See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by \"controller\" (i.e. LLM if <code>llm_delegate</code> is true, otherwise USER) and subsequent response by non-controller [When a tool is involved, this will not give intended results. See <code>done_if_response</code>, <code>done_if_no_response</code> below]. termination]. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\". See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true, resets the agent's message history at every run.</p> <code>True</code> <code>default_human_response</code> <code>str | None</code> <p>default response from user; useful for testing, to avoid interactive input from user. [Instead of this, setting <code>interactive</code> usually suffices]</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\" Note: When interactive = False, the one exception is when the user is explicitly addressed, via \"@user\" or using RecipientTool, in which case the system will wait for a user response. In other words, use <code>interactive=False</code> when you want a \"largely non-interactive\" run, with the exception of explicit user addressing.</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, when interactive=True, only user can quit the root task (Ignored when interactive=False).</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> <code>allow_null_result</code> <code>bool</code> <p>If true, create dummy NO_ANSWER response when no valid response is found in a step. Optional, default is False. Note: In non-interactive mode, when this is set to True, you can have a situation where an LLM generates (non-tool) text, and no other responders have valid responses, and a \"Null result\" is inserted as a dummy response from the User entity, so the LLM will now respond to this Null result, and this will continue until the LLM emits a DONE signal (if instructed to do so), otherwise langroid detects a potential infinite loop after a certain number of such steps (= <code>TaskConfig.inf_loop_wait_factor</code>) and will raise an InfiniteLoopException.</p> <code>False</code> <code>max_stalled_steps</code> <code>int</code> <p>task considered done after this many consecutive steps with no progress. Default is 3.</p> <code>5</code> <code>done_if_no_response</code> <code>List[Responder]</code> <p>consider task done if NULL response from any of these responders. Default is empty list.</p> <code>[]</code> <code>done_if_response</code> <code>List[Responder]</code> <p>consider task done if NON-NULL response from any of these responders. Default is empty list.</p> <code>[]</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Optional[Agent] = None,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = True,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n    allow_null_result: bool = False,\n    max_stalled_steps: int = 5,\n    done_if_no_response: List[Responder] = [],\n    done_if_response: List[Responder] = [],\n    config: TaskConfig = TaskConfig(),\n    **kwargs: Any,  # catch-all for any legacy params, for backwards compatibility\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool):\n            Whether to delegate \"control\" to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve, and decides when a task is done.\n            The \"controlling entity\" is either the LLM or the USER.\n            (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        single_round (bool):\n            If true, task runs until one message by \"controller\"\n            (i.e. LLM if `llm_delegate` is true, otherwise USER)\n            and subsequent response by non-controller [When a tool is involved,\n            this will not give intended results. See `done_if_response`,\n            `done_if_no_response` below].\n            termination]. If false, runs for the specified number of turns in\n            `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true, resets the agent's message history *at every run*.\n        default_human_response (str|None): default response from user; useful for\n            testing, to avoid interactive input from user.\n            [Instead of this, setting `interactive` usually suffices]\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n            Note: When interactive = False, the one exception is when the user\n            is explicitly addressed, via \"@user\" or using RecipientTool, in which\n            case the system will wait for a user response. In other words, use\n            `interactive=False` when you want a \"largely non-interactive\"\n            run, with the exception of explicit user addressing.\n        only_user_quits_root (bool): if true, when interactive=True, only user can\n            quit the root task (Ignored when interactive=False).\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n        allow_null_result (bool):\n            If true, create dummy NO_ANSWER response when no valid response is found\n            in a step.\n            Optional, default is False.\n            *Note:* In non-interactive mode, when this is set to True,\n            you can have a situation where an LLM generates (non-tool) text,\n            and no other responders have valid responses, and a \"Null result\"\n            is inserted as a dummy response from the User entity, so the LLM\n            will now respond to this Null result, and this will continue\n            until the LLM emits a DONE signal (if instructed to do so),\n            otherwise langroid detects a potential infinite loop after\n            a certain number of such steps (= `TaskConfig.inf_loop_wait_factor`)\n            and will raise an InfiniteLoopException.\n        max_stalled_steps (int): task considered done after this many consecutive\n            steps with no progress. Default is 3.\n        done_if_no_response (List[Responder]): consider task done if NULL\n            response from any of these responders. Default is empty list.\n        done_if_response (List[Responder]): consider task done if NON-NULL\n            response from any of these responders. Default is empty list.\n    \"\"\"\n    if agent is None:\n        agent = ChatAgent()\n    self.callbacks = SimpleNamespace(\n        show_subtask_response=noop_fn,\n        set_parent_agent=noop_fn,\n    )\n    self.config = config\n    # how to behave as a sub-task; can be overriden by `add_sub_task()`\n    self.config_sub_task = copy.deepcopy(config)\n    # counts of distinct pending messages in history,\n    # to help detect (exact) infinite loops\n    self.message_counter: Counter[str] = Counter()\n    self._init_message_counter()\n\n    self.history: Deque[str] = deque(\n        maxlen=self.config.inf_loop_cycle_len * self.config.inf_loop_wait_factor\n    )\n    # copy the agent's config, so that we don't modify the original agent's config,\n    # which may be shared by other agents.\n    try:\n        config_copy = copy.deepcopy(agent.config)\n        agent.config = config_copy\n    except Exception:\n        logger.warning(\n            \"\"\"\n            Failed to deep-copy Agent config during task creation, \n            proceeding with original config. Be aware that changes to \n            the config may affect other agents using the same config.\n            \"\"\"\n        )\n    self.restart = restart\n    agent = cast(ChatAgent, agent)\n    self.agent: ChatAgent = agent\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        self.agent.clear_history(0)\n        self.agent.clear_dialog()\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            self.agent.set_system_message(system_message)\n        if user_message:\n            self.agent.set_user_message(user_message)\n    self.max_cost: float = 0\n    self.max_tokens: int = 0\n    self.session_id: str = \"\"\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.color_log: bool = False if settings.notebook else True\n\n    self.n_stalled_steps = 0  # how many consecutive steps with no progress?\n    # how many 2-step-apart alternations of no_answer step-result have we had,\n    # i.e. x1, N/A, x2, N/A, x3, N/A ...\n    self.n_no_answer_alternations = 0\n    self._no_answer_step: int = -5\n    self._step_idx = -1  # current step index\n    self.max_stalled_steps = max_stalled_steps\n    self.done_if_response = [r.value for r in done_if_response]\n    self.done_if_no_response = [r.value for r in done_if_no_response]\n    self.is_done = False  # is task done (based on response)?\n    self.is_pass_thru = False  # is current response a pass-thru?\n    if name:\n        # task name overrides name in agent config\n        agent.config.name = name\n    self.name = name or agent.config.name\n    self.value: str = self.name\n\n    self.default_human_response = default_human_response\n    if default_human_response is not None:\n        # only override agent's default_human_response if it is explicitly set\n        self.agent.default_human_response = default_human_response\n    self.interactive = interactive\n    self.agent.interactive = interactive\n    self.only_user_quits_root = only_user_quits_root\n    self.message_history_idx = -1\n\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n    self.allow_null_result = allow_null_result\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    if llm_delegate:\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM (as the Controller) asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        if self.single_round:\n            # 0: User (as Controller) asks,\n            # 1: LLM replies.\n            self.turns = 1\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n    agent: ChatAgent = self.agent.clone(i)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=self.restart,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        erase_substeps=self.erase_substeps,\n        allow_null_result=self.allow_null_result,\n        max_stalled_steps=self.max_stalled_steps,\n        done_if_no_response=[Entity(s) for s in self.done_if_no_response],\n        done_if_response=[Entity(s) for s in self.done_if_response],\n        config=self.config,\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.kill_session","title":"<code>kill_session(session_id='')</code>  <code>classmethod</code>","text":"<p>Kill the session with the given session_id.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>@classmethod\ndef kill_session(cls, session_id: str = \"\") -&gt; None:\n    \"\"\"\n    Kill the session with the given session_id.\n    \"\"\"\n    session_id_kill_key = f\"{session_id}:kill\"\n    cls.cache().store(session_id_kill_key, \"1\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.kill","title":"<code>kill()</code>","text":"<p>Kill the task run associated with the current session.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def kill(self) -&gt; None:\n    \"\"\"\n    Kill the task run associated with the current session.\n    \"\"\"\n    self._cache_session_store(\"kill\", \"1\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response (unless a sub-task is explicitly addressed).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]</code> <p>A task, or list of tasks, or a tuple of task and task config, or a list of tuples of task and task config. These tasks are added as sub-tasks of the current task. The task configs (if any) dictate how the tasks are run when invoked as sub-tasks of other tasks. This allows users to specify behavior applicable only in the context of a particular task-subtask combination.</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(\n    self,\n    task: (\n        Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response (unless a sub-task is explicitly addressed).\n\n    Args:\n        task: A task, or list of tasks, or a tuple of task and task config,\n            or a list of tuples of task and task config.\n            These tasks are added as sub-tasks of the current task.\n            The task configs (if any) dictate how the tasks are run when\n            invoked as sub-tasks of other tasks. This allows users to specify\n            behavior applicable only in the context of a particular task-subtask\n            combination.\n    \"\"\"\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n\n    if isinstance(task, tuple):\n        task, config = task\n    else:\n        config = TaskConfig()\n    task.config_sub_task = config\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    elif msg is None and len(self.agent.message_history) &gt; 1:\n        # if agent has a history beyond system msg, set the\n        # pending message to the ChatDocument linked from\n        # last message in the history\n        last_agent_msg = self.agent.message_history[-1]\n        self.pending_message = ChatDocument.from_id(last_agent_msg.chat_document_id)\n        if self.pending_message is not None:\n            self.pending_sender = self.pending_message.metadata.sender\n    else:\n        if isinstance(msg, ChatDocument):\n            # carefully deep-copy: fresh metadata.id, register\n            # as new obj in registry\n            self.pending_message = ChatDocument.deepcopy(msg)\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n            # update parent, child, agent pointers\n            if msg is not None:\n                msg.metadata.child_id = self.pending_message.metadata.id\n                self.pending_message.metadata.parent_id = msg.metadata.id\n            self.pending_message.metadata.agent_id = self.agent.id\n\n    self._show_pending_message_if_debug()\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    else:\n        self.logger = RichFileLogger(\n            str(Path(self.config.logs_dir) / f\"{self.name}.log\"),\n            color=self.color_log,\n        )\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    else:\n        self.tsv_logger = setup_file_logger(\n            \"tsv_logger\",\n            str(Path(self.config.logs_dir) / f\"{self.name}.tsv\"),\n        )\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    self.log_message(Entity.USER, self.pending_message)\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.reset_all_sub_tasks","title":"<code>reset_all_sub_tasks()</code>","text":"<p>Recursively reset message history of own agent and all sub-tasks</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def reset_all_sub_tasks(self) -&gt; None:\n    \"\"\"Recursively reset message history of own agent and all sub-tasks\"\"\"\n    self.agent.clear_history(0)\n    self.agent.clear_dialog()\n    for t in self.sub_tasks:\n        t.reset_all_sub_tasks()\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run","title":"<code>run(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='')</code>","text":"<p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n    if (self.restart and caller is None) or (\n        self.config_sub_task.restart_as_subtask and caller is not None\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    assert (\n        msg is None or isinstance(msg, str) or isinstance(msg, ChatDocument)\n    ), f\"msg arg in Task.run() must be None, str, or ChatDocument, not {type(msg)}\"\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        self.step()\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='')</code>  <code>async</code>","text":"<p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>initial user-role message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User). Note that <code>msg</code>, if passed, is treated as message with role <code>user</code>; a \"system\" role message should not be passed here.</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <code>max_cost</code> <code>float</code> <p>max cost allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>max tokens allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>session_id</code> <code>str</code> <p>session id for the task</p> <code>''</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (str|ChatDocument): initial *user-role* message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User). Note that `msg`, if passed, is treated as\n            message with role `user`; a \"system\" role message should not be\n            passed here.\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n        max_cost (float): max cost allowed for the task (default 0 -&gt; no limit)\n        max_tokens (int): max tokens allowed for the task (default 0 -&gt; no limit)\n        session_id (str): session id for the task\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if (\n        self.restart\n        and caller is None\n        or self.config_sub_task.restart_as_subtask\n        and caller is not None\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=True,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        await self.step_async()\n        await asyncio.sleep(0.01)  # temp yield to avoid blocking\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details. TODO: Except for the self.response() calls, this fn should be identical to <code>step_async()</code>. Consider refactoring to avoid duplication.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    TODO: Except for the self.response() calls, this fn should be identical to\n    `step_async()`. Consider refactoring to avoid duplication.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # When in interactive mode,\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:  # did not find a valid response\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        result = e.run(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n            max_cost=self.max_cost,\n            max_tokens=self.max_tokens,\n        )\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\" if result is None else str(ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n    return self._process_result_routing(result)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        result = await e.run_async(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n            max_cost=self.max_cost,\n            max_tokens=self.max_tokens,\n        )\n        result_str = str(ChatDocument.to_LLMMessage(result))\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n    return self._process_result_routing(result)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.result","title":"<code>result(status=None)</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Note the result of a task is returned as if it is from the User entity.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusCode</code> <p>status of the task when it ended</p> <code>None</code> <p>Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self, status: StatusCode | None = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n\n    Note the result of a task is returned as if it is from the User entity.\n\n    Args:\n        status (StatusCode): status of the task when it ended\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    if status in [StatusCode.STALLED, StatusCode.MAX_TURNS, StatusCode.INF_LOOP]:\n        # In these case we don't know (and don't want to try to guess)\n        # what the task result should be, so we return None\n        return None\n\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    if DONE in content:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    fun_call = result_msg.function_call if result_msg else None\n    tool_messages = result_msg.tool_messages if result_msg else []\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else \"\"\n    tool_ids = result_msg.metadata.tool_ids if result_msg else []\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    result_doc = ChatDocument(\n        content=content,\n        function_call=fun_call,\n        tool_messages=tool_messages,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            status=status or (result_msg.metadata.status if result_msg else None),\n            sender_name=self.name,\n            recipient=recipient,\n            tool_ids=tool_ids,\n            parent_id=result_msg.id() if result_msg else \"\",\n            agent_id=str(self.agent.id),\n        ),\n    )\n    if self.pending_message is not None:\n        self.pending_message.metadata.child_id = result_doc.id()\n\n    return result_doc\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.done","title":"<code>done(result=None, r=None)</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Args:     result (ChatDocument|None): result from a responder     r (Responder|None): responder that produced the result         Not used here, but could be used by derived classes. Returns:     bool: True if task is done, False otherwise     StatusCode: status code indicating why task is done</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(\n    self, result: ChatDocument | None = None, r: Responder | None = None\n) -&gt; Tuple[bool, StatusCode]:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Args:\n        result (ChatDocument|None): result from a responder\n        r (Responder|None): responder that produced the result\n            Not used here, but could be used by derived classes.\n    Returns:\n        bool: True if task is done, False otherwise\n        StatusCode: status code indicating why task is done\n    \"\"\"\n    if self._is_kill():\n        return (True, StatusCode.KILL)\n    result = result or self.pending_message\n    user_quit = (\n        result is not None\n        and (result.content in USER_QUIT_STRINGS or DONE in result.content)\n        and result.metadata.sender == Entity.USER\n    )\n    if self._level == 0 and self.interactive and self.only_user_quits_root:\n        # for top-level task, in interactive mode, only user can quit out\n        return (user_quit, StatusCode.USER_QUIT if user_quit else StatusCode.OK)\n\n    if self.is_done:\n        return (True, StatusCode.DONE)\n\n    if self.n_stalled_steps &gt;= self.max_stalled_steps:\n        # we are stuck, so bail to avoid infinite loop\n        logger.warning(\n            f\"Task {self.name} stuck for {self.max_stalled_steps} steps; exiting.\"\n        )\n        return (True, StatusCode.STALLED)\n\n    if self.max_cost &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[1] &gt; self.max_cost:\n                logger.warning(\n                    f\"Task {self.name} cost exceeded {self.max_cost}; exiting.\"\n                )\n                return (True, StatusCode.MAX_COST)\n        except Exception:\n            pass\n\n    if self.max_tokens &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[0] &gt; self.max_tokens:\n                logger.warning(\n                    f\"Task {self.name} uses &gt; {self.max_tokens} tokens; exiting.\"\n                )\n                return (True, StatusCode.MAX_TOKENS)\n        except Exception:\n            pass\n    final = (\n        # no valid response from any entity/agent in current turn\n        result is None\n        # An entity decided task is done\n        or DONE in result.content\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and result.metadata.recipient == self.caller.name\n        )\n        or user_quit\n    )\n    return (final, StatusCode.OK)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.valid","title":"<code>valid(result, r)</code>","text":"<p>Is the result from a Responder (i.e. an entity or sub-task) such that we can stop searching for responses in this step?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(\n    self,\n    result: Optional[ChatDocument],\n    r: Responder,\n) -&gt; bool:\n    \"\"\"\n    Is the result from a Responder (i.e. an entity or sub-task)\n    such that we can stop searching for responses in this step?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n\n    # if task would be considered done given responder r's `result`,\n    # then consider the result valid.\n    if result is not None and self.done(result, r)[0]:\n        return True\n    return (\n        result is not None\n        and not self._is_empty_message(result)\n        # some weaker LLMs, including even GPT-4o, may say \"DO-NOT-KNOW.\"\n        # (with a punctuation at the end), so need to strip out punctuation\n        and re.sub(r\"[,.!?:]\", \"\", result.content.strip()) != NO_ANSWER\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    default_values = ChatDocLoggerFields().dict().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.parse_routing","title":"<code>parse_routing(msg)</code>","text":"<p>Parse routing instruction if any, of the form: PASS:  (pass current pending msg to recipient) SEND:  (send content to recipient) @  (send content to recipient) Args:     msg (ChatDocument|str|None): message to parse Returns:     Tuple[bool|None, str|None, str|None]:         bool: true=PASS, false=SEND, or None if neither         str: recipient, or None         str: content to send, or None Source code in <code>langroid/agent/task.py</code> <pre><code>def parse_routing(\n    msg: ChatDocument | str,\n) -&gt; Tuple[bool | None, str | None, str | None]:\n    \"\"\"\n    Parse routing instruction if any, of the form:\n    PASS:&lt;recipient&gt;  (pass current pending msg to recipient)\n    SEND:&lt;recipient&gt; &lt;content&gt; (send content to recipient)\n    @&lt;recipient&gt; &lt;content&gt; (send content to recipient)\n    Args:\n        msg (ChatDocument|str|None): message to parse\n    Returns:\n        Tuple[bool|None, str|None, str|None]:\n            bool: true=PASS, false=SEND, or None if neither\n            str: recipient, or None\n            str: content to send, or None\n    \"\"\"\n    # handle routing instruction in result if any,\n    # of the form PASS=&lt;recipient&gt;\n    content = msg.content if isinstance(msg, ChatDocument) else msg\n    content = content.strip()\n    if PASS in content and PASS_TO not in content:\n        return True, None, None\n    if PASS_TO in content and content.split(\":\")[1] != \"\":\n        return True, content.split(\":\")[1], None\n    if (\n        SEND_TO in content\n        and (addressee_content := parse_addressed_message(content, SEND_TO))[0]\n        is not None\n    ):\n        (addressee, content_to_send) = addressee_content\n        # if no content then treat same as PASS_TO\n        if content_to_send == \"\":\n            return True, addressee, None\n        else:\n            return False, addressee, content_to_send\n    if (\n        AT in content\n        and (addressee_content := parse_addressed_message(content, AT))[0] is not None\n    ):\n        (addressee, content_to_send) = addressee_content\n        # if no content then treat same as PASS_TO\n        if content_to_send == \"\":\n            return True, addressee, None\n        else:\n            return False, addressee, content_to_send\n\n    return None, None, None\n</code></pre>"},{"location":"reference/agent/tool_message/","title":"tool_message","text":"<p>langroid/agent/tool_message.py </p> <p>Structured messages to an agent, typically from an LLM, to be handled by an agent. The messages could represent, for example: - information or data given to the agent - request for information or data from the agent - request to run a method of the agent</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p> <code>result</code> <code>str</code> <p>example of result of agent method.</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with JSON formatting instructions. Each example can be either: - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or - a tuple (description, ToolMessage instance), where the description is     a natural language \"thought\" that leads to the tool usage,     e.g. (\"I want to find the square of 5\",  SquareTool(num=5))     In some scenarios, including such a description can significantly     enhance reliability of tool use. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\" | Tuple[str, \"ToolMessage\"]]:\n    \"\"\"\n    Examples to use in few-shot demos with JSON formatting instructions.\n    Each example can be either:\n    - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or\n    - a tuple (description, ToolMessage instance), where the description is\n        a natural language \"thought\" that leads to the tool usage,\n        e.g. (\"I want to find the square of 5\",  SquareTool(num=5))\n        In some scenarios, including such a description can significantly\n        enhance reliability of tool use.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.usage_examples","title":"<code>usage_examples(random=False)</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing examples of how to use the tool-message.</p> <p>Parameters:</p> Name Type Description Default <code>random</code> <code>bool</code> <p>whether to pick a random example from the list of examples. Set to <code>true</code> when using this to illustrate a dialog between LLM and user. (if false, use ALL examples)</p> <code>False</code> <p>Returns:     str: examples of how to use the tool/function-call</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_examples(cls, random: bool = False) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing examples of how to use the tool-message.\n\n    Args:\n        random (bool): whether to pick a random example from the list of examples.\n            Set to `true` when using this to illustrate a dialog between LLM and\n            user.\n            (if false, use ALL examples)\n    Returns:\n        str: examples of how to use the tool/function-call\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    if random:\n        examples = [choice(cls.examples())]\n    else:\n        examples = cls.examples()\n    examples_jsons = [\n        (\n            f\"EXAMPLE {i}: (THOUGHT: {ex[0]}) =&gt; \\n{ex[1].json_example()}\"\n            if isinstance(ex, tuple)\n            else f\"EXAMPLE {i}:\\n {ex.json_example()}\"\n        )\n        for i, ex in enumerate(examples, 1)\n    ]\n    return \"\\n\\n\".join(examples_jsons)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.json_instructions","title":"<code>json_instructions(tool=False)</code>  <code>classmethod</code>","text":"<p>Default Instructions to the LLM showing how to use the tool/function-call. Works for GPT4 but override this for weaker LLMs if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>bool</code> <p>instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM) (or else it would be for OpenAI Function calls)</p> <code>False</code> <p>Returns:     str: instructions on how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef json_instructions(cls, tool: bool = False) -&gt; str:\n    \"\"\"\n    Default Instructions to the LLM showing how to use the tool/function-call.\n    Works for GPT4 but override this for weaker LLMs if needed.\n\n    Args:\n        tool: instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM)\n            (or else it would be for OpenAI Function calls)\n    Returns:\n        str: instructions on how to use the message\n    \"\"\"\n    # TODO: when we attempt to use a \"simpler schema\"\n    # (i.e. all nested fields explicit without definitions),\n    # we seem to get worse results, so we turn it off for now\n    param_dict = (\n        # cls.simple_schema() if tool else\n        cls.llm_function_schema(request=True).parameters\n    )\n    examples_str = \"\"\n    if cls.examples():\n        examples_str = \"EXAMPLES:\\n\" + cls.usage_examples()\n    return textwrap.dedent(\n        f\"\"\"\n        TOOL: {cls.default_value(\"request\")}\n        PURPOSE: {cls.default_value(\"purpose\")} \n        JSON FORMAT: {\n            json.dumps(param_dict, indent=4)\n        }\n        {examples_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.json_group_instructions","title":"<code>json_group_instructions()</code>  <code>staticmethod</code>","text":"<p>Template for instructions for a group of tools. Works with GPT4 but override this for weaker LLMs if needed.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@staticmethod\ndef json_group_instructions() -&gt; str:\n    \"\"\"Template for instructions for a group of tools.\n    Works with GPT4 but override this for weaker LLMs if needed.\n    \"\"\"\n    return textwrap.dedent(\n        \"\"\"\n        === ALL AVAILABLE TOOLS and THEIR JSON FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {json_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above JSON format.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False, defaults=True)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <code>defaults</code> <code>bool</code> <p>whether to include fields with default values in the schema,     in the \"properties\" section.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(\n    cls,\n    request: bool = False,\n    defaults: bool = True,\n) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n        defaults: whether to include fields with default values in the schema,\n                in the \"properties\" section.\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = cls.schema()\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = (\n        [\"result\", \"purpose\"] if request else [\"request\", \"result\", \"purpose\"]\n    )\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes and (defaults or details.get(\"default\") is None)\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\"),\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.simple_schema","title":"<code>simple_schema()</code>  <code>classmethod</code>","text":"<p>Return a simplified schema for the message, with only the request and required fields. Returns:     Dict[str, Any]: simplified schema</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef simple_schema(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a simplified schema for the message, with only the request and\n    required fields.\n    Returns:\n        Dict[str, Any]: simplified schema\n    \"\"\"\n    schema = generate_simple_schema(cls, exclude=[\"result\", \"purpose\"])\n    return schema\n</code></pre>"},{"location":"reference/agent/callbacks/","title":"callbacks","text":"<p>langroid/agent/callbacks/init.py </p>"},{"location":"reference/agent/callbacks/chainlit/","title":"chainlit","text":"<p>langroid/agent/callbacks/chainlit.py </p> <p>Callbacks for Chainlit integration.</p>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks","title":"<code>ChainlitAgentCallbacks(agent, msg=None, config=ChainlitCallbackConfig())</code>","text":"<p>Inject Chainlit callbacks into a Langroid Agent</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def __init__(\n    self,\n    agent: lr.Agent,\n    msg: cl.Message = None,\n    config: ChainlitCallbackConfig = ChainlitCallbackConfig(),\n):\n    \"\"\"Add callbacks to the agent, and save the initial message,\n    so we can alter the display of the first user message.\n    \"\"\"\n    agent.callbacks.start_llm_stream = self.start_llm_stream\n    agent.callbacks.cancel_llm_stream = self.cancel_llm_stream\n    agent.callbacks.finish_llm_stream = self.finish_llm_stream\n    agent.callbacks.show_llm_response = self.show_llm_response\n    agent.callbacks.show_agent_response = self.show_agent_response\n    agent.callbacks.get_user_response = self.get_user_response\n    agent.callbacks.get_last_step = self.get_last_step\n    agent.callbacks.set_parent_agent = self.set_parent_agent\n    agent.callbacks.show_error_message = self.show_error_message\n    agent.callbacks.show_start_response = self.show_start_response\n    self.config = config\n\n    self.agent: lr.Agent = agent\n    if msg is not None:\n        self.show_first_user_message(msg)\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.start_llm_stream","title":"<code>start_llm_stream()</code>","text":"<p>Returns a streaming fn that can be passed to the LLM class</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def start_llm_stream(self) -&gt; Callable[[str], None]:\n    \"\"\"Returns a streaming fn that can be passed to the LLM class\"\"\"\n    self.stream = cl.Step(\n        id=self.curr_step.id if self.curr_step is not None else None,\n        name=self._entity_name(\"llm\"),\n        type=\"llm\",\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = self.stream\n    self.curr_step = None\n    logger.info(\n        f\"\"\"\n        Starting LLM stream for {self.agent.config.name}\n        id = {self.stream.id} \n        under parent {self._get_parent_id()}\n    \"\"\"\n    )\n    run_sync(self.stream.send())  # type: ignore\n\n    def stream_token(t: str) -&gt; None:\n        if self.stream is None:\n            raise ValueError(\"Stream not initialized\")\n        run_sync(self.stream.stream_token(t))\n\n    return stream_token\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.cancel_llm_stream","title":"<code>cancel_llm_stream()</code>","text":"<p>Called when cached response found.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def cancel_llm_stream(self) -&gt; None:\n    \"\"\"Called when cached response found.\"\"\"\n    self.last_step = None\n    if self.stream is not None:\n        run_sync(self.stream.remove())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.finish_llm_stream","title":"<code>finish_llm_stream(content, is_tool=False)</code>","text":"<p>Update the stream, and display entire response in the right language.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def finish_llm_stream(self, content: str, is_tool: bool = False) -&gt; None:\n    \"\"\"Update the stream, and display entire response in the right language.\"\"\"\n    if self.agent.llm is None or self.stream is None:\n        raise ValueError(\"LLM or stream not initialized\")\n    if content == \"\":\n        run_sync(self.stream.remove())  # type: ignore\n    else:\n        run_sync(self.stream.update())  # type: ignore\n    stream_id = self.stream.id if content else None\n    step = cl.Step(\n        id=stream_id,\n        name=self._entity_name(\"llm\", tool=is_tool),\n        type=\"llm\",\n        parent_id=self._get_parent_id(),\n        language=\"json\" if is_tool else None,\n    )\n    step.output = textwrap.dedent(content) or NO_ANSWER\n    logger.info(\n        f\"\"\"\n        Finish STREAM LLM response for {self.agent.config.name}\n        id = {step.id} \n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.update())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_llm_response","title":"<code>show_llm_response(content, is_tool=False, cached=False, language=None)</code>","text":"<p>Show non-streaming LLM response.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_llm_response(\n    self,\n    content: str,\n    is_tool: bool = False,\n    cached: bool = False,\n    language: str | None = None,\n) -&gt; None:\n    \"\"\"Show non-streaming LLM response.\"\"\"\n    step = cl.Step(\n        id=self.curr_step.id if self.curr_step is not None else None,\n        name=self._entity_name(\"llm\", tool=is_tool, cached=cached),\n        type=\"llm\",\n        parent_id=self._get_parent_id(),\n        language=language or (\"json\" if is_tool else None),\n    )\n    self.last_step = step\n    self.curr_step = None\n    step.output = textwrap.dedent(content) or NO_ANSWER\n    logger.info(\n        f\"\"\"\n        Showing NON-STREAM LLM response for {self.agent.config.name}\n        id = {step.id} \n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_error_message","title":"<code>show_error_message(error)</code>","text":"<p>Show error message as a step.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_error_message(self, error: str) -&gt; None:\n    \"\"\"Show error message as a step.\"\"\"\n    step = cl.Step(\n        name=self.agent.config.name + f\"({ERROR})\",\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n        language=\"text\",\n    )\n    self.last_step = step\n    step.output = error\n    run_sync(step.send())\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_agent_response","title":"<code>show_agent_response(content, language='text')</code>","text":"<p>Show message from agent (typically tool handler). Agent response can be considered as a \"step\" between LLM response and user response</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_agent_response(self, content: str, language=\"text\") -&gt; None:\n    \"\"\"Show message from agent (typically tool handler).\n    Agent response can be considered as a \"step\"\n    between LLM response and user response\n    \"\"\"\n    step = cl.Step(\n        id=self.curr_step.id if self.curr_step is not None else None,\n        name=self._entity_name(\"agent\"),\n        type=\"tool\",\n        parent_id=self._get_parent_id(),\n        language=language,\n    )\n    if language == \"text\":\n        content = wrap_text_preserving_structure(content, width=90)\n    self.last_step = step\n    self.curr_step = None\n    step.output = content\n    logger.info(\n        f\"\"\"\n        Showing AGENT response for {self.agent.config.name}\n        id = {step.id} \n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_start_response","title":"<code>show_start_response(entity)</code>","text":"<p>When there's a potentially long-running process, start a step, so that the UI displays a spinner while the process is running.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_start_response(self, entity: str) -&gt; None:\n    \"\"\"When there's a potentially long-running process, start a step,\n    so that the UI displays a spinner while the process is running.\"\"\"\n    if self.curr_step is not None:\n        run_sync(self.curr_step.remove())  # type: ignore\n    step = cl.Step(\n        name=self._entity_name(entity),\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n        language=\"text\",\n    )\n    step.output = \"\"\n    self.last_step = step\n    self.curr_step = step\n    logger.info(\n        f\"\"\"\n        Showing START response for {self.agent.config.name} ({entity})\n        id = {step.id} \n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.get_user_response","title":"<code>get_user_response(prompt)</code>","text":"<p>Ask for user response, wait for it, and return it, as a cl.Step rather than as a cl.Message so we can nest it under the parent step.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def get_user_response(self, prompt: str) -&gt; str:\n    \"\"\"Ask for user response, wait for it, and return it,\n    as a cl.Step rather than as a cl.Message so we can nest it\n    under the parent step.\n    \"\"\"\n    return run_sync(self.ask_user_step(prompt=prompt, suppress_values=[\"c\"]))\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_user_response","title":"<code>show_user_response(message)</code>","text":"<p>Show user response as a step.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_user_response(self, message: str) -&gt; None:\n    \"\"\"Show user response as a step.\"\"\"\n    step = cl.Step(\n        id=cl.context.current_step.id,\n        name=self._entity_name(\"user\"),\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n    )\n    step.output = message\n    logger.info(\n        f\"\"\"\n        Showing USER response for {self.agent.config.name}\n        id = {step.id} \n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_first_user_message","title":"<code>show_first_user_message(msg)</code>","text":"<p>Show first user message as a step.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_first_user_message(self, msg: cl.Message):\n    \"\"\"Show first user message as a step.\"\"\"\n    step = cl.Step(\n        id=msg.id,\n        name=self._entity_name(\"user\"),\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = step\n    step.output = msg.content\n    run_sync(step.update())\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.ask_user_step","title":"<code>ask_user_step(prompt, timeout=USER_TIMEOUT, suppress_values=['c'])</code>  <code>async</code>","text":"<p>Ask user for input, as a step nested under parent_id. Rather than rely entirely on AskUserMessage (which doesn't let us nest the question + answer under a step), we instead create fake steps for the question and answer, and only rely on AskUserMessage with an empty prompt to await user response.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to display to user</p> required <code>timeout</code> <code>int</code> <p>Timeout in seconds</p> <code>USER_TIMEOUT</code> <code>suppress_values</code> <code>List[str]</code> <p>List of values to suppress from display (e.g. \"c\" for continue)</p> <code>['c']</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>User response</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>async def ask_user_step(\n    self,\n    prompt: str,\n    timeout: int = USER_TIMEOUT,\n    suppress_values: List[str] = [\"c\"],\n) -&gt; str:\n    \"\"\"\n    Ask user for input, as a step nested under parent_id.\n    Rather than rely entirely on AskUserMessage (which doesn't let us\n    nest the question + answer under a step), we instead create fake\n    steps for the question and answer, and only rely on AskUserMessage\n    with an empty prompt to await user response.\n\n    Args:\n        prompt (str): Prompt to display to user\n        timeout (int): Timeout in seconds\n        suppress_values (List[str]): List of values to suppress from display\n            (e.g. \"c\" for continue)\n\n    Returns:\n        str: User response\n    \"\"\"\n\n    # save hide_cot status to restore later\n    # (We should probably use a ctx mgr for this)\n    hide_cot = config.ui.hide_cot\n\n    # force hide_cot to False so that the user question + response is visible\n    config.ui.hide_cot = False\n\n    if prompt != \"\":\n        # Create a question step to ask user\n        question_step = cl.Step(\n            name=f\"{self.agent.config.name} (AskUser \u2753)\",\n            type=\"run\",\n            parent_id=self._get_parent_id(),\n        )\n        question_step.output = prompt\n        await question_step.send()  # type: ignore\n\n    # Use AskUserMessage to await user response,\n    # but with an empty prompt so the question is not visible,\n    # but still pauses for user input in the input box.\n    res = await cl.AskUserMessage(\n        content=\"\",\n        timeout=timeout,\n    ).send()\n\n    if res is None:\n        run_sync(\n            cl.Message(\n                content=f\"Timed out after {USER_TIMEOUT} seconds. Exiting.\"\n            ).send()\n        )\n        return \"x\"\n\n    # The above will try to display user response in res\n    # but we create fake step with same id as res and\n    # erase it using empty output so it's not displayed\n    step = cl.Step(\n        id=res[\"id\"],\n        name=\"TempUserResponse\",\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n    )\n    step.output = \"\"\n    await step.update()  # type: ignore\n\n    # Finally, reproduce the user response at right nesting level\n    if res[\"output\"] in suppress_values:\n        config.ui.hide_cot = hide_cot  # restore original value\n        return \"\"\n\n    step = cl.Step(\n        name=self._entity_name(entity=\"user\"),\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n    )\n    step.output = res[\"output\"]\n    await step.send()  # type: ignore\n    config.ui.hide_cot = hide_cot  # restore original value\n    return res[\"output\"]\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitTaskCallbacks","title":"<code>ChainlitTaskCallbacks(task, msg=None, config=ChainlitCallbackConfig())</code>","text":"<p>             Bases: <code>ChainlitAgentCallbacks</code></p> <p>Recursively inject ChainlitAgentCallbacks into a Langroid Task's agent and agents of sub-tasks.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def __init__(\n    self,\n    task: lr.Task,\n    msg: cl.Message = None,\n    config: ChainlitCallbackConfig = ChainlitCallbackConfig(),\n):\n    \"\"\"Inject callbacks recursively, ensuring msg is passed to the\n    top-level agent\"\"\"\n\n    super().__init__(task.agent, msg, config)\n    self._inject_callbacks(task)\n    self.task = task\n    if config.show_subtask_response:\n        self.task.callbacks.show_subtask_response = self.show_subtask_response\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitTaskCallbacks.show_subtask_response","title":"<code>show_subtask_response(task, content, is_tool=False)</code>","text":"<p>Show sub-task response as a step, nested at the right level.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_subtask_response(\n    self, task: lr.Task, content: str, is_tool: bool = False\n) -&gt; None:\n    \"\"\"Show sub-task response as a step, nested at the right level.\"\"\"\n\n    # The step should nest under the calling agent's last step\n    step = cl.Step(\n        name=self.task.agent.config.name + f\"( \u23ce From {task.agent.config.name})\",\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n        language=\"json\" if is_tool else None,\n    )\n    step.output = content or NO_ANSWER\n    self.last_step = step\n    run_sync(step.send())\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.setup_llm","title":"<code>setup_llm()</code>  <code>async</code>","text":"<p>From the session <code>llm_settings</code>, create new LLMConfig and LLM objects, save them in session state.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>@no_type_check\nasync def setup_llm() -&gt; None:\n    \"\"\"From the session `llm_settings`, create new LLMConfig and LLM objects,\n    save them in session state.\"\"\"\n    llm_settings = cl.user_session.get(\"llm_settings\", {})\n    model = llm_settings.get(\"chat_model\")\n    context_length = llm_settings.get(\"context_length\", 16_000)\n    temperature = llm_settings.get(\"temperature\", 0.2)\n    timeout = llm_settings.get(\"timeout\", 90)\n    logger.info(f\"Using model: {model}\")\n    llm_config = lm.OpenAIGPTConfig(\n        chat_model=model or lm.OpenAIChatModel.GPT4_TURBO,\n        # or, other possibilities for example:\n        # \"litellm/ollama_chat/mistral\"\n        # \"litellm/ollama_chat/mistral:7b-instruct-v0.2-q8_0\"\n        # \"litellm/ollama/llama2\"\n        # \"local/localhost:8000/v1\"\n        # \"local/localhost:8000\"\n        chat_context_length=context_length,  # adjust based on model\n        temperature=temperature,\n        timeout=timeout,\n    )\n    llm = lm.OpenAIGPT(llm_config)\n    cl.user_session.set(\"llm_config\", llm_config)\n    cl.user_session.set(\"llm\", llm)\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.update_llm","title":"<code>update_llm(new_settings)</code>  <code>async</code>","text":"<p>Update LLMConfig and LLM from settings, and save in session state.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>@no_type_check\nasync def update_llm(new_settings: Dict[str, Any]) -&gt; None:\n    \"\"\"Update LLMConfig and LLM from settings, and save in session state.\"\"\"\n    cl.user_session.set(\"llm_settings\", new_settings)\n    await inform_llm_settings()\n    await setup_llm()\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.get_text_files","title":"<code>get_text_files(message, extensions=['.txt', '.pdf', '.doc', '.docx'])</code>  <code>async</code>","text":"<p>Get dict (file_name -&gt; file_path) from files uploaded in chat msg</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>async def get_text_files(\n    message: cl.Message,\n    extensions: List[str] = [\".txt\", \".pdf\", \".doc\", \".docx\"],\n) -&gt; Dict[str, str]:\n    \"\"\"Get dict (file_name -&gt; file_path) from files uploaded in chat msg\"\"\"\n\n    files = [file for file in message.elements if file.path.endswith(tuple(extensions))]\n    return {file.name: file.path for file in files}\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.wrap_text_preserving_structure","title":"<code>wrap_text_preserving_structure(text, width=90)</code>","text":"<p>Wrap text preserving paragraph breaks. Typically used to format an agent_response output, which may have long lines with no newlines or paragraph breaks.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def wrap_text_preserving_structure(text: str, width: int = 90) -&gt; str:\n    \"\"\"Wrap text preserving paragraph breaks. Typically used to\n    format an agent_response output, which may have long lines\n    with no newlines or paragraph breaks.\"\"\"\n\n    paragraphs = text.split(\"\\n\\n\")  # Split the text into paragraphs\n    wrapped_text = []\n\n    for para in paragraphs:\n        if para.strip():  # If the paragraph is not just whitespace\n            # Wrap this paragraph and add it to the result\n            wrapped_paragraph = textwrap.fill(para, width=width)\n            wrapped_text.append(wrapped_paragraph)\n        else:\n            # Preserve paragraph breaks\n            wrapped_text.append(\"\")\n\n    return \"\\n\\n\".join(wrapped_text)\n</code></pre>"},{"location":"reference/agent/special/","title":"special","text":"<p>langroid/agent/special/init.py </p>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent","title":"<code>RelevanceExtractorAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for extracting segments from text, that are relevant to a given query.</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def __init__(self, config: RelevanceExtractorAgentConfig):\n    super().__init__(config)\n    self.config: RelevanceExtractorAgentConfig = config\n    self.enable_message(SegmentExtractTool)\n    self.numbered_passage: Optional[str] = None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    \"\"\"\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    return super().llm_response(prompt)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM The LLM is expected to generate a structured msg according to the SegmentExtractTool schema, i.e. it should contain a <code>segment_list</code> field whose value is a list of segment numbers or ranges, like \"10,12,14-17\".</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    The LLM is expected to generate a structured msg according to the\n    SegmentExtractTool schema, i.e. it should contain a `segment_list` field\n    whose value is a list of segment numbers or ranges, like \"10,12,14-17\".\n    \"\"\"\n\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    return await super().llm_response_async(prompt)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.extract_segments","title":"<code>extract_segments(msg)</code>","text":"<p>Method to handle a segmentExtractTool message from LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def extract_segments(self, msg: SegmentExtractTool) -&gt; str:\n    \"\"\"Method to handle a segmentExtractTool message from LLM\"\"\"\n    spec = msg.segment_list\n    if len(self.message_history) == 0:\n        return DONE + \" \" + NO_ANSWER\n    if spec is None or spec.strip() in [\"\", NO_ANSWER]:\n        return DONE + \" \" + NO_ANSWER\n    assert self.numbered_passage is not None, \"No numbered passage\"\n    # assume this has numbered segments\n    try:\n        extracts = extract_numbered_segments(self.numbered_passage, spec)\n    except Exception:\n        return DONE + \" \" + NO_ANSWER\n    # this response ends the task by saying DONE\n    return DONE + \" \" + extracts\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Handle case where LLM forgets to use SegmentExtractTool</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Handle case where LLM forgets to use SegmentExtractTool\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM:\n        return DONE + \" \" + NO_ANSWER\n    else:\n        return None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent","title":"<code>DocChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: DocChatAgentConfig,\n):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    self.original_docs: List[Document] = []\n    self.original_docs_length = 0\n    self.from_dataframe = False\n    self.df_description = \"\"\n    self.chunked_docs: List[Document] = []\n    self.chunked_docs_clean: List[Document] = []\n    self.response: None | Document = None\n    if len(config.doc_paths) &gt; 0:\n        self.ingest()\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.clear","title":"<code>clear()</code>","text":"<p>Clear the document collection and the specific collection in vecdb</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the document collection and the specific collection in vecdb\"\"\"\n    self.original_docs = []\n    self.original_docs_length = 0\n    self.chunked_docs = []\n    self.chunked_docs_clean = []\n    if self.vecdb is None:\n        logger.warning(\"Attempting to clear VecDB, but VecDB not set.\")\n        return\n    collection_name = self.vecdb.config.collection_name\n    if collection_name is None:\n        return\n    try:\n        # Note we may have used a vecdb with a config.collection_name\n        # different from the agent's config.vecdb.collection_name!!\n        self.vecdb.delete_collection(collection_name)\n        self.vecdb = VectorStore.create(self.vecdb.config)\n    except Exception as e:\n        logger.warning(\n            f\"\"\"\n            Error while deleting collection {collection_name}:\n            {e}\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> <p>Returns:</p> Type Description <code>None</code> <p>dict with keys: n_splits: number of splits urls: list of urls paths: list of file paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n    \"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n\n    Returns:\n        dict with keys:\n            n_splits: number of splits\n            urls: list of urls\n            paths: list of file paths\n    \"\"\"\n    if len(self.config.doc_paths) == 0:\n        # we must be using a previously defined collection\n        # But let's get all the chunked docs so we can\n        # do keyword and other non-vector searches\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.setup_documents(filter=self.config.filter)\n        return\n    self.ingest_doc_paths(self.config.doc_paths)  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest_doc_paths","title":"<code>ingest_doc_paths(paths, metadata=[], doc_type=None)</code>","text":"<p>Split, ingest docs from specified paths, do not add these to config.doc_paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>str | bytes | List[str | bytes]</code> <p>document paths, urls or byte-content of docs. The bytes option is intended to support cases where a document has already been read in as bytes (e.g. from an API or a database), and we want to avoid having to write it to a temporary file just to read it back in.</p> required <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each path. If a single dict is passed in, it is used for all paths.</p> <code>[]</code> <code>doc_type</code> <code>str | DocumentType | None</code> <p>DocumentType to use for parsing, if known. MUST apply to all docs if specified. This is especially useful when the <code>paths</code> are of bytes type, to help with document type detection.</p> <code>None</code> <p>Returns:     List of Document objects</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_doc_paths(\n    self,\n    paths: str | bytes | List[str | bytes],\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n    doc_type: str | DocumentType | None = None,\n) -&gt; List[Document]:\n    \"\"\"Split, ingest docs from specified paths,\n    do not add these to config.doc_paths.\n\n    Args:\n        paths: document paths, urls or byte-content of docs.\n            The bytes option is intended to support cases where a document\n            has already been read in as bytes (e.g. from an API or a database),\n            and we want to avoid having to write it to a temporary file\n            just to read it back in.\n        metadata: List of metadata dicts, one for each path.\n            If a single dict is passed in, it is used for all paths.\n        doc_type: DocumentType to use for parsing, if known.\n            MUST apply to all docs if specified.\n            This is especially useful when the `paths` are of bytes type,\n            to help with document type detection.\n    Returns:\n        List of Document objects\n    \"\"\"\n    if isinstance(paths, str) or isinstance(paths, bytes):\n        paths = [paths]\n    all_paths = paths\n    paths_meta: Dict[int, Any] = {}\n    urls_meta: Dict[int, Any] = {}\n    idxs = range(len(all_paths))\n    url_idxs, path_idxs, bytes_idxs = get_urls_paths_bytes_indices(all_paths)\n    urls = [all_paths[i] for i in url_idxs]\n    paths = [all_paths[i] for i in path_idxs]\n    bytes_list = [all_paths[i] for i in bytes_idxs]\n    path_idxs.extend(bytes_idxs)\n    paths.extend(bytes_list)\n    if (isinstance(metadata, list) and len(metadata) &gt; 0) or not isinstance(\n        metadata, list\n    ):\n        if isinstance(metadata, list):\n            idx2meta = {\n                p: (\n                    m\n                    if isinstance(m, dict)\n                    else (isinstance(m, DocMetaData) and m.dict())\n                )  # appease mypy\n                for p, m in zip(idxs, metadata)\n            }\n        elif isinstance(metadata, dict):\n            idx2meta = {p: metadata for p in idxs}\n        else:\n            idx2meta = {p: metadata.dict() for p in idxs}\n        urls_meta = {u: idx2meta[u] for u in url_idxs}\n        paths_meta = {p: idx2meta[p] for p in path_idxs}\n    docs: List[Document] = []\n    parser = Parser(self.config.parsing)\n    if len(urls) &gt; 0:\n        for ui in url_idxs:\n            meta = urls_meta.get(ui, {})\n            loader = URLLoader(urls=[all_paths[ui]], parser=parser)  # type: ignore\n            url_docs = loader.load()\n            # update metadata of each doc with meta\n            for d in url_docs:\n                d.metadata = d.metadata.copy(update=meta)\n            docs.extend(url_docs)\n    if len(paths) &gt; 0:  # paths OR bytes are handled similarly\n        for pi in path_idxs:\n            meta = paths_meta.get(pi, {})\n            p = all_paths[pi]\n            path_docs = RepoLoader.get_documents(\n                p,\n                parser=parser,\n                doc_type=doc_type,\n            )\n            # update metadata of each doc with meta\n            for d in path_docs:\n                d.metadata = d.metadata.copy(update=meta)\n            docs.extend(path_docs)\n    n_docs = len(docs)\n    n_splits = self.ingest_docs(docs, split=self.config.split)\n    if n_docs == 0:\n        return []\n    n_urls = len(urls)\n    n_paths = len(paths)\n    print(\n        f\"\"\"\n    [green]I have processed the following {n_urls} URLs\n    and {n_paths} docs into {n_splits} parts:\n    \"\"\".strip()\n    )\n    path_reps = [p if isinstance(p, str) else \"bytes\" for p in paths]\n    print(\"\\n\".join([u for u in urls if isinstance(u, str)]))  # appease mypy\n    print(\"\\n\".join(path_reps))\n    return docs\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs, split=True, metadata=[])</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects</p> required <code>split</code> <code>bool</code> <p>Whether to split docs into chunks. Default is True. If False, docs are treated as \"chunks\" and are not split.</p> <code>True</code> <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each doc, to augment whatever metadata is already in the doc. [ASSUME no conflicting keys between the two metadata dicts.] If a single dict is passed in, it is used for all docs.</p> <code>[]</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(\n    self,\n    docs: List[Document],\n    split: bool = True,\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n) -&gt; int:\n    \"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n\n    Args:\n        docs: List of Document objects\n        split: Whether to split docs into chunks. Default is True.\n            If False, docs are treated as \"chunks\" and are not split.\n        metadata: List of metadata dicts, one for each doc, to augment\n            whatever metadata is already in the doc.\n            [ASSUME no conflicting keys between the two metadata dicts.]\n            If a single dict is passed in, it is used for all docs.\n    \"\"\"\n    if isinstance(metadata, list) and len(metadata) &gt; 0:\n        for d, m in zip(docs, metadata):\n            d.metadata = d.metadata.copy(\n                update=m if isinstance(m, dict) else m.dict()  # type: ignore\n            )\n    elif isinstance(metadata, dict):\n        for d in docs:\n            d.metadata = d.metadata.copy(update=metadata)\n    elif isinstance(metadata, DocMetaData):\n        for d in docs:\n            d.metadata = d.metadata.copy(update=metadata.dict())\n\n    self.original_docs.extend(docs)\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    for d in docs:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n    if split:\n        docs = self.parser.split(docs)\n    else:\n        self.parser.add_window_ids(docs)\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs for all docs, before batching.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    if len(self.config.add_fields_to_content) &gt; 0:\n        fields = [\n            f for f in extract_fields(docs[0], self.config.add_fields_to_content)\n        ]\n        if len(fields) &gt; 0:\n            for d in docs:\n                key_vals = extract_fields(d, fields)\n                d.content = (\n                    \",\".join(f\"{k}={v}\" for k, v in key_vals.items())\n                    + \",content=\"\n                    + d.content\n                )\n    docs = docs[: self.config.parsing.max_chunks]\n    # add embeddings in batches, to stay under limit of embeddings API\n    batches = list(batched(docs, self.config.embed_batch_size))\n    for batch in batches:\n        self.vecdb.add_documents(batch)\n    self.original_docs_length = self.doc_length(docs)\n    self.setup_documents(docs, filter=self.config.filter)\n    return len(docs)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.retrieval_tool","title":"<code>retrieval_tool(msg)</code>","text":"<p>Handle the RetrievalTool message</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def retrieval_tool(self, msg: RetrievalTool) -&gt; str:\n    \"\"\"Handle the RetrievalTool message\"\"\"\n    self.config.retrieve_only = True\n    self.config.parsing.n_similar_docs = msg.num_results\n    content_doc = self.answer_from_docs(msg.query)\n    return content_doc.content\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.document_compatible_dataframe","title":"<code>document_compatible_dataframe(df, content='content', metadata=[])</code>  <code>staticmethod</code>","text":"<p>Convert dataframe so it is compatible with Document class: - has \"content\" column - has an \"id\" column to be used as Document.metadata.id</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to convert</p> required <code>content</code> <code>str</code> <p>name of content column</p> <code>'content'</code> <code>metadata</code> <code>List[str]</code> <p>list of metadata column names</p> <code>[]</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, List[str]]</code> <p>Tuple[pd.DataFrame, List[str]]: dataframe, metadata - dataframe: dataframe with \"content\" column and \"id\" column - metadata: list of metadata column names, including \"id\"</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef document_compatible_dataframe(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    Convert dataframe so it is compatible with Document class:\n    - has \"content\" column\n    - has an \"id\" column to be used as Document.metadata.id\n\n    Args:\n        df: dataframe to convert\n        content: name of content column\n        metadata: list of metadata column names\n\n    Returns:\n        Tuple[pd.DataFrame, List[str]]: dataframe, metadata\n            - dataframe: dataframe with \"content\" column and \"id\" column\n            - metadata: list of metadata column names, including \"id\"\n    \"\"\"\n    if content not in df.columns:\n        raise ValueError(\n            f\"\"\"\n            Content column {content} not in dataframe,\n            so we cannot ingest into the DocChatAgent.\n            Please specify the `content` parameter as a suitable\n            text-based column in the dataframe.\n            \"\"\"\n        )\n    if content != \"content\":\n        # rename content column to \"content\", leave existing column intact\n        df = df.rename(columns={content: \"content\"}, inplace=False)\n\n    actual_metadata = metadata.copy()\n    if \"id\" not in df.columns:\n        docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n        ids = [str(d.id()) for d in docs]\n        df[\"id\"] = ids\n\n    if \"id\" not in actual_metadata:\n        actual_metadata += [\"id\"]\n\n    return df, actual_metadata\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest a dataframe into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"\n    Ingest a dataframe into vecdb.\n    \"\"\"\n    self.from_dataframe = True\n    self.df_description = describe_dataframe(\n        df, filter_fields=self.config.filter_fields, n_vals=5\n    )\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    # When ingesting a dataframe we will no longer do any chunking,\n    # so we mark each doc as a chunk.\n    # TODO - revisit this since we may still want to chunk large text columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return self.ingest_docs(docs)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.setup_documents","title":"<code>setup_documents(docs=[], filter=None)</code>","text":"<p>Setup <code>self.chunked_docs</code> and <code>self.chunked_docs_clean</code> based on possible filter. These will be used in various non-vector-based search functions, e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects. This is empty when we are calling this method after initial doc ingestion.</p> <code>[]</code> <code>filter</code> <code>str | None</code> <p>Filter condition for various lexical/semantic search fns.</p> <code>None</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def setup_documents(\n    self,\n    docs: List[Document] = [],\n    filter: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Setup `self.chunked_docs` and `self.chunked_docs_clean`\n    based on possible filter.\n    These will be used in various non-vector-based search functions,\n    e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.\n\n    Args:\n        docs: List of Document objects. This is empty when we are calling this\n            method after initial doc ingestion.\n        filter: Filter condition for various lexical/semantic search fns.\n    \"\"\"\n    if filter is None and len(docs) &gt; 0:\n        # no filter, so just use the docs passed in\n        self.chunked_docs.extend(docs)\n    else:\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.chunked_docs = self.vecdb.get_all_documents(where=filter or \"\")\n\n    self.chunked_docs_clean = [\n        Document(content=preprocess_text(d.content), metadata=d.metadata)\n        for d in self.chunked_docs\n    ]\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_field_values","title":"<code>get_field_values(fields)</code>","text":"<p>Get string-listing of possible values of each filterable field, e.g. {     \"genre\": \"crime, drama, mystery, ... (10 more)\",     \"certificate\": \"R, PG-13, PG, R\", }</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_field_values(self, fields: list[str]) -&gt; Dict[str, str]:\n    \"\"\"Get string-listing of possible values of each filterable field,\n    e.g.\n    {\n        \"genre\": \"crime, drama, mystery, ... (10 more)\",\n        \"certificate\": \"R, PG-13, PG, R\",\n    }\n    \"\"\"\n    field_values: Dict[str, Set[str]] = {}\n    # make empty set for each field\n    for f in fields:\n        field_values[f] = set()\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # get all documents and accumulate possible values of each field until 10\n    docs = self.vecdb.get_all_documents()  # only works for vecdbs that support this\n    for d in docs:\n        # extract fields from d\n        doc_field_vals = extract_fields(d, fields)\n        for field, val in doc_field_vals.items():\n            field_values[field].add(val)\n    # For each field make a string showing list of possible values,\n    # truncate to 20 values, and if there are more, indicate how many\n    # more there are, e.g. Genre: crime, drama, mystery, ... (20 more)\n    field_values_list = {}\n    for f in fields:\n        vals = list(field_values[f])\n        n = len(vals)\n        remaining = n - 20\n        vals = vals[:20]\n        if n &gt; 20:\n            vals.append(f\"(...{remaining} more)\")\n        # make a string of the values, ensure they are strings\n        field_values_list[f] = \", \".join(str(v) for v in vals)\n    return field_values_list\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs Args:     docs: list of Document objects Returns:     int: number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n    \"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    return self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.user_docs_ingest_dialog","title":"<code>user_docs_ingest_dialog()</code>","text":"<p>Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def user_docs_ingest_dialog(self) -&gt; None:\n    \"\"\"\n    Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    n_deletes = self.vecdb.clear_empty_collections()\n    collections = self.vecdb.list_collections()\n    collection_name = \"NEW\"\n    is_new_collection = False\n    replace_collection = False\n    if len(collections) &gt; 0:\n        n = len(collections)\n        delete_str = (\n            f\"(deleted {n_deletes} empty collections)\" if n_deletes &gt; 0 else \"\"\n        )\n        print(f\"Found {n} collections: {delete_str}\")\n        for i, option in enumerate(collections, start=1):\n            print(f\"{i}. {option}\")\n        while True:\n            choice = Prompt.ask(\n                f\"Enter 1-{n} to select a collection, \"\n                \"or hit ENTER to create a NEW collection, \"\n                \"or -1 to DELETE ALL COLLECTIONS\",\n                default=\"0\",\n            )\n            try:\n                if -1 &lt;= int(choice) &lt;= n:\n                    break\n            except Exception:\n                pass\n\n        if choice == \"-1\":\n            confirm = Prompt.ask(\n                \"Are you sure you want to delete all collections?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            if confirm == \"y\":\n                self.vecdb.clear_all_collections(really=True)\n                collection_name = \"NEW\"\n\n        if int(choice) &gt; 0:\n            collection_name = collections[int(choice) - 1]\n            print(f\"Using collection {collection_name}\")\n            choice = Prompt.ask(\n                \"Would you like to replace this collection?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            replace_collection = choice == \"y\"\n\n    if collection_name == \"NEW\":\n        is_new_collection = True\n        collection_name = Prompt.ask(\n            \"What would you like to name the NEW collection?\",\n            default=\"doc-chat\",\n        )\n\n    self.vecdb.set_collection(collection_name, replace=replace_collection)\n\n    default_urls_str = (\n        \" (or leave empty for default URLs)\" if is_new_collection else \"\"\n    )\n    print(f\"[blue]Enter some URLs or file/dir paths below {default_urls_str}\")\n    inputs = get_list_from_user()\n    if len(inputs) == 0:\n        if is_new_collection:\n            inputs = self.config.default_paths\n    self.config.doc_paths = inputs  # type: ignore\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs. Args:     docs: list of Document objects Returns:     str: string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n    \"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\n    contents = [f\"Extract: {d.content}\" for d in docs]\n    sources = [d.metadata.source for d in docs]\n    sources = [f\"Source: {s}\" if s is not None else \"\" for s in sources]\n    return \"\\n\".join(\n        [\n            f\"\"\"\n            [{i+1}]\n            {content}\n            {source}\n            \"\"\"\n            for i, (content, source) in enumerate(zip(contents, sources))\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible Args:     question: question to answer     passages: list of <code>Document</code> objects each containing a possibly relevant         snippet, and metadata Returns:     a <code>Document</code> object containing the answer,     and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(\n    self, question: str, passages: List[Document]\n) -&gt; ChatDocument:\n    \"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n\n    \"\"\"\n\n    passages_str = self.doc_string(passages)\n    # Substitute Q and P into the templatized prompt\n\n    final_prompt = self.config.summarize_prompt.format(\n        question=question, extracts=passages_str\n    )\n    show_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n\n    # Generate the final verbatim extract based on the final prompt.\n    # Note this will send entire message history, plus this final_prompt\n    # to the LLM, and self.message_history will be updated to include\n    # 2 new LLMMessage objects:\n    # one for `final_prompt`, and one for the LLM response\n\n    if self.config.conversation_mode:\n        # respond with temporary context\n        answer_doc = super()._llm_response_temp_context(question, final_prompt)\n    else:\n        answer_doc = super().llm_response_forget(final_prompt)\n\n    final_answer = answer_doc.content.strip()\n    show_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\n\n    citations = extract_markdown_references(final_answer)\n\n    citations_str = \"\"\n    if len(citations) &gt; 0:\n        # append [i] source, content for each citation\n        citations_str = \"\\n\".join(\n            [\n                f\"[^{c}] {passages[c-1].metadata.source}\"\n                f\"\\n{format_footnote_text(passages[c-1].content)}\"\n                for c in citations\n            ]\n        )\n\n    return ChatDocument(\n        content=final_answer,  # does not contain citations\n        metadata=ChatDocMetaData(\n            source=citations_str,  # only the citations\n            sender=Entity.LLM,\n            has_citation=len(citations) &gt; 0,\n            cached=getattr(answer_doc.metadata, \"cached\", False),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.rerank_with_diversity","title":"<code>rerank_with_diversity(passages)</code>","text":"<p>Rerank a list of items in such a way that each successive item is least similar (on average) to the earlier items.</p> <p>Args: query (str): The query for which the passages are relevant. passages (List[Document]): A list of Documents to be reranked.</p> <p>Returns: List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_with_diversity(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank a list of items in such a way that each successive item is least similar\n    (on average) to the earlier items.\n\n    Args:\n    query (str): The query for which the passages are relevant.\n    passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n    List[Documents]: A reranked list of Documents.\n    \"\"\"\n\n    if self.vecdb is None:\n        logger.warning(\"No vecdb; cannot use rerank_with_diversity\")\n        return passages\n    emb_model = self.vecdb.embedding_model\n    emb_fn = emb_model.embedding_fn()\n    embs = emb_fn([p.content for p in passages])\n    embs_arr = [np.array(e) for e in embs]\n    indices = list(range(len(passages)))\n\n    # Helper function to compute average similarity to\n    # items in the current result list.\n    def avg_similarity_to_result(i: int, result: List[int]) -&gt; float:\n        return sum(  # type: ignore\n            (embs_arr[i] @ embs_arr[j])\n            / (np.linalg.norm(embs_arr[i]) * np.linalg.norm(embs_arr[j]))\n            for j in result\n        ) / len(result)\n\n    # copy passages to items\n    result = [indices.pop(0)]  # Start with the first item.\n\n    while indices:\n        # Find the item that has the least average similarity\n        # to items in the result list.\n        least_similar_item = min(\n            indices, key=lambda i: avg_similarity_to_result(i, result)\n        )\n        result.append(least_similar_item)\n        indices.remove(least_similar_item)\n\n    # return passages in order of result list\n    return [passages[i] for i in result]\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.rerank_to_periphery","title":"<code>rerank_to_periphery(passages)</code>","text":"<p>Rerank to avoid Lost In the Middle (LIM) problem, where LLMs pay more attention to items at the ends of a list, rather than the middle. So we re-rank to make the best passages appear at the periphery of the list. https://arxiv.org/abs/2307.03172</p> <p>Example reranking: 1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>A list of Documents to be reranked.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_to_periphery(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank to avoid Lost In the Middle (LIM) problem,\n    where LLMs pay more attention to items at the ends of a list,\n    rather than the middle. So we re-rank to make the best passages\n    appear at the periphery of the list.\n    https://arxiv.org/abs/2307.03172\n\n    Example reranking:\n    1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2\n\n    Args:\n        passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n        List[Documents]: A reranked list of Documents.\n\n    \"\"\"\n    # Splitting items into odds and evens based on index, not value\n    odds = passages[::2]\n    evens = passages[1::2][::-1]\n\n    # Merging them back together\n    return odds + evens\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.add_context_window","title":"<code>add_context_window(docs_scores)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. We use these stored window_ids to retrieve the desired number (self.config.n_neighbor_chunks) of neighbors on either side of the current chunk.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def add_context_window(\n    self,\n    docs_scores: List[Tuple[Document, float]],\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk. We use these stored\n    window_ids to retrieve the desired number\n    (self.config.n_neighbor_chunks) of neighbors\n    on either side of the current chunk.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None or self.config.n_neighbor_chunks == 0:\n        return docs_scores\n    if len(docs_scores) == 0:\n        return []\n    if set(docs_scores[0][0].__fields__) != {\"content\", \"metadata\"}:\n        # Do not add context window when there are other fields besides just\n        # content and metadata, since we do not know how to set those other fields\n        # for newly created docs with combined content.\n        return docs_scores\n    return self.vecdb.add_context_window(docs_scores, self.config.n_neighbor_chunks)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_semantic_search_results","title":"<code>get_semantic_search_results(query, k=10)</code>","text":"<p>Get semantic search results from vecdb. Args:     query (str): query to search for     k (int): number of results to return Returns:     List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_semantic_search_results(\n    self,\n    query: str,\n    k: int = 10,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Get semantic search results from vecdb.\n    Args:\n        query (str): query to search for\n        k (int): number of results to return\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # Note: for dynamic filtering based on a query, users can\n    # use the `temp_update` context-manager to pass in a `filter` to self.config,\n    # e.g.:\n    # with temp_update(self.config, {\"filter\": \"metadata.source=='source1'\"}):\n    #     docs_scores = self.get_semantic_search_results(query, k=k)\n    # This avoids having pass the `filter` argument to every function call\n    # upstream of this one.\n    # The `temp_update` context manager is defined in\n    # `langroid/utils/pydantic_utils.py`\n    return self.vecdb.similar_texts_with_scores(\n        query,\n        k=k,\n        where=self.config.filter,\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_relevant_chunks","title":"<code>get_relevant_chunks(query, query_proxies=[])</code>","text":"<p>The retrieval stage in RAG: get doc-chunks that are most \"relevant\" to the query (and possibly any proxy queries), from the document-store, which currently is the vector store, but in theory could be any document store, or even web-search. This stage does NOT involve an LLM, and the retrieved chunks could either be pre-chunked text (from the initial pre-processing stage where chunks were stored in the vector store), or they could be dynamically retrieved based on a window around a lexical match.</p> <p>These are the steps (some optional based on config): - semantic search based on vector-embedding distance, from vecdb - lexical search using bm25-ranking (keyword similarity) - fuzzy matching (keyword similarity) - re-ranking of doc-chunks by relevance to query, using cross-encoder,    and pick top k</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>original query (assumed to be in stand-alone form)</p> required <code>query_proxies</code> <code>List[str]</code> <p>possible rephrases, or hypothetical answer to query     (e.g. for HyDE-type retrieval)</p> <code>[]</code> <p>Returns:</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_relevant_chunks(\n    self, query: str, query_proxies: List[str] = []\n) -&gt; List[Document]:\n    \"\"\"\n    The retrieval stage in RAG: get doc-chunks that are most \"relevant\"\n    to the query (and possibly any proxy queries), from the document-store,\n    which currently is the vector store,\n    but in theory could be any document store, or even web-search.\n    This stage does NOT involve an LLM, and the retrieved chunks\n    could either be pre-chunked text (from the initial pre-processing stage\n    where chunks were stored in the vector store), or they could be\n    dynamically retrieved based on a window around a lexical match.\n\n    These are the steps (some optional based on config):\n    - semantic search based on vector-embedding distance, from vecdb\n    - lexical search using bm25-ranking (keyword similarity)\n    - fuzzy matching (keyword similarity)\n    - re-ranking of doc-chunks by relevance to query, using cross-encoder,\n       and pick top k\n\n    Args:\n        query: original query (assumed to be in stand-alone form)\n        query_proxies: possible rephrases, or hypothetical answer to query\n                (e.g. for HyDE-type retrieval)\n\n    Returns:\n\n    \"\"\"\n    # if we are using cross-encoder reranking, we can retrieve more docs\n    # during retrieval, and leave it to the cross-encoder re-ranking\n    # to whittle down to self.config.parsing.n_similar_docs\n    retrieval_multiple = 1 if self.config.cross_encoder_reranking_model == \"\" else 3\n\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    with status(\"[cyan]Searching VecDB for relevant doc passages...\"):\n        docs_and_scores: List[Tuple[Document, float]] = []\n        for q in [query] + query_proxies:\n            docs_and_scores += self.get_semantic_search_results(\n                q,\n                k=self.config.parsing.n_similar_docs * retrieval_multiple,\n            )\n    # keep only docs with unique d.id()\n    id2doc_score = {d.id(): (d, s) for d, s in docs_and_scores}\n    docs_and_scores = list(id2doc_score.values())\n    passages = [d for (d, _) in docs_and_scores]\n    # passages = [\n    #     Document(content=d.content, metadata=d.metadata)\n    #     for (d, _) in docs_and_scores\n    # ]\n\n    if self.config.use_bm25_search:\n        docs_scores = self.get_similar_chunks_bm25(query, retrieval_multiple)\n        passages += [d for (d, _) in docs_scores]\n\n    if self.config.use_fuzzy_match:\n        fuzzy_match_docs = self.get_fuzzy_matches(query, retrieval_multiple)\n        passages += fuzzy_match_docs\n\n    # keep unique passages\n    id2passage = {p.id(): p for p in passages}\n    passages = list(id2passage.values())\n\n    if len(passages) == 0:\n        return []\n\n    if self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n    # now passages can potentially have a lot of doc chunks,\n    # so we re-rank them using a cross-encoder scoring model,\n    # and pick top k where k = config.parsing.n_similar_docs\n    # https://www.sbert.net/examples/applications/retrieve_rerank\n    if self.config.cross_encoder_reranking_model != \"\":\n        passages = self.rerank_with_cross_encoder(query, passages)\n\n    if self.config.rerank_diversity:\n        # reorder to increase diversity among top docs\n        passages = self.rerank_with_diversity(passages)\n\n    if self.config.rerank_periphery:\n        # reorder so most important docs are at periphery\n        # (see Lost In the Middle issue).\n        passages = self.rerank_to_periphery(passages)\n\n    if not self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n\n    return passages\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_relevant_extracts","title":"<code>get_relevant_extracts(query)</code>","text":"<p>Get list of (verbatim) extracts from doc-chunks relevant to answering a query.</p> <p>These are the stages (some optional based on config): - use LLM to convert query to stand-alone query - optionally use LLM to rephrase query to use below - optionally use LLM to generate hypothetical answer (HyDE) to use below. - get_relevant_chunks(): get doc-chunks relevant to query and proxies - use LLM to get relevant extracts from doc-chunks</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to search for</p> required <p>Returns:</p> Name Type Description <code>query</code> <code>str</code> <p>stand-alone version of input query</p> <code>List[Document]</code> <p>List[Document]: list of relevant extracts</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef get_relevant_extracts(self, query: str) -&gt; Tuple[str, List[Document]]:\n    \"\"\"\n    Get list of (verbatim) extracts from doc-chunks relevant to answering a query.\n\n    These are the stages (some optional based on config):\n    - use LLM to convert query to stand-alone query\n    - optionally use LLM to rephrase query to use below\n    - optionally use LLM to generate hypothetical answer (HyDE) to use below.\n    - get_relevant_chunks(): get doc-chunks relevant to query and proxies\n    - use LLM to get relevant extracts from doc-chunks\n\n    Args:\n        query (str): query to search for\n\n    Returns:\n        query (str): stand-alone version of input query\n        List[Document]: list of relevant extracts\n\n    \"\"\"\n    if len(self.dialog) &gt; 0 and not self.config.assistant_mode:\n        # Regardless of whether we are in conversation mode or not,\n        # for relevant doc/chunk extraction, we must convert the query\n        # to a standalone query to get more relevant results.\n        with status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\n            with StreamingIfAllowed(self.llm, False):\n                query = self.llm.followup_to_standalone(self.dialog, query)\n        print(f\"[orange2]New query: {query}\")\n\n    proxies = []\n    if self.config.hypothetical_answer:\n        answer = self.llm_hypothetical_answer(query)\n        proxies = [answer]\n\n    if self.config.n_query_rephrases &gt; 0:\n        rephrases = self.llm_rephrase_query(query)\n        proxies += rephrases\n\n    passages = self.get_relevant_chunks(query, proxies)  # no LLM involved\n\n    if len(passages) == 0:\n        return query, []\n\n    with status(\"[cyan]LLM Extracting verbatim passages...\"):\n        with StreamingIfAllowed(self.llm, False):\n            # these are async calls, one per passage; turn off streaming\n            extracts = self.get_verbatim_extracts(query, passages)\n            extracts = [e for e in extracts if e.content != NO_ANSWER]\n\n    return query, extracts\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_verbatim_extracts","title":"<code>get_verbatim_extracts(query, passages)</code>","text":"<p>Run RelevanceExtractorAgent in async/concurrent mode on passages, to extract portions relevant to answering query, from each passage. Args:     query (str): query to answer     passages (List[Documents]): list of passages to extract from</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Documents containing extracts and metadata.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_verbatim_extracts(\n    self,\n    query: str,\n    passages: List[Document],\n) -&gt; List[Document]:\n    \"\"\"\n    Run RelevanceExtractorAgent in async/concurrent mode on passages,\n    to extract portions relevant to answering query, from each passage.\n    Args:\n        query (str): query to answer\n        passages (List[Documents]): list of passages to extract from\n\n    Returns:\n        List[Document]: list of Documents containing extracts and metadata.\n    \"\"\"\n    agent_cfg = self.config.relevance_extractor_config\n    if agent_cfg is None:\n        # no relevance extraction: simply return passages\n        return passages\n    if agent_cfg.llm is None:\n        # Use main DocChatAgent's LLM if not provided explicitly:\n        # this reduces setup burden on the user\n        agent_cfg.llm = self.config.llm\n    agent_cfg.query = query\n    agent_cfg.segment_length = self.config.extraction_granularity\n    agent_cfg.llm.stream = False  # disable streaming for concurrent calls\n\n    agent = RelevanceExtractorAgent(agent_cfg)\n    task = Task(\n        agent,\n        name=\"Relevance-Extractor\",\n        interactive=False,\n    )\n\n    extracts = run_batch_tasks(\n        task,\n        passages,\n        input_map=lambda msg: msg.content,\n        output_map=lambda ans: ans.content if ans is not None else NO_ANSWER,\n    )\n\n    # Caution: Retain ALL other fields in the Documents (which could be\n    # other than just `content` and `metadata`), while simply replacing\n    # `content` with the extracted portions\n    passage_extracts = []\n    for p, e in zip(passages, extracts):\n        if e == NO_ANSWER or len(e) == 0:\n            continue\n        p_copy = p.copy()\n        p_copy.content = e\n        passage_extracts.append(p_copy)\n\n    return passage_extracts\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on relevant docs from the VecDB</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to answer</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>ChatDocument</code> <p>answer</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def answer_from_docs(self, query: str) -&gt; ChatDocument:\n    \"\"\"\n    Answer query based on relevant docs from the VecDB\n\n    Args:\n        query (str): query to answer\n\n    Returns:\n        Document: answer\n    \"\"\"\n    response = ChatDocument(\n        content=NO_ANSWER,\n        metadata=ChatDocMetaData(\n            source=\"None\",\n            sender=Entity.LLM,\n        ),\n    )\n    # query may be updated to a stand-alone version\n    query, extracts = self.get_relevant_extracts(query)\n    if len(extracts) == 0:\n        return response\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if self.config.retrieve_only:\n        # only return extracts, skip LLM-based summary answer\n        meta = dict(\n            sender=Entity.LLM,\n        )\n        # copy metadata from first doc, unclear what to do here.\n        meta.update(extracts[0].metadata)\n        return ChatDocument(\n            content=\"\\n\\n\".join([e.content for e in extracts]),\n            metadata=ChatDocMetaData(**meta),  # type: ignore\n        )\n    response = self.get_summary_answer(query, extracts)\n\n    self.update_dialog(query, response.content)\n    self.response = response  # save last response\n    return response\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\n    self,\n    instruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n    \"\"\"Summarize all docs\"\"\"\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if len(self.original_docs) == 0:\n        logger.warning(\n            \"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection?\n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs.\n            \"\"\"\n        )\n        return None\n    full_text = \"\\n\\n\".join([d.content for d in self.original_docs])\n    if self.parser is None:\n        raise ValueError(\"No parser defined\")\n    tot_tokens = self.parser.num_tokens(full_text)\n    MAX_INPUT_TOKENS = (\n        self.llm.completion_context_length()\n        - self.config.llm.max_output_tokens\n        - 100\n    )\n    if tot_tokens &gt; MAX_INPUT_TOKENS:\n        # truncate\n        full_text = self.parser.tokenizer.decode(\n            self.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n        )\n        logger.warning(\n            f\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n        )\n    prompt = f\"\"\"\n    {instruction}\n\n    FULL TEXT:\n    {full_text}\n    \"\"\".strip()\n    with StreamingIfAllowed(self.llm):\n        summary = ChatAgent.llm_response(self, prompt)\n        return summary\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; ChatDocument | None:\n    \"\"\"Show evidence for last response\"\"\"\n    if self.response is None:\n        print(\"[magenta]No response yet\")\n        return None\n    source = self.response.metadata.source\n    if len(source) &gt; 0:\n        print(\"[magenta]\" + source)\n    else:\n        print(\"[magenta]No source found\")\n    return None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RetrieverAgent","title":"<code>RetrieverAgent(config)</code>","text":"<p>             Bases: <code>DocChatAgent</code></p> <p>Agent for just retrieving chunks/docs/extracts matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def __init__(self, config: DocChatAgentConfig):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    logger.warning(\n        \"\"\"\n    `RetrieverAgent` is deprecated. Use `DocChatAgent` instead, with\n    `DocChatAgentConfig.retrieve_only=True`, and if you want to retrieve\n    FULL relevant doc-contents rather than just extracts, then set\n    `DocChatAgentConfig.extraction_granularity=-1`\n    \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent","title":"<code>LanceDocChatAgent(cfg)</code>","text":"<p>             Bases: <code>DocChatAgent</code></p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def __init__(self, cfg: DocChatAgentConfig):\n    super().__init__(cfg)\n    self.config: DocChatAgentConfig = cfg\n    self.enable_message(QueryPlanTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent.query_plan","title":"<code>query_plan(msg)</code>","text":"<p>Handle the LLM's use of the FilterTool. Temporarily set the config filter and either return the final answer in case there's a dataframe_calc, or return the rephrased query so the LLM can handle it.</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def query_plan(self, msg: QueryPlanTool) -&gt; str:\n    \"\"\"\n    Handle the LLM's use of the FilterTool.\n    Temporarily set the config filter and either return the final answer\n    in case there's a dataframe_calc, or return the rephrased query\n    so the LLM can handle it.\n    \"\"\"\n    # create document-subset based on this filter\n    plan = msg.plan\n    try:\n        self.setup_documents(filter=plan.filter or None)\n    except Exception as e:\n        logger.error(f\"Error setting up documents: {e}\")\n        # say DONE with err msg so it goes back to LanceFilterAgent\n        return f\"\"\"\n        {DONE} Possible Filter Error:\\n {e}\n\n        Note that only the following fields are allowed in the filter\n        of a query plan: \n        {\", \".join(self.config.filter_fields)}\n        \"\"\"\n\n    # update the filter so it is used in the DocChatAgent\n    self.config.filter = plan.filter or None\n    if plan.dataframe_calc:\n        # we just get relevant docs then do the calculation\n        # TODO if calc causes err, it is captured in result,\n        # and LLM can correct the calc based on the err,\n        # and this will cause retrieval all over again,\n        # which may be wasteful if only the calc part is wrong.\n        # The calc step can later be done with a separate Agent/Tool.\n        if plan.query is None or plan.query.strip() == \"\":\n            if plan.filter is None or plan.filter.strip() == \"\":\n                return \"\"\"DONE\n                Cannot execute Query Plan since filter as well as \n                rephrased query are empty.\n                \"\"\"\n            else:\n                # no query to match, so just get all docs matching filter\n                docs = self.vecdb.get_all_documents(plan.filter)\n        else:\n            _, docs = self.get_relevant_extracts(plan.query)\n        if len(docs) == 0:\n            return DONE + \" \" + NO_ANSWER\n        result = self.vecdb.compute_from_docs(docs, plan.dataframe_calc)\n        return DONE + \" \" + result\n    else:\n        # pass on the query so LLM can handle it\n        return plan.query\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest from a dataframe. Assume we are doing this once, not incrementally</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"Ingest from a dataframe. Assume we are doing this once, not incrementally\"\"\"\n\n    self.from_dataframe = True\n    if df.shape[0] == 0:\n        raise ValueError(\n            \"\"\"\n            LanceDocChatAgent.ingest_dataframe() received an empty dataframe.\n            \"\"\"\n        )\n    n = df.shape[0]\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs, into the `content` field for all rows.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    fields = [f for f in self.config.add_fields_to_content if f in df.columns]\n    if len(fields) &gt; 0:\n        df[content] = df.apply(\n            lambda row: (\",\".join(f\"{f}={row[f]}\" for f in fields))\n            + \", content=\"\n            + row[content],\n            axis=1,\n        )\n\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    self.df_description = describe_dataframe(\n        df,\n        filter_fields=self.config.filter_fields,\n        n_vals=10,\n    )\n    self.vecdb.add_dataframe(df, content=\"content\", metadata=metadata)\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    # We assume \"content\" is available as top-level field\n    if \"content\" in tbl.schema.names:\n        tbl.create_fts_index(\"content\", replace=True)\n    # We still need to do the below so that\n    # other types of searches in DocChatAgent\n    # can work, as they require Document objects\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    self.setup_documents(docs)\n    # mark each doc as already-chunked so we don't try to split them further\n    # TODO later we may want to split large text-columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return n  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent.get_similar_chunks_bm25","title":"<code>get_similar_chunks_bm25(query, multiple)</code>","text":"<p>Override the DocChatAgent.get_similar_chunks_bm25() to use LanceDB FTS (Full Text Search).</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def get_similar_chunks_bm25(\n    self, query: str, multiple: int\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Override the DocChatAgent.get_similar_chunks_bm25()\n    to use LanceDB FTS (Full Text Search).\n    \"\"\"\n    # Clean up query: replace all newlines with spaces in query,\n    # force special search keywords to lower case, remove quotes,\n    # so it's not interpreted as search syntax\n    query_clean = (\n        query.replace(\"\\n\", \" \")\n        .replace(\"AND\", \"and\")\n        .replace(\"OR\", \"or\")\n        .replace(\"NOT\", \"not\")\n        .replace(\"'\", \"\")\n        .replace('\"', \"\")\n    )\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    result = (\n        tbl.search(query_clean)\n        .where(self.config.filter or None)\n        .limit(self.config.parsing.n_similar_docs * multiple)\n    )\n    docs = self.vecdb._lance_result_to_docs(result)\n    scores = [r[\"score\"] for r in result.to_list()]\n    return list(zip(docs, scores))\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.TableChatAgent","title":"<code>TableChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def __init__(self, config: TableChatAgentConfig):\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n    else:\n        df = read_tabular_data(config.data, config.separator)\n\n    df.columns = df.columns.str.strip().str.replace(\" +\", \"_\", regex=True)\n\n    self.df = df\n    summary = dataframe_summary(df)\n    config.system_message = config.system_message.format(summary=summary)\n\n    super().__init__(config)\n    self.config: TableChatAgentConfig = config\n\n    logger.info(\n        f\"\"\"TableChatAgent initialized with dataframe of shape {self.df.shape}\n        and columns: \n        {self.df.columns}\n        \"\"\"\n    )\n    # enable the agent to use and handle the PandasEvalTool\n    self.enable_message(PandasEvalTool)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.TableChatAgent.pandas_eval","title":"<code>pandas_eval(msg)</code>","text":"<p>Handle a PandasEvalTool message by evaluating the <code>expression</code> field     and returning the result. Args:     msg (PandasEvalTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of running the code along with any print output.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def pandas_eval(self, msg: PandasEvalTool) -&gt; str:\n    \"\"\"\n    Handle a PandasEvalTool message by evaluating the `expression` field\n        and returning the result.\n    Args:\n        msg (PandasEvalTool): The tool-message to handle.\n\n    Returns:\n        str: The result of running the code along with any print output.\n    \"\"\"\n    self.sent_expression = True\n    exprn = msg.expression\n    local_vars = {\"df\": self.df}\n    # Create a string-based I/O stream\n    code_out = io.StringIO()\n\n    # Temporarily redirect standard output to our string-based I/O stream\n    sys.stdout = code_out\n\n    # Evaluate the last line and get the result\n    try:\n        eval_result = pd.eval(exprn, local_dict=local_vars)\n    except Exception as e:\n        eval_result = f\"ERROR: {type(e)}: {e}\"\n\n    if eval_result is None:\n        eval_result = \"\"\n\n    # Always restore the original standard output\n    sys.stdout = sys.__stdout__\n\n    # If df has been modified in-place, save the changes back to self.df\n    self.df = local_vars[\"df\"]\n\n    # Get the resulting string from the I/O stream\n    print_result = code_out.getvalue() or \"\"\n    sep = \"\\n\" if print_result else \"\"\n    # Combine the print and eval results\n    result = f\"{print_result}{sep}{eval_result}\"\n    if result == \"\":\n        result = \"No result\"\n    # Return the result\n    return result\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.TableChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Handle various LLM deviations</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Handle various LLM deviations\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == lr.Entity.LLM:\n        if msg.content.strip() == DONE and self.sent_expression:\n            # LLM sent an expression (i.e. used the `pandas_eval` tool)\n            # but upon receiving the results, simply said DONE without\n            # narrating the result as instructed.\n            return \"\"\"\n                You forgot to PRESENT the answer to the user's query\n                based on the results from `pandas_eval` tool.\n            \"\"\"\n        if self.sent_expression:\n            # LLM forgot to say DONE\n            self.sent_expression = False\n            return DONE + \" \" + PASS\n        else:\n            # LLM forgot to use the `pandas_eval` tool\n            return \"\"\"\n                You forgot to use the `pandas_eval` tool/function \n                to find the answer.\n                Try again using the `pandas_eval` tool/function.\n                \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.PandasEvalTool","title":"<code>PandasEvalTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Tool/function to evaluate a pandas expression involving a dataframe <code>df</code></p>"},{"location":"reference/agent/special/#langroid.agent.special.dataframe_summary","title":"<code>dataframe_summary(df)</code>","text":"<p>Generate a structured summary for a pandas DataFrame containing numerical and categorical values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to summarize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A nicely structured and formatted summary string.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>@no_type_check\ndef dataframe_summary(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Generate a structured summary for a pandas DataFrame containing numerical\n    and categorical values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to summarize.\n\n    Returns:\n        str: A nicely structured and formatted summary string.\n    \"\"\"\n\n    # Column names display\n    col_names_str = (\n        \"COLUMN NAMES:\\n\" + \" \".join([f\"'{col}'\" for col in df.columns]) + \"\\n\\n\"\n    )\n\n    # Numerical data summary\n    num_summary = df.describe().map(lambda x: \"{:.2f}\".format(x))\n    num_str = \"Numerical Column Summary:\\n\" + num_summary.to_string() + \"\\n\\n\"\n\n    # Categorical data summary\n    cat_columns = df.select_dtypes(include=[np.object_]).columns\n    cat_summary_list = []\n\n    for col in cat_columns:\n        unique_values = df[col].unique()\n        if len(unique_values) &lt; 10:\n            cat_summary_list.append(f\"'{col}': {', '.join(map(str, unique_values))}\")\n        else:\n            cat_summary_list.append(f\"'{col}': {df[col].nunique()} unique values\")\n\n    cat_str = \"Categorical Column Summary:\\n\" + \"\\n\".join(cat_summary_list) + \"\\n\\n\"\n\n    # Missing values summary\n    nan_summary = df.isnull().sum().rename(\"missing_values\").to_frame()\n    nan_str = \"Missing Values Column Summary:\\n\" + nan_summary.to_string() + \"\\n\"\n\n    # Combine the summaries into one structured string\n    summary_str = col_names_str + num_str + cat_str + nan_str\n\n    return summary_str\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/","title":"doc_chat_agent","text":"<p>langroid/agent/special/doc_chat_agent.py </p> <p>Agent that supports asking queries about a set of documents, using retrieval-augmented generation (RAG).</p> <p>Functionality includes: - summarizing a document, with a custom instruction; see <code>summarize_docs</code> - asking a question about a document; see <code>answer_from_docs</code></p> <p>Note: to use the sentence-transformer embeddings, you must install langroid with the [hf-embeddings] extra, e.g.:</p> <p>pip install \"langroid[hf-embeddings]\"</p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent","title":"<code>DocChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: DocChatAgentConfig,\n):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    self.original_docs: List[Document] = []\n    self.original_docs_length = 0\n    self.from_dataframe = False\n    self.df_description = \"\"\n    self.chunked_docs: List[Document] = []\n    self.chunked_docs_clean: List[Document] = []\n    self.response: None | Document = None\n    if len(config.doc_paths) &gt; 0:\n        self.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.clear","title":"<code>clear()</code>","text":"<p>Clear the document collection and the specific collection in vecdb</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the document collection and the specific collection in vecdb\"\"\"\n    self.original_docs = []\n    self.original_docs_length = 0\n    self.chunked_docs = []\n    self.chunked_docs_clean = []\n    if self.vecdb is None:\n        logger.warning(\"Attempting to clear VecDB, but VecDB not set.\")\n        return\n    collection_name = self.vecdb.config.collection_name\n    if collection_name is None:\n        return\n    try:\n        # Note we may have used a vecdb with a config.collection_name\n        # different from the agent's config.vecdb.collection_name!!\n        self.vecdb.delete_collection(collection_name)\n        self.vecdb = VectorStore.create(self.vecdb.config)\n    except Exception as e:\n        logger.warning(\n            f\"\"\"\n            Error while deleting collection {collection_name}:\n            {e}\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> <p>Returns:</p> Type Description <code>None</code> <p>dict with keys: n_splits: number of splits urls: list of urls paths: list of file paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n    \"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n\n    Returns:\n        dict with keys:\n            n_splits: number of splits\n            urls: list of urls\n            paths: list of file paths\n    \"\"\"\n    if len(self.config.doc_paths) == 0:\n        # we must be using a previously defined collection\n        # But let's get all the chunked docs so we can\n        # do keyword and other non-vector searches\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.setup_documents(filter=self.config.filter)\n        return\n    self.ingest_doc_paths(self.config.doc_paths)  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_doc_paths","title":"<code>ingest_doc_paths(paths, metadata=[], doc_type=None)</code>","text":"<p>Split, ingest docs from specified paths, do not add these to config.doc_paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>str | bytes | List[str | bytes]</code> <p>document paths, urls or byte-content of docs. The bytes option is intended to support cases where a document has already been read in as bytes (e.g. from an API or a database), and we want to avoid having to write it to a temporary file just to read it back in.</p> required <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each path. If a single dict is passed in, it is used for all paths.</p> <code>[]</code> <code>doc_type</code> <code>str | DocumentType | None</code> <p>DocumentType to use for parsing, if known. MUST apply to all docs if specified. This is especially useful when the <code>paths</code> are of bytes type, to help with document type detection.</p> <code>None</code> <p>Returns:     List of Document objects</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_doc_paths(\n    self,\n    paths: str | bytes | List[str | bytes],\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n    doc_type: str | DocumentType | None = None,\n) -&gt; List[Document]:\n    \"\"\"Split, ingest docs from specified paths,\n    do not add these to config.doc_paths.\n\n    Args:\n        paths: document paths, urls or byte-content of docs.\n            The bytes option is intended to support cases where a document\n            has already been read in as bytes (e.g. from an API or a database),\n            and we want to avoid having to write it to a temporary file\n            just to read it back in.\n        metadata: List of metadata dicts, one for each path.\n            If a single dict is passed in, it is used for all paths.\n        doc_type: DocumentType to use for parsing, if known.\n            MUST apply to all docs if specified.\n            This is especially useful when the `paths` are of bytes type,\n            to help with document type detection.\n    Returns:\n        List of Document objects\n    \"\"\"\n    if isinstance(paths, str) or isinstance(paths, bytes):\n        paths = [paths]\n    all_paths = paths\n    paths_meta: Dict[int, Any] = {}\n    urls_meta: Dict[int, Any] = {}\n    idxs = range(len(all_paths))\n    url_idxs, path_idxs, bytes_idxs = get_urls_paths_bytes_indices(all_paths)\n    urls = [all_paths[i] for i in url_idxs]\n    paths = [all_paths[i] for i in path_idxs]\n    bytes_list = [all_paths[i] for i in bytes_idxs]\n    path_idxs.extend(bytes_idxs)\n    paths.extend(bytes_list)\n    if (isinstance(metadata, list) and len(metadata) &gt; 0) or not isinstance(\n        metadata, list\n    ):\n        if isinstance(metadata, list):\n            idx2meta = {\n                p: (\n                    m\n                    if isinstance(m, dict)\n                    else (isinstance(m, DocMetaData) and m.dict())\n                )  # appease mypy\n                for p, m in zip(idxs, metadata)\n            }\n        elif isinstance(metadata, dict):\n            idx2meta = {p: metadata for p in idxs}\n        else:\n            idx2meta = {p: metadata.dict() for p in idxs}\n        urls_meta = {u: idx2meta[u] for u in url_idxs}\n        paths_meta = {p: idx2meta[p] for p in path_idxs}\n    docs: List[Document] = []\n    parser = Parser(self.config.parsing)\n    if len(urls) &gt; 0:\n        for ui in url_idxs:\n            meta = urls_meta.get(ui, {})\n            loader = URLLoader(urls=[all_paths[ui]], parser=parser)  # type: ignore\n            url_docs = loader.load()\n            # update metadata of each doc with meta\n            for d in url_docs:\n                d.metadata = d.metadata.copy(update=meta)\n            docs.extend(url_docs)\n    if len(paths) &gt; 0:  # paths OR bytes are handled similarly\n        for pi in path_idxs:\n            meta = paths_meta.get(pi, {})\n            p = all_paths[pi]\n            path_docs = RepoLoader.get_documents(\n                p,\n                parser=parser,\n                doc_type=doc_type,\n            )\n            # update metadata of each doc with meta\n            for d in path_docs:\n                d.metadata = d.metadata.copy(update=meta)\n            docs.extend(path_docs)\n    n_docs = len(docs)\n    n_splits = self.ingest_docs(docs, split=self.config.split)\n    if n_docs == 0:\n        return []\n    n_urls = len(urls)\n    n_paths = len(paths)\n    print(\n        f\"\"\"\n    [green]I have processed the following {n_urls} URLs\n    and {n_paths} docs into {n_splits} parts:\n    \"\"\".strip()\n    )\n    path_reps = [p if isinstance(p, str) else \"bytes\" for p in paths]\n    print(\"\\n\".join([u for u in urls if isinstance(u, str)]))  # appease mypy\n    print(\"\\n\".join(path_reps))\n    return docs\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs, split=True, metadata=[])</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects</p> required <code>split</code> <code>bool</code> <p>Whether to split docs into chunks. Default is True. If False, docs are treated as \"chunks\" and are not split.</p> <code>True</code> <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each doc, to augment whatever metadata is already in the doc. [ASSUME no conflicting keys between the two metadata dicts.] If a single dict is passed in, it is used for all docs.</p> <code>[]</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(\n    self,\n    docs: List[Document],\n    split: bool = True,\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n) -&gt; int:\n    \"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n\n    Args:\n        docs: List of Document objects\n        split: Whether to split docs into chunks. Default is True.\n            If False, docs are treated as \"chunks\" and are not split.\n        metadata: List of metadata dicts, one for each doc, to augment\n            whatever metadata is already in the doc.\n            [ASSUME no conflicting keys between the two metadata dicts.]\n            If a single dict is passed in, it is used for all docs.\n    \"\"\"\n    if isinstance(metadata, list) and len(metadata) &gt; 0:\n        for d, m in zip(docs, metadata):\n            d.metadata = d.metadata.copy(\n                update=m if isinstance(m, dict) else m.dict()  # type: ignore\n            )\n    elif isinstance(metadata, dict):\n        for d in docs:\n            d.metadata = d.metadata.copy(update=metadata)\n    elif isinstance(metadata, DocMetaData):\n        for d in docs:\n            d.metadata = d.metadata.copy(update=metadata.dict())\n\n    self.original_docs.extend(docs)\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    for d in docs:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n    if split:\n        docs = self.parser.split(docs)\n    else:\n        self.parser.add_window_ids(docs)\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs for all docs, before batching.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    if len(self.config.add_fields_to_content) &gt; 0:\n        fields = [\n            f for f in extract_fields(docs[0], self.config.add_fields_to_content)\n        ]\n        if len(fields) &gt; 0:\n            for d in docs:\n                key_vals = extract_fields(d, fields)\n                d.content = (\n                    \",\".join(f\"{k}={v}\" for k, v in key_vals.items())\n                    + \",content=\"\n                    + d.content\n                )\n    docs = docs[: self.config.parsing.max_chunks]\n    # add embeddings in batches, to stay under limit of embeddings API\n    batches = list(batched(docs, self.config.embed_batch_size))\n    for batch in batches:\n        self.vecdb.add_documents(batch)\n    self.original_docs_length = self.doc_length(docs)\n    self.setup_documents(docs, filter=self.config.filter)\n    return len(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.retrieval_tool","title":"<code>retrieval_tool(msg)</code>","text":"<p>Handle the RetrievalTool message</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def retrieval_tool(self, msg: RetrievalTool) -&gt; str:\n    \"\"\"Handle the RetrievalTool message\"\"\"\n    self.config.retrieve_only = True\n    self.config.parsing.n_similar_docs = msg.num_results\n    content_doc = self.answer_from_docs(msg.query)\n    return content_doc.content\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.document_compatible_dataframe","title":"<code>document_compatible_dataframe(df, content='content', metadata=[])</code>  <code>staticmethod</code>","text":"<p>Convert dataframe so it is compatible with Document class: - has \"content\" column - has an \"id\" column to be used as Document.metadata.id</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to convert</p> required <code>content</code> <code>str</code> <p>name of content column</p> <code>'content'</code> <code>metadata</code> <code>List[str]</code> <p>list of metadata column names</p> <code>[]</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, List[str]]</code> <p>Tuple[pd.DataFrame, List[str]]: dataframe, metadata - dataframe: dataframe with \"content\" column and \"id\" column - metadata: list of metadata column names, including \"id\"</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef document_compatible_dataframe(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    Convert dataframe so it is compatible with Document class:\n    - has \"content\" column\n    - has an \"id\" column to be used as Document.metadata.id\n\n    Args:\n        df: dataframe to convert\n        content: name of content column\n        metadata: list of metadata column names\n\n    Returns:\n        Tuple[pd.DataFrame, List[str]]: dataframe, metadata\n            - dataframe: dataframe with \"content\" column and \"id\" column\n            - metadata: list of metadata column names, including \"id\"\n    \"\"\"\n    if content not in df.columns:\n        raise ValueError(\n            f\"\"\"\n            Content column {content} not in dataframe,\n            so we cannot ingest into the DocChatAgent.\n            Please specify the `content` parameter as a suitable\n            text-based column in the dataframe.\n            \"\"\"\n        )\n    if content != \"content\":\n        # rename content column to \"content\", leave existing column intact\n        df = df.rename(columns={content: \"content\"}, inplace=False)\n\n    actual_metadata = metadata.copy()\n    if \"id\" not in df.columns:\n        docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n        ids = [str(d.id()) for d in docs]\n        df[\"id\"] = ids\n\n    if \"id\" not in actual_metadata:\n        actual_metadata += [\"id\"]\n\n    return df, actual_metadata\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest a dataframe into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"\n    Ingest a dataframe into vecdb.\n    \"\"\"\n    self.from_dataframe = True\n    self.df_description = describe_dataframe(\n        df, filter_fields=self.config.filter_fields, n_vals=5\n    )\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    # When ingesting a dataframe we will no longer do any chunking,\n    # so we mark each doc as a chunk.\n    # TODO - revisit this since we may still want to chunk large text columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return self.ingest_docs(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.setup_documents","title":"<code>setup_documents(docs=[], filter=None)</code>","text":"<p>Setup <code>self.chunked_docs</code> and <code>self.chunked_docs_clean</code> based on possible filter. These will be used in various non-vector-based search functions, e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects. This is empty when we are calling this method after initial doc ingestion.</p> <code>[]</code> <code>filter</code> <code>str | None</code> <p>Filter condition for various lexical/semantic search fns.</p> <code>None</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def setup_documents(\n    self,\n    docs: List[Document] = [],\n    filter: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Setup `self.chunked_docs` and `self.chunked_docs_clean`\n    based on possible filter.\n    These will be used in various non-vector-based search functions,\n    e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.\n\n    Args:\n        docs: List of Document objects. This is empty when we are calling this\n            method after initial doc ingestion.\n        filter: Filter condition for various lexical/semantic search fns.\n    \"\"\"\n    if filter is None and len(docs) &gt; 0:\n        # no filter, so just use the docs passed in\n        self.chunked_docs.extend(docs)\n    else:\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.chunked_docs = self.vecdb.get_all_documents(where=filter or \"\")\n\n    self.chunked_docs_clean = [\n        Document(content=preprocess_text(d.content), metadata=d.metadata)\n        for d in self.chunked_docs\n    ]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_field_values","title":"<code>get_field_values(fields)</code>","text":"<p>Get string-listing of possible values of each filterable field, e.g. {     \"genre\": \"crime, drama, mystery, ... (10 more)\",     \"certificate\": \"R, PG-13, PG, R\", }</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_field_values(self, fields: list[str]) -&gt; Dict[str, str]:\n    \"\"\"Get string-listing of possible values of each filterable field,\n    e.g.\n    {\n        \"genre\": \"crime, drama, mystery, ... (10 more)\",\n        \"certificate\": \"R, PG-13, PG, R\",\n    }\n    \"\"\"\n    field_values: Dict[str, Set[str]] = {}\n    # make empty set for each field\n    for f in fields:\n        field_values[f] = set()\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # get all documents and accumulate possible values of each field until 10\n    docs = self.vecdb.get_all_documents()  # only works for vecdbs that support this\n    for d in docs:\n        # extract fields from d\n        doc_field_vals = extract_fields(d, fields)\n        for field, val in doc_field_vals.items():\n            field_values[field].add(val)\n    # For each field make a string showing list of possible values,\n    # truncate to 20 values, and if there are more, indicate how many\n    # more there are, e.g. Genre: crime, drama, mystery, ... (20 more)\n    field_values_list = {}\n    for f in fields:\n        vals = list(field_values[f])\n        n = len(vals)\n        remaining = n - 20\n        vals = vals[:20]\n        if n &gt; 20:\n            vals.append(f\"(...{remaining} more)\")\n        # make a string of the values, ensure they are strings\n        field_values_list[f] = \", \".join(str(v) for v in vals)\n    return field_values_list\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs Args:     docs: list of Document objects Returns:     int: number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n    \"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    return self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.user_docs_ingest_dialog","title":"<code>user_docs_ingest_dialog()</code>","text":"<p>Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def user_docs_ingest_dialog(self) -&gt; None:\n    \"\"\"\n    Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    n_deletes = self.vecdb.clear_empty_collections()\n    collections = self.vecdb.list_collections()\n    collection_name = \"NEW\"\n    is_new_collection = False\n    replace_collection = False\n    if len(collections) &gt; 0:\n        n = len(collections)\n        delete_str = (\n            f\"(deleted {n_deletes} empty collections)\" if n_deletes &gt; 0 else \"\"\n        )\n        print(f\"Found {n} collections: {delete_str}\")\n        for i, option in enumerate(collections, start=1):\n            print(f\"{i}. {option}\")\n        while True:\n            choice = Prompt.ask(\n                f\"Enter 1-{n} to select a collection, \"\n                \"or hit ENTER to create a NEW collection, \"\n                \"or -1 to DELETE ALL COLLECTIONS\",\n                default=\"0\",\n            )\n            try:\n                if -1 &lt;= int(choice) &lt;= n:\n                    break\n            except Exception:\n                pass\n\n        if choice == \"-1\":\n            confirm = Prompt.ask(\n                \"Are you sure you want to delete all collections?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            if confirm == \"y\":\n                self.vecdb.clear_all_collections(really=True)\n                collection_name = \"NEW\"\n\n        if int(choice) &gt; 0:\n            collection_name = collections[int(choice) - 1]\n            print(f\"Using collection {collection_name}\")\n            choice = Prompt.ask(\n                \"Would you like to replace this collection?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            replace_collection = choice == \"y\"\n\n    if collection_name == \"NEW\":\n        is_new_collection = True\n        collection_name = Prompt.ask(\n            \"What would you like to name the NEW collection?\",\n            default=\"doc-chat\",\n        )\n\n    self.vecdb.set_collection(collection_name, replace=replace_collection)\n\n    default_urls_str = (\n        \" (or leave empty for default URLs)\" if is_new_collection else \"\"\n    )\n    print(f\"[blue]Enter some URLs or file/dir paths below {default_urls_str}\")\n    inputs = get_list_from_user()\n    if len(inputs) == 0:\n        if is_new_collection:\n            inputs = self.config.default_paths\n    self.config.doc_paths = inputs  # type: ignore\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs. Args:     docs: list of Document objects Returns:     str: string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n    \"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\n    contents = [f\"Extract: {d.content}\" for d in docs]\n    sources = [d.metadata.source for d in docs]\n    sources = [f\"Source: {s}\" if s is not None else \"\" for s in sources]\n    return \"\\n\".join(\n        [\n            f\"\"\"\n            [{i+1}]\n            {content}\n            {source}\n            \"\"\"\n            for i, (content, source) in enumerate(zip(contents, sources))\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible Args:     question: question to answer     passages: list of <code>Document</code> objects each containing a possibly relevant         snippet, and metadata Returns:     a <code>Document</code> object containing the answer,     and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(\n    self, question: str, passages: List[Document]\n) -&gt; ChatDocument:\n    \"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n\n    \"\"\"\n\n    passages_str = self.doc_string(passages)\n    # Substitute Q and P into the templatized prompt\n\n    final_prompt = self.config.summarize_prompt.format(\n        question=question, extracts=passages_str\n    )\n    show_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n\n    # Generate the final verbatim extract based on the final prompt.\n    # Note this will send entire message history, plus this final_prompt\n    # to the LLM, and self.message_history will be updated to include\n    # 2 new LLMMessage objects:\n    # one for `final_prompt`, and one for the LLM response\n\n    if self.config.conversation_mode:\n        # respond with temporary context\n        answer_doc = super()._llm_response_temp_context(question, final_prompt)\n    else:\n        answer_doc = super().llm_response_forget(final_prompt)\n\n    final_answer = answer_doc.content.strip()\n    show_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\n\n    citations = extract_markdown_references(final_answer)\n\n    citations_str = \"\"\n    if len(citations) &gt; 0:\n        # append [i] source, content for each citation\n        citations_str = \"\\n\".join(\n            [\n                f\"[^{c}] {passages[c-1].metadata.source}\"\n                f\"\\n{format_footnote_text(passages[c-1].content)}\"\n                for c in citations\n            ]\n        )\n\n    return ChatDocument(\n        content=final_answer,  # does not contain citations\n        metadata=ChatDocMetaData(\n            source=citations_str,  # only the citations\n            sender=Entity.LLM,\n            has_citation=len(citations) &gt; 0,\n            cached=getattr(answer_doc.metadata, \"cached\", False),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.rerank_with_diversity","title":"<code>rerank_with_diversity(passages)</code>","text":"<p>Rerank a list of items in such a way that each successive item is least similar (on average) to the earlier items.</p> <p>Args: query (str): The query for which the passages are relevant. passages (List[Document]): A list of Documents to be reranked.</p> <p>Returns: List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_with_diversity(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank a list of items in such a way that each successive item is least similar\n    (on average) to the earlier items.\n\n    Args:\n    query (str): The query for which the passages are relevant.\n    passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n    List[Documents]: A reranked list of Documents.\n    \"\"\"\n\n    if self.vecdb is None:\n        logger.warning(\"No vecdb; cannot use rerank_with_diversity\")\n        return passages\n    emb_model = self.vecdb.embedding_model\n    emb_fn = emb_model.embedding_fn()\n    embs = emb_fn([p.content for p in passages])\n    embs_arr = [np.array(e) for e in embs]\n    indices = list(range(len(passages)))\n\n    # Helper function to compute average similarity to\n    # items in the current result list.\n    def avg_similarity_to_result(i: int, result: List[int]) -&gt; float:\n        return sum(  # type: ignore\n            (embs_arr[i] @ embs_arr[j])\n            / (np.linalg.norm(embs_arr[i]) * np.linalg.norm(embs_arr[j]))\n            for j in result\n        ) / len(result)\n\n    # copy passages to items\n    result = [indices.pop(0)]  # Start with the first item.\n\n    while indices:\n        # Find the item that has the least average similarity\n        # to items in the result list.\n        least_similar_item = min(\n            indices, key=lambda i: avg_similarity_to_result(i, result)\n        )\n        result.append(least_similar_item)\n        indices.remove(least_similar_item)\n\n    # return passages in order of result list\n    return [passages[i] for i in result]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.rerank_to_periphery","title":"<code>rerank_to_periphery(passages)</code>","text":"<p>Rerank to avoid Lost In the Middle (LIM) problem, where LLMs pay more attention to items at the ends of a list, rather than the middle. So we re-rank to make the best passages appear at the periphery of the list. https://arxiv.org/abs/2307.03172</p> <p>Example reranking: 1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>A list of Documents to be reranked.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_to_periphery(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank to avoid Lost In the Middle (LIM) problem,\n    where LLMs pay more attention to items at the ends of a list,\n    rather than the middle. So we re-rank to make the best passages\n    appear at the periphery of the list.\n    https://arxiv.org/abs/2307.03172\n\n    Example reranking:\n    1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2\n\n    Args:\n        passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n        List[Documents]: A reranked list of Documents.\n\n    \"\"\"\n    # Splitting items into odds and evens based on index, not value\n    odds = passages[::2]\n    evens = passages[1::2][::-1]\n\n    # Merging them back together\n    return odds + evens\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.add_context_window","title":"<code>add_context_window(docs_scores)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. We use these stored window_ids to retrieve the desired number (self.config.n_neighbor_chunks) of neighbors on either side of the current chunk.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def add_context_window(\n    self,\n    docs_scores: List[Tuple[Document, float]],\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk. We use these stored\n    window_ids to retrieve the desired number\n    (self.config.n_neighbor_chunks) of neighbors\n    on either side of the current chunk.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None or self.config.n_neighbor_chunks == 0:\n        return docs_scores\n    if len(docs_scores) == 0:\n        return []\n    if set(docs_scores[0][0].__fields__) != {\"content\", \"metadata\"}:\n        # Do not add context window when there are other fields besides just\n        # content and metadata, since we do not know how to set those other fields\n        # for newly created docs with combined content.\n        return docs_scores\n    return self.vecdb.add_context_window(docs_scores, self.config.n_neighbor_chunks)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_semantic_search_results","title":"<code>get_semantic_search_results(query, k=10)</code>","text":"<p>Get semantic search results from vecdb. Args:     query (str): query to search for     k (int): number of results to return Returns:     List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_semantic_search_results(\n    self,\n    query: str,\n    k: int = 10,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Get semantic search results from vecdb.\n    Args:\n        query (str): query to search for\n        k (int): number of results to return\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # Note: for dynamic filtering based on a query, users can\n    # use the `temp_update` context-manager to pass in a `filter` to self.config,\n    # e.g.:\n    # with temp_update(self.config, {\"filter\": \"metadata.source=='source1'\"}):\n    #     docs_scores = self.get_semantic_search_results(query, k=k)\n    # This avoids having pass the `filter` argument to every function call\n    # upstream of this one.\n    # The `temp_update` context manager is defined in\n    # `langroid/utils/pydantic_utils.py`\n    return self.vecdb.similar_texts_with_scores(\n        query,\n        k=k,\n        where=self.config.filter,\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_relevant_chunks","title":"<code>get_relevant_chunks(query, query_proxies=[])</code>","text":"<p>The retrieval stage in RAG: get doc-chunks that are most \"relevant\" to the query (and possibly any proxy queries), from the document-store, which currently is the vector store, but in theory could be any document store, or even web-search. This stage does NOT involve an LLM, and the retrieved chunks could either be pre-chunked text (from the initial pre-processing stage where chunks were stored in the vector store), or they could be dynamically retrieved based on a window around a lexical match.</p> <p>These are the steps (some optional based on config): - semantic search based on vector-embedding distance, from vecdb - lexical search using bm25-ranking (keyword similarity) - fuzzy matching (keyword similarity) - re-ranking of doc-chunks by relevance to query, using cross-encoder,    and pick top k</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>original query (assumed to be in stand-alone form)</p> required <code>query_proxies</code> <code>List[str]</code> <p>possible rephrases, or hypothetical answer to query     (e.g. for HyDE-type retrieval)</p> <code>[]</code> <p>Returns:</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_relevant_chunks(\n    self, query: str, query_proxies: List[str] = []\n) -&gt; List[Document]:\n    \"\"\"\n    The retrieval stage in RAG: get doc-chunks that are most \"relevant\"\n    to the query (and possibly any proxy queries), from the document-store,\n    which currently is the vector store,\n    but in theory could be any document store, or even web-search.\n    This stage does NOT involve an LLM, and the retrieved chunks\n    could either be pre-chunked text (from the initial pre-processing stage\n    where chunks were stored in the vector store), or they could be\n    dynamically retrieved based on a window around a lexical match.\n\n    These are the steps (some optional based on config):\n    - semantic search based on vector-embedding distance, from vecdb\n    - lexical search using bm25-ranking (keyword similarity)\n    - fuzzy matching (keyword similarity)\n    - re-ranking of doc-chunks by relevance to query, using cross-encoder,\n       and pick top k\n\n    Args:\n        query: original query (assumed to be in stand-alone form)\n        query_proxies: possible rephrases, or hypothetical answer to query\n                (e.g. for HyDE-type retrieval)\n\n    Returns:\n\n    \"\"\"\n    # if we are using cross-encoder reranking, we can retrieve more docs\n    # during retrieval, and leave it to the cross-encoder re-ranking\n    # to whittle down to self.config.parsing.n_similar_docs\n    retrieval_multiple = 1 if self.config.cross_encoder_reranking_model == \"\" else 3\n\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    with status(\"[cyan]Searching VecDB for relevant doc passages...\"):\n        docs_and_scores: List[Tuple[Document, float]] = []\n        for q in [query] + query_proxies:\n            docs_and_scores += self.get_semantic_search_results(\n                q,\n                k=self.config.parsing.n_similar_docs * retrieval_multiple,\n            )\n    # keep only docs with unique d.id()\n    id2doc_score = {d.id(): (d, s) for d, s in docs_and_scores}\n    docs_and_scores = list(id2doc_score.values())\n    passages = [d for (d, _) in docs_and_scores]\n    # passages = [\n    #     Document(content=d.content, metadata=d.metadata)\n    #     for (d, _) in docs_and_scores\n    # ]\n\n    if self.config.use_bm25_search:\n        docs_scores = self.get_similar_chunks_bm25(query, retrieval_multiple)\n        passages += [d for (d, _) in docs_scores]\n\n    if self.config.use_fuzzy_match:\n        fuzzy_match_docs = self.get_fuzzy_matches(query, retrieval_multiple)\n        passages += fuzzy_match_docs\n\n    # keep unique passages\n    id2passage = {p.id(): p for p in passages}\n    passages = list(id2passage.values())\n\n    if len(passages) == 0:\n        return []\n\n    if self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n    # now passages can potentially have a lot of doc chunks,\n    # so we re-rank them using a cross-encoder scoring model,\n    # and pick top k where k = config.parsing.n_similar_docs\n    # https://www.sbert.net/examples/applications/retrieve_rerank\n    if self.config.cross_encoder_reranking_model != \"\":\n        passages = self.rerank_with_cross_encoder(query, passages)\n\n    if self.config.rerank_diversity:\n        # reorder to increase diversity among top docs\n        passages = self.rerank_with_diversity(passages)\n\n    if self.config.rerank_periphery:\n        # reorder so most important docs are at periphery\n        # (see Lost In the Middle issue).\n        passages = self.rerank_to_periphery(passages)\n\n    if not self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n\n    return passages\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_relevant_extracts","title":"<code>get_relevant_extracts(query)</code>","text":"<p>Get list of (verbatim) extracts from doc-chunks relevant to answering a query.</p> <p>These are the stages (some optional based on config): - use LLM to convert query to stand-alone query - optionally use LLM to rephrase query to use below - optionally use LLM to generate hypothetical answer (HyDE) to use below. - get_relevant_chunks(): get doc-chunks relevant to query and proxies - use LLM to get relevant extracts from doc-chunks</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to search for</p> required <p>Returns:</p> Name Type Description <code>query</code> <code>str</code> <p>stand-alone version of input query</p> <code>List[Document]</code> <p>List[Document]: list of relevant extracts</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef get_relevant_extracts(self, query: str) -&gt; Tuple[str, List[Document]]:\n    \"\"\"\n    Get list of (verbatim) extracts from doc-chunks relevant to answering a query.\n\n    These are the stages (some optional based on config):\n    - use LLM to convert query to stand-alone query\n    - optionally use LLM to rephrase query to use below\n    - optionally use LLM to generate hypothetical answer (HyDE) to use below.\n    - get_relevant_chunks(): get doc-chunks relevant to query and proxies\n    - use LLM to get relevant extracts from doc-chunks\n\n    Args:\n        query (str): query to search for\n\n    Returns:\n        query (str): stand-alone version of input query\n        List[Document]: list of relevant extracts\n\n    \"\"\"\n    if len(self.dialog) &gt; 0 and not self.config.assistant_mode:\n        # Regardless of whether we are in conversation mode or not,\n        # for relevant doc/chunk extraction, we must convert the query\n        # to a standalone query to get more relevant results.\n        with status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\n            with StreamingIfAllowed(self.llm, False):\n                query = self.llm.followup_to_standalone(self.dialog, query)\n        print(f\"[orange2]New query: {query}\")\n\n    proxies = []\n    if self.config.hypothetical_answer:\n        answer = self.llm_hypothetical_answer(query)\n        proxies = [answer]\n\n    if self.config.n_query_rephrases &gt; 0:\n        rephrases = self.llm_rephrase_query(query)\n        proxies += rephrases\n\n    passages = self.get_relevant_chunks(query, proxies)  # no LLM involved\n\n    if len(passages) == 0:\n        return query, []\n\n    with status(\"[cyan]LLM Extracting verbatim passages...\"):\n        with StreamingIfAllowed(self.llm, False):\n            # these are async calls, one per passage; turn off streaming\n            extracts = self.get_verbatim_extracts(query, passages)\n            extracts = [e for e in extracts if e.content != NO_ANSWER]\n\n    return query, extracts\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_verbatim_extracts","title":"<code>get_verbatim_extracts(query, passages)</code>","text":"<p>Run RelevanceExtractorAgent in async/concurrent mode on passages, to extract portions relevant to answering query, from each passage. Args:     query (str): query to answer     passages (List[Documents]): list of passages to extract from</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Documents containing extracts and metadata.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_verbatim_extracts(\n    self,\n    query: str,\n    passages: List[Document],\n) -&gt; List[Document]:\n    \"\"\"\n    Run RelevanceExtractorAgent in async/concurrent mode on passages,\n    to extract portions relevant to answering query, from each passage.\n    Args:\n        query (str): query to answer\n        passages (List[Documents]): list of passages to extract from\n\n    Returns:\n        List[Document]: list of Documents containing extracts and metadata.\n    \"\"\"\n    agent_cfg = self.config.relevance_extractor_config\n    if agent_cfg is None:\n        # no relevance extraction: simply return passages\n        return passages\n    if agent_cfg.llm is None:\n        # Use main DocChatAgent's LLM if not provided explicitly:\n        # this reduces setup burden on the user\n        agent_cfg.llm = self.config.llm\n    agent_cfg.query = query\n    agent_cfg.segment_length = self.config.extraction_granularity\n    agent_cfg.llm.stream = False  # disable streaming for concurrent calls\n\n    agent = RelevanceExtractorAgent(agent_cfg)\n    task = Task(\n        agent,\n        name=\"Relevance-Extractor\",\n        interactive=False,\n    )\n\n    extracts = run_batch_tasks(\n        task,\n        passages,\n        input_map=lambda msg: msg.content,\n        output_map=lambda ans: ans.content if ans is not None else NO_ANSWER,\n    )\n\n    # Caution: Retain ALL other fields in the Documents (which could be\n    # other than just `content` and `metadata`), while simply replacing\n    # `content` with the extracted portions\n    passage_extracts = []\n    for p, e in zip(passages, extracts):\n        if e == NO_ANSWER or len(e) == 0:\n            continue\n        p_copy = p.copy()\n        p_copy.content = e\n        passage_extracts.append(p_copy)\n\n    return passage_extracts\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on relevant docs from the VecDB</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to answer</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>ChatDocument</code> <p>answer</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def answer_from_docs(self, query: str) -&gt; ChatDocument:\n    \"\"\"\n    Answer query based on relevant docs from the VecDB\n\n    Args:\n        query (str): query to answer\n\n    Returns:\n        Document: answer\n    \"\"\"\n    response = ChatDocument(\n        content=NO_ANSWER,\n        metadata=ChatDocMetaData(\n            source=\"None\",\n            sender=Entity.LLM,\n        ),\n    )\n    # query may be updated to a stand-alone version\n    query, extracts = self.get_relevant_extracts(query)\n    if len(extracts) == 0:\n        return response\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if self.config.retrieve_only:\n        # only return extracts, skip LLM-based summary answer\n        meta = dict(\n            sender=Entity.LLM,\n        )\n        # copy metadata from first doc, unclear what to do here.\n        meta.update(extracts[0].metadata)\n        return ChatDocument(\n            content=\"\\n\\n\".join([e.content for e in extracts]),\n            metadata=ChatDocMetaData(**meta),  # type: ignore\n        )\n    response = self.get_summary_answer(query, extracts)\n\n    self.update_dialog(query, response.content)\n    self.response = response  # save last response\n    return response\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\n    self,\n    instruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n    \"\"\"Summarize all docs\"\"\"\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if len(self.original_docs) == 0:\n        logger.warning(\n            \"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection?\n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs.\n            \"\"\"\n        )\n        return None\n    full_text = \"\\n\\n\".join([d.content for d in self.original_docs])\n    if self.parser is None:\n        raise ValueError(\"No parser defined\")\n    tot_tokens = self.parser.num_tokens(full_text)\n    MAX_INPUT_TOKENS = (\n        self.llm.completion_context_length()\n        - self.config.llm.max_output_tokens\n        - 100\n    )\n    if tot_tokens &gt; MAX_INPUT_TOKENS:\n        # truncate\n        full_text = self.parser.tokenizer.decode(\n            self.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n        )\n        logger.warning(\n            f\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n        )\n    prompt = f\"\"\"\n    {instruction}\n\n    FULL TEXT:\n    {full_text}\n    \"\"\".strip()\n    with StreamingIfAllowed(self.llm):\n        summary = ChatAgent.llm_response(self, prompt)\n        return summary\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; ChatDocument | None:\n    \"\"\"Show evidence for last response\"\"\"\n    if self.response is None:\n        print(\"[magenta]No response yet\")\n        return None\n    source = self.response.metadata.source\n    if len(source) &gt; 0:\n        print(\"[magenta]\" + source)\n    else:\n        print(\"[magenta]No source found\")\n    return None\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/","title":"lance_doc_chat_agent","text":"<p>langroid/agent/special/lance_doc_chat_agent.py </p> <p>LanceDocChatAgent is a subclass of DocChatAgent that uses LanceDB as a vector store: - Uses the DocChatAgentConfig.filter variable     (a sql string) in the <code>where</code> clause to do filtered vector search. - Overrides the get_similar_chunks_bm25() to use LanceDB FTS (Full Text Search).</p> For usage see <ul> <li><code>tests/main/test_lance_doc_chat_agent.py</code>.</li> <li>example script <code>examples/docqa/lance_rag.py</code>.</li> </ul>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent","title":"<code>LanceDocChatAgent(cfg)</code>","text":"<p>             Bases: <code>DocChatAgent</code></p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def __init__(self, cfg: DocChatAgentConfig):\n    super().__init__(cfg)\n    self.config: DocChatAgentConfig = cfg\n    self.enable_message(QueryPlanTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent.query_plan","title":"<code>query_plan(msg)</code>","text":"<p>Handle the LLM's use of the FilterTool. Temporarily set the config filter and either return the final answer in case there's a dataframe_calc, or return the rephrased query so the LLM can handle it.</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def query_plan(self, msg: QueryPlanTool) -&gt; str:\n    \"\"\"\n    Handle the LLM's use of the FilterTool.\n    Temporarily set the config filter and either return the final answer\n    in case there's a dataframe_calc, or return the rephrased query\n    so the LLM can handle it.\n    \"\"\"\n    # create document-subset based on this filter\n    plan = msg.plan\n    try:\n        self.setup_documents(filter=plan.filter or None)\n    except Exception as e:\n        logger.error(f\"Error setting up documents: {e}\")\n        # say DONE with err msg so it goes back to LanceFilterAgent\n        return f\"\"\"\n        {DONE} Possible Filter Error:\\n {e}\n\n        Note that only the following fields are allowed in the filter\n        of a query plan: \n        {\", \".join(self.config.filter_fields)}\n        \"\"\"\n\n    # update the filter so it is used in the DocChatAgent\n    self.config.filter = plan.filter or None\n    if plan.dataframe_calc:\n        # we just get relevant docs then do the calculation\n        # TODO if calc causes err, it is captured in result,\n        # and LLM can correct the calc based on the err,\n        # and this will cause retrieval all over again,\n        # which may be wasteful if only the calc part is wrong.\n        # The calc step can later be done with a separate Agent/Tool.\n        if plan.query is None or plan.query.strip() == \"\":\n            if plan.filter is None or plan.filter.strip() == \"\":\n                return \"\"\"DONE\n                Cannot execute Query Plan since filter as well as \n                rephrased query are empty.\n                \"\"\"\n            else:\n                # no query to match, so just get all docs matching filter\n                docs = self.vecdb.get_all_documents(plan.filter)\n        else:\n            _, docs = self.get_relevant_extracts(plan.query)\n        if len(docs) == 0:\n            return DONE + \" \" + NO_ANSWER\n        result = self.vecdb.compute_from_docs(docs, plan.dataframe_calc)\n        return DONE + \" \" + result\n    else:\n        # pass on the query so LLM can handle it\n        return plan.query\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest from a dataframe. Assume we are doing this once, not incrementally</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"Ingest from a dataframe. Assume we are doing this once, not incrementally\"\"\"\n\n    self.from_dataframe = True\n    if df.shape[0] == 0:\n        raise ValueError(\n            \"\"\"\n            LanceDocChatAgent.ingest_dataframe() received an empty dataframe.\n            \"\"\"\n        )\n    n = df.shape[0]\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs, into the `content` field for all rows.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    fields = [f for f in self.config.add_fields_to_content if f in df.columns]\n    if len(fields) &gt; 0:\n        df[content] = df.apply(\n            lambda row: (\",\".join(f\"{f}={row[f]}\" for f in fields))\n            + \", content=\"\n            + row[content],\n            axis=1,\n        )\n\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    self.df_description = describe_dataframe(\n        df,\n        filter_fields=self.config.filter_fields,\n        n_vals=10,\n    )\n    self.vecdb.add_dataframe(df, content=\"content\", metadata=metadata)\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    # We assume \"content\" is available as top-level field\n    if \"content\" in tbl.schema.names:\n        tbl.create_fts_index(\"content\", replace=True)\n    # We still need to do the below so that\n    # other types of searches in DocChatAgent\n    # can work, as they require Document objects\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    self.setup_documents(docs)\n    # mark each doc as already-chunked so we don't try to split them further\n    # TODO later we may want to split large text-columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return n  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent.get_similar_chunks_bm25","title":"<code>get_similar_chunks_bm25(query, multiple)</code>","text":"<p>Override the DocChatAgent.get_similar_chunks_bm25() to use LanceDB FTS (Full Text Search).</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def get_similar_chunks_bm25(\n    self, query: str, multiple: int\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Override the DocChatAgent.get_similar_chunks_bm25()\n    to use LanceDB FTS (Full Text Search).\n    \"\"\"\n    # Clean up query: replace all newlines with spaces in query,\n    # force special search keywords to lower case, remove quotes,\n    # so it's not interpreted as search syntax\n    query_clean = (\n        query.replace(\"\\n\", \" \")\n        .replace(\"AND\", \"and\")\n        .replace(\"OR\", \"or\")\n        .replace(\"NOT\", \"not\")\n        .replace(\"'\", \"\")\n        .replace('\"', \"\")\n    )\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    result = (\n        tbl.search(query_clean)\n        .where(self.config.filter or None)\n        .limit(self.config.parsing.n_similar_docs * multiple)\n    )\n    docs = self.vecdb._lance_result_to_docs(result)\n    scores = [r[\"score\"] for r in result.to_list()]\n    return list(zip(docs, scores))\n</code></pre>"},{"location":"reference/agent/special/lance_tools/","title":"lance_tools","text":"<p>langroid/agent/special/lance_tools.py </p>"},{"location":"reference/agent/special/relevance_extractor_agent/","title":"relevance_extractor_agent","text":"<p>langroid/agent/special/relevance_extractor_agent.py </p> <p>Agent to retrieve relevant segments from a body of text, that are relevant to a query.</p>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent","title":"<code>RelevanceExtractorAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for extracting segments from text, that are relevant to a given query.</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def __init__(self, config: RelevanceExtractorAgentConfig):\n    super().__init__(config)\n    self.config: RelevanceExtractorAgentConfig = config\n    self.enable_message(SegmentExtractTool)\n    self.numbered_passage: Optional[str] = None\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    \"\"\"\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    return super().llm_response(prompt)\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM The LLM is expected to generate a structured msg according to the SegmentExtractTool schema, i.e. it should contain a <code>segment_list</code> field whose value is a list of segment numbers or ranges, like \"10,12,14-17\".</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    The LLM is expected to generate a structured msg according to the\n    SegmentExtractTool schema, i.e. it should contain a `segment_list` field\n    whose value is a list of segment numbers or ranges, like \"10,12,14-17\".\n    \"\"\"\n\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    return await super().llm_response_async(prompt)\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.extract_segments","title":"<code>extract_segments(msg)</code>","text":"<p>Method to handle a segmentExtractTool message from LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def extract_segments(self, msg: SegmentExtractTool) -&gt; str:\n    \"\"\"Method to handle a segmentExtractTool message from LLM\"\"\"\n    spec = msg.segment_list\n    if len(self.message_history) == 0:\n        return DONE + \" \" + NO_ANSWER\n    if spec is None or spec.strip() in [\"\", NO_ANSWER]:\n        return DONE + \" \" + NO_ANSWER\n    assert self.numbered_passage is not None, \"No numbered passage\"\n    # assume this has numbered segments\n    try:\n        extracts = extract_numbered_segments(self.numbered_passage, spec)\n    except Exception:\n        return DONE + \" \" + NO_ANSWER\n    # this response ends the task by saying DONE\n    return DONE + \" \" + extracts\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Handle case where LLM forgets to use SegmentExtractTool</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Handle case where LLM forgets to use SegmentExtractTool\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM:\n        return DONE + \" \" + NO_ANSWER\n    else:\n        return None\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/","title":"retriever_agent","text":"<p>langroid/agent/special/retriever_agent.py </p> <p>Deprecated: use DocChatAgent instead, with DocChatAgentConfig.retrieve_only=True, and if you want to retrieve FULL relevant doc-contents rather than just extracts, then set DocChatAgentConfig.extraction_granularity=-1</p> <p>This is an agent to retrieve relevant extracts from a vector store, where the LLM is used to filter for \"true\" relevance after retrieval from the vector store. This is essentially the same as DocChatAgent, except that instead of generating final summary answer based on relevant extracts, it just returns those extracts. See test_retriever_agent.py for example usage.</p>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent","title":"<code>RetrieverAgent(config)</code>","text":"<p>             Bases: <code>DocChatAgent</code></p> <p>Agent for just retrieving chunks/docs/extracts matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def __init__(self, config: DocChatAgentConfig):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    logger.warning(\n        \"\"\"\n    `RetrieverAgent` is deprecated. Use `DocChatAgent` instead, with\n    `DocChatAgentConfig.retrieve_only=True`, and if you want to retrieve\n    FULL relevant doc-contents rather than just extracts, then set\n    `DocChatAgentConfig.extraction_granularity=-1`\n    \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/","title":"table_chat_agent","text":"<p>langroid/agent/special/table_chat_agent.py </p> <p>Agent that supports asking queries about a tabular dataset, internally represented as a Pandas dataframe. The <code>TableChatAgent</code> is configured with a dataset, which can be a Pandas df, file or URL. The delimiter/separator is auto-detected. In response to a user query, the Agent's LLM generates a Pandas expression (involving a dataframe <code>df</code>) to answer the query. The expression is passed via the <code>pandas_eval</code> tool/function-call, which is handled by the Agent's <code>pandas_eval</code> method. This method evaluates the expression and returns the result as a string.</p>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.PandasEvalTool","title":"<code>PandasEvalTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Tool/function to evaluate a pandas expression involving a dataframe <code>df</code></p>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent","title":"<code>TableChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def __init__(self, config: TableChatAgentConfig):\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n    else:\n        df = read_tabular_data(config.data, config.separator)\n\n    df.columns = df.columns.str.strip().str.replace(\" +\", \"_\", regex=True)\n\n    self.df = df\n    summary = dataframe_summary(df)\n    config.system_message = config.system_message.format(summary=summary)\n\n    super().__init__(config)\n    self.config: TableChatAgentConfig = config\n\n    logger.info(\n        f\"\"\"TableChatAgent initialized with dataframe of shape {self.df.shape}\n        and columns: \n        {self.df.columns}\n        \"\"\"\n    )\n    # enable the agent to use and handle the PandasEvalTool\n    self.enable_message(PandasEvalTool)\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent.pandas_eval","title":"<code>pandas_eval(msg)</code>","text":"<p>Handle a PandasEvalTool message by evaluating the <code>expression</code> field     and returning the result. Args:     msg (PandasEvalTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of running the code along with any print output.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def pandas_eval(self, msg: PandasEvalTool) -&gt; str:\n    \"\"\"\n    Handle a PandasEvalTool message by evaluating the `expression` field\n        and returning the result.\n    Args:\n        msg (PandasEvalTool): The tool-message to handle.\n\n    Returns:\n        str: The result of running the code along with any print output.\n    \"\"\"\n    self.sent_expression = True\n    exprn = msg.expression\n    local_vars = {\"df\": self.df}\n    # Create a string-based I/O stream\n    code_out = io.StringIO()\n\n    # Temporarily redirect standard output to our string-based I/O stream\n    sys.stdout = code_out\n\n    # Evaluate the last line and get the result\n    try:\n        eval_result = pd.eval(exprn, local_dict=local_vars)\n    except Exception as e:\n        eval_result = f\"ERROR: {type(e)}: {e}\"\n\n    if eval_result is None:\n        eval_result = \"\"\n\n    # Always restore the original standard output\n    sys.stdout = sys.__stdout__\n\n    # If df has been modified in-place, save the changes back to self.df\n    self.df = local_vars[\"df\"]\n\n    # Get the resulting string from the I/O stream\n    print_result = code_out.getvalue() or \"\"\n    sep = \"\\n\" if print_result else \"\"\n    # Combine the print and eval results\n    result = f\"{print_result}{sep}{eval_result}\"\n    if result == \"\":\n        result = \"No result\"\n    # Return the result\n    return result\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Handle various LLM deviations</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Handle various LLM deviations\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == lr.Entity.LLM:\n        if msg.content.strip() == DONE and self.sent_expression:\n            # LLM sent an expression (i.e. used the `pandas_eval` tool)\n            # but upon receiving the results, simply said DONE without\n            # narrating the result as instructed.\n            return \"\"\"\n                You forgot to PRESENT the answer to the user's query\n                based on the results from `pandas_eval` tool.\n            \"\"\"\n        if self.sent_expression:\n            # LLM forgot to say DONE\n            self.sent_expression = False\n            return DONE + \" \" + PASS\n        else:\n            # LLM forgot to use the `pandas_eval` tool\n            return \"\"\"\n                You forgot to use the `pandas_eval` tool/function \n                to find the answer.\n                Try again using the `pandas_eval` tool/function.\n                \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.dataframe_summary","title":"<code>dataframe_summary(df)</code>","text":"<p>Generate a structured summary for a pandas DataFrame containing numerical and categorical values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to summarize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A nicely structured and formatted summary string.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>@no_type_check\ndef dataframe_summary(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Generate a structured summary for a pandas DataFrame containing numerical\n    and categorical values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to summarize.\n\n    Returns:\n        str: A nicely structured and formatted summary string.\n    \"\"\"\n\n    # Column names display\n    col_names_str = (\n        \"COLUMN NAMES:\\n\" + \" \".join([f\"'{col}'\" for col in df.columns]) + \"\\n\\n\"\n    )\n\n    # Numerical data summary\n    num_summary = df.describe().map(lambda x: \"{:.2f}\".format(x))\n    num_str = \"Numerical Column Summary:\\n\" + num_summary.to_string() + \"\\n\\n\"\n\n    # Categorical data summary\n    cat_columns = df.select_dtypes(include=[np.object_]).columns\n    cat_summary_list = []\n\n    for col in cat_columns:\n        unique_values = df[col].unique()\n        if len(unique_values) &lt; 10:\n            cat_summary_list.append(f\"'{col}': {', '.join(map(str, unique_values))}\")\n        else:\n            cat_summary_list.append(f\"'{col}': {df[col].nunique()} unique values\")\n\n    cat_str = \"Categorical Column Summary:\\n\" + \"\\n\".join(cat_summary_list) + \"\\n\\n\"\n\n    # Missing values summary\n    nan_summary = df.isnull().sum().rename(\"missing_values\").to_frame()\n    nan_str = \"Missing Values Column Summary:\\n\" + nan_summary.to_string() + \"\\n\"\n\n    # Combine the summaries into one structured string\n    summary_str = col_names_str + num_str + cat_str + nan_str\n\n    return summary_str\n</code></pre>"},{"location":"reference/agent/special/lance_rag/","title":"lance_rag","text":"<p>langroid/agent/special/lance_rag/init.py </p>"},{"location":"reference/agent/special/lance_rag/critic_agent/","title":"critic_agent","text":"<p>langroid/agent/special/lance_rag/critic_agent.py </p> <p>QueryPlanCritic is a ChatAgent that is created with a specific document schema.</p> <p>Its role is to provide feedback on a Query Plan, which consists of: - filter condition if needed (or empty string if no filter is needed) - query - a possibly rephrased query that can be used to match the <code>content</code> field - dataframe_calc - a Pandas-dataframe calculation/aggregation string, possibly empty - original_query - the original query for reference - result - the answer received from an assistant that used this QUERY PLAN.</p> <p>This agent has access to two tools: - QueryPlanTool: The handler method for this tool re-writes the query plan   in plain text (non-JSON) so the LLM can provide its feedback using the   QueryPlanFeedbackTool. - QueryPlanFeedbackTool: LLM uses this tool to provide feedback on the Query Plan</p>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic","title":"<code>QueryPlanCritic(cfg)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Critic for LanceQueryPlanAgent, provides feedback on query plan + answer.</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def __init__(self, cfg: LanceQueryPlanAgentConfig):\n    super().__init__(cfg)\n    self.config = cfg\n    self.enable_message(QueryPlanAnswerTool, use=False, handle=True)\n    self.enable_message(QueryPlanFeedbackTool, use=True, handle=True)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic.query_plan_answer","title":"<code>query_plan_answer(msg)</code>","text":"<p>Present query plan + answer in plain text (not JSON) so LLM can give feedback</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def query_plan_answer(self, msg: QueryPlanAnswerTool) -&gt; str:\n    \"\"\"Present query plan + answer in plain text (not JSON)\n    so LLM can give feedback\"\"\"\n    return plain_text_query_plan(msg)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic.query_plan_feedback","title":"<code>query_plan_feedback(msg)</code>","text":"<p>Format Valid so return to Query Planner</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def query_plan_feedback(self, msg: QueryPlanFeedbackTool) -&gt; str:\n    \"\"\"Format Valid so return to Query Planner\"\"\"\n    return DONE + \" \" + PASS  # return to Query Planner\n</code></pre>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Remind the LLM to use QueryPlanFeedbackTool since it forgot</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Remind the LLM to use QueryPlanFeedbackTool since it forgot\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM:\n        return \"\"\"\n        You forgot to use the `query_plan_feedback` tool/function.\n        Re-try your response using the `query_plan_feedback` tool/function,\n        remember to provide feedback in the `feedback` field,\n        and if any fix is suggested, provide it in the `suggested_fix` field.\n        \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/lance_rag/lance_rag_task/","title":"lance_rag_task","text":"<p>langroid/agent/special/lance_rag/lance_rag_task.py </p> <p>The LanceRAGTaskCreator.new() method creates a 3-Agent system that uses this agent. It takes a LanceDocChatAgent instance as argument, and adds two more agents: - LanceQueryPlanAgent, which is given the LanceDB schema in LanceDocChatAgent, and based on this schema, for a given user query, creates a Query Plan using the QueryPlanTool, which contains a filter, a rephrased query, and a dataframe_calc. - QueryPlanCritic, which is given the LanceDB schema in LanceDocChatAgent,  and gives feedback on the Query Plan and Result using the QueryPlanFeedbackTool.</p> <p>The LanceRAGTaskCreator.new() method sets up the given LanceDocChatAgent and QueryPlanCritic as sub-tasks of the LanceQueryPlanAgent's task.</p> <p>Langroid's built-in task orchestration ensures that: - the LanceQueryPlanAgent reformulates the plan based     on the QueryPlanCritics's feedback, - LLM deviations are corrected via tools and overrides of ChatAgent methods.</p>"},{"location":"reference/agent/special/lance_rag/lance_rag_task/#langroid.agent.special.lance_rag.lance_rag_task.LanceRAGTaskCreator","title":"<code>LanceRAGTaskCreator</code>","text":""},{"location":"reference/agent/special/lance_rag/lance_rag_task/#langroid.agent.special.lance_rag.lance_rag_task.LanceRAGTaskCreator.new","title":"<code>new(agent, interactive=True)</code>  <code>staticmethod</code>","text":"<p>Add a LanceFilterAgent to the LanceDocChatAgent, set up the corresponding Tasks, connect them, and return the top-level query_plan_task.</p> Source code in <code>langroid/agent/special/lance_rag/lance_rag_task.py</code> <pre><code>@staticmethod\ndef new(\n    agent: LanceDocChatAgent,\n    interactive: bool = True,\n) -&gt; Task:\n    \"\"\"\n    Add a LanceFilterAgent to the LanceDocChatAgent,\n    set up the corresponding Tasks, connect them,\n    and return the top-level query_plan_task.\n    \"\"\"\n    doc_agent_name = \"LanceRAG\"\n    critic_name = \"QueryPlanCritic\"\n    query_plan_agent_config = LanceQueryPlanAgentConfig(\n        critic_name=critic_name,\n        doc_agent_name=doc_agent_name,\n        doc_schema=agent._get_clean_vecdb_schema(),\n    )\n    query_plan_agent_config.set_system_message()\n\n    critic_config = QueryPlanCriticConfig(\n        doc_schema=agent._get_clean_vecdb_schema(),\n    )\n    critic_config.set_system_message()\n\n    query_planner = LanceQueryPlanAgent(query_plan_agent_config)\n    query_plan_task = Task(\n        query_planner,\n        interactive=interactive,\n    )\n    critic_agent = QueryPlanCritic(critic_config)\n    critic_task = Task(\n        critic_agent,\n        interactive=False,\n    )\n    rag_task = Task(\n        agent,\n        name=\"LanceRAG\",\n        interactive=False,\n        done_if_response=[Entity.LLM],  # done when non-null response from LLM\n        done_if_no_response=[Entity.LLM],  # done when null response from LLM\n    )\n    query_plan_task.add_sub_task([critic_task, rag_task])\n    return query_plan_task\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/","title":"query_planner_agent","text":"<p>langroid/agent/special/lance_rag/query_planner_agent.py </p> <p>LanceQueryPlanAgent is a ChatAgent created with a specific document schema. Given a QUERY, the LLM constructs a Query Plan consisting of: - filter condition if needed (or empty string if no filter is needed) - query - a possibly rephrased query that can be used to match the <code>content</code> field - dataframe_calc - a Pandas-dataframe calculation/aggregation string, possibly empty - original_query - the original query for reference</p> <p>This agent has access to two tools: - QueryPlanTool, which is used to generate the Query Plan, and the handler of     this tool simply passes it on to the RAG agent named in config.doc_agent_name. - QueryPlanFeedbackTool, which is used to handle feedback on the Query Plan and   Result from the RAG agent. The QueryPlanFeedbackTool is used by   the QueryPlanCritic, who inserts feedback into the <code>feedback</code> field</p>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent","title":"<code>LanceQueryPlanAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def __init__(self, config: LanceQueryPlanAgentConfig):\n    super().__init__(config)\n    self.config: LanceQueryPlanAgentConfig = config\n    self.curr_query_plan: QueryPlan | None = None\n    # how many times re-trying query plan in response to feedback:\n    self.n_retries: int = 0\n    self.result: str = \"\"  # answer received from LanceRAG\n    # This agent should generate the QueryPlanTool\n    # as well as handle it for validation\n    self.enable_message(QueryPlanTool, use=True, handle=True)\n    self.enable_message(QueryPlanFeedbackTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.query_plan","title":"<code>query_plan(msg)</code>","text":"<p>Valid, forward to RAG Agent</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def query_plan(self, msg: QueryPlanTool) -&gt; str:\n    \"\"\"Valid, forward to RAG Agent\"\"\"\n    # save, to be used to assemble QueryPlanResultTool\n    if len(msg.plan.dataframe_calc.split(\"\\n\")) &gt; 1:\n        return \"DATAFRAME CALCULATION must be a SINGLE LINE; Retry the `query_plan`\"\n    self.curr_query_plan = msg.plan\n    return PASS_TO + self.config.doc_agent_name\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.query_plan_feedback","title":"<code>query_plan_feedback(msg)</code>","text":"<p>Process Critic feedback on QueryPlan + Answer from RAG Agent</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def query_plan_feedback(self, msg: QueryPlanFeedbackTool) -&gt; str:\n    \"\"\"Process Critic feedback on QueryPlan + Answer from RAG Agent\"\"\"\n    # We should have saved answer in self.result by this time,\n    # since this Agent seeks feedback only after receiving RAG answer.\n    if msg.suggested_fix == \"\":\n        self.n_retries = 0\n        # This means the Query Plan or Result is good, as judged by Critic\n        if self.result == \"\":\n            # This was feedback for query with no result\n            return \"QUERY PLAN LOOKS GOOD!\"\n        elif self.result == NO_ANSWER:\n            return NO_ANSWER\n        else:  # non-empty and non-null answer\n            return DONE + \" \" + self.result\n    self.n_retries += 1\n    if self.n_retries &gt;= self.config.max_retries:\n        # bail out to avoid infinite loop\n        self.n_retries = 0\n        return DONE + \" \" + NO_ANSWER\n    return f\"\"\"\n    here is FEEDBACK about your QUERY PLAN, and a SUGGESTED FIX.\n    Modify the QUERY PLAN if needed:\n    FEEDBACK: {msg.feedback}\n    SUGGESTED FIX: {msg.suggested_fix}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"Process answer received from RAG Agent <p>Construct a QueryPlanAnswerTool with the answer, and forward to Critic for feedback.</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Process answer received from RAG Agent:\n     Construct a QueryPlanAnswerTool with the answer,\n     and forward to Critic for feedback.\n    \"\"\"\n    # TODO we don't need to use this fallback method. instead we can\n    # first call result = super().agent_response(), and if result is None,\n    # then we know there was no tool, so we run below code\n    if (\n        isinstance(msg, ChatDocument)\n        and self.curr_query_plan is not None\n        and msg.metadata.parent is not None\n    ):\n        # save result, to be used in query_plan_feedback()\n        self.result = msg.content\n        # assemble QueryPlanAnswerTool...\n        query_plan_answer_tool = QueryPlanAnswerTool(  # type: ignore\n            plan=self.curr_query_plan,\n            answer=self.result,\n        )\n        response_tmpl = self.create_agent_response()\n        # ... add the QueryPlanAnswerTool to the response\n        # (Notice how the Agent is directly sending a tool, not the LLM)\n        response_tmpl.tool_messages = [query_plan_answer_tool]\n        # set the recipient to the Critic so it can give feedback\n        response_tmpl.metadata.recipient = self.config.critic_name\n        self.curr_query_plan = None  # reset\n        return response_tmpl\n    if (\n        isinstance(msg, ChatDocument)\n        and not self.has_tool_message_attempt(msg)\n        and msg.metadata.sender == lr.Entity.LLM\n    ):\n        # remind LLM to use the QueryPlanFeedbackTool\n        return \"\"\"\n        You forgot to use the `query_plan` tool/function.\n        Re-try your response using the `query_plan` tool/function.\n        \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/neo4j/","title":"neo4j","text":"<p>langroid/agent/special/neo4j/init.py </p>"},{"location":"reference/agent/special/neo4j/csv_kg_chat/","title":"csv_kg_chat","text":"<p>langroid/agent/special/neo4j/csv_kg_chat.py </p>"},{"location":"reference/agent/special/neo4j/csv_kg_chat/#langroid.agent.special.neo4j.csv_kg_chat.CSVGraphAgent","title":"<code>CSVGraphAgent(config)</code>","text":"<p>             Bases: <code>Neo4jChatAgent</code></p> Source code in <code>langroid/agent/special/neo4j/csv_kg_chat.py</code> <pre><code>def __init__(self, config: CSVGraphAgentConfig):\n    formatted_build_instr = \"\"\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n        self.df = df\n    else:\n        if config.data:\n            df = read_tabular_data(config.data, config.separator)\n            df_cleaned = _preprocess_dataframe_for_neo4j(df)\n\n            df_cleaned.columns = df_cleaned.columns.str.strip().str.replace(\n                \" +\", \"_\", regex=True\n            )\n\n            self.df = df_cleaned\n\n            formatted_build_instr = BUILD_KG_INSTRUCTIONS.format(\n                header=self.df.columns, sample_rows=self.df.head(3)\n            )\n\n    config.system_message = config.system_message + formatted_build_instr\n    super().__init__(config)\n\n    self.config: Neo4jChatAgentConfig = config\n\n    self.enable_message(PandasToKGTool)\n</code></pre>"},{"location":"reference/agent/special/neo4j/csv_kg_chat/#langroid.agent.special.neo4j.csv_kg_chat.CSVGraphAgent.pandas_to_kg","title":"<code>pandas_to_kg(msg)</code>","text":"<p>Creates nodes and relationships in the graph database based on the data in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>PandasToKGTool</code> <p>An instance of the PandasToKGTool class containing the necessary information for generating nodes.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string indicating the success or failure of the operation.</p> Source code in <code>langroid/agent/special/neo4j/csv_kg_chat.py</code> <pre><code>def pandas_to_kg(self, msg: PandasToKGTool) -&gt; str:\n    \"\"\"\n    Creates nodes and relationships in the graph database based on the data in\n    a CSV file.\n\n    Args:\n        msg (PandasToKGTool): An instance of the PandasToKGTool class containing\n            the necessary information for generating nodes.\n\n    Returns:\n        str: A string indicating the success or failure of the operation.\n    \"\"\"\n    with status(\"[cyan]Generating graph database...\"):\n        if self.df is not None and hasattr(self.df, \"iterrows\"):\n            for counter, (index, row) in enumerate(self.df.iterrows()):\n                row_dict = row.to_dict()\n                response = self.write_query(\n                    msg.cypherQuery,\n                    parameters={header: row_dict[header] for header in msg.args},\n                )\n                # there is a possibility the generated cypher query is not correct\n                # so we need to check the response before continuing to the\n                # iteration\n                if counter == 0 and not response.success:\n                    return str(response.data)\n        return \"Graph database successfully generated\"\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/","title":"neo4j_chat_agent","text":"<p>langroid/agent/special/neo4j/neo4j_chat_agent.py </p>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent","title":"<code>Neo4jChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database information is not provided in the config.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def __init__(self, config: Neo4jChatAgentConfig):\n    \"\"\"Initialize the Neo4jChatAgent.\n\n    Raises:\n        ValueError: If database information is not provided in the config.\n    \"\"\"\n    self.config = config\n    self._validate_config()\n    self._import_neo4j()\n    self._initialize_connection()\n    self._init_tool_messages()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.close","title":"<code>close()</code>","text":"<p>close the connection</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"close the connection\"\"\"\n    if self.driver:\n        self.driver.close()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.retry_query","title":"<code>retry_query(e, query)</code>","text":"<p>Generate an error message for a failed Cypher query and return it.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>The exception raised during the Cypher query execution.</p> required <code>query</code> <code>str</code> <p>The Cypher query that failed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def retry_query(self, e: Exception, query: str) -&gt; str:\n    \"\"\"\n    Generate an error message for a failed Cypher query and return it.\n\n    Args:\n        e (Exception): The exception raised during the Cypher query execution.\n        query (str): The Cypher query that failed.\n\n    Returns:\n        str: The error message.\n    \"\"\"\n    logger.error(f\"Cypher Query failed: {query}\\nException: {e}\")\n\n    # Construct the error message\n    error_message_template = f\"\"\"\\\n    {NEO4J_ERROR_MSG}: '{query}'\n    {str(e)}\n    Run a new query, correcting the errors.\n    \"\"\"\n\n    return error_message_template\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.read_query","title":"<code>read_query(query, parameters=None)</code>","text":"<p>Executes a given Cypher query with parameters on the Neo4j database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The Cypher query string to be executed.</p> required <code>parameters</code> <code>Optional[Dict[Any, Any]]</code> <p>A dictionary of parameters for                                     the query.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>QueryResult</code> <code>QueryResult</code> <p>An object representing the outcome of the query execution.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def read_query(\n    self, query: str, parameters: Optional[Dict[Any, Any]] = None\n) -&gt; QueryResult:\n    \"\"\"\n    Executes a given Cypher query with parameters on the Neo4j database.\n\n    Args:\n        query (str): The Cypher query string to be executed.\n        parameters (Optional[Dict[Any, Any]]): A dictionary of parameters for\n                                                the query.\n\n    Returns:\n        QueryResult: An object representing the outcome of the query execution.\n    \"\"\"\n    if not self.driver:\n        return QueryResult(\n            success=False, data=\"No database connection is established.\"\n        )\n\n    try:\n        assert isinstance(self.config, Neo4jChatAgentConfig)\n        with self.driver.session(\n            database=self.config.neo4j_settings.database\n        ) as session:\n            result = session.run(query, parameters)\n            if result.peek():\n                records = [record.data() for record in result]\n                return QueryResult(success=True, data=records)\n            else:\n                return QueryResult(success=True, data=[])\n    except Exception as e:\n        logger.error(f\"Failed to execute query: {query}\\n{e}\")\n        error_message = self.retry_query(e, query)\n        return QueryResult(success=False, data=error_message)\n    finally:\n        self.close()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.write_query","title":"<code>write_query(query, parameters=None)</code>","text":"<p>Executes a write transaction using a given Cypher query on the Neo4j database. This method should be used for queries that modify the database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The Cypher query string to be executed.</p> required <code>parameters</code> <code>dict</code> <p>A dict of parameters for the Cypher query.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>QueryResult</code> <code>QueryResult</code> <p>An object representing the outcome of the query execution.          It contains a success flag and an optional error message.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def write_query(\n    self, query: str, parameters: Optional[Dict[Any, Any]] = None\n) -&gt; QueryResult:\n    \"\"\"\n    Executes a write transaction using a given Cypher query on the Neo4j database.\n    This method should be used for queries that modify the database.\n\n    Args:\n        query (str): The Cypher query string to be executed.\n        parameters (dict, optional): A dict of parameters for the Cypher query.\n\n    Returns:\n        QueryResult: An object representing the outcome of the query execution.\n                     It contains a success flag and an optional error message.\n    \"\"\"\n    if not self.driver:\n        return QueryResult(\n            success=False, data=\"No database connection is established.\"\n        )\n\n    try:\n        assert isinstance(self.config, Neo4jChatAgentConfig)\n        with self.driver.session(\n            database=self.config.neo4j_settings.database\n        ) as session:\n            session.write_transaction(lambda tx: tx.run(query, parameters))\n            return QueryResult(success=True)\n    except Exception as e:\n        logging.warning(f\"An error occurred: {e}\")\n        error_message = self.retry_query(e, query)\n        return QueryResult(success=False, data=error_message)\n    finally:\n        self.close()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.remove_database","title":"<code>remove_database()</code>","text":"<p>Deletes all nodes and relationships from the current Neo4j database.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def remove_database(self) -&gt; None:\n    \"\"\"Deletes all nodes and relationships from the current Neo4j database.\"\"\"\n    delete_query = \"\"\"\n            MATCH (n)\n            DETACH DELETE n\n        \"\"\"\n    response = self.write_query(delete_query)\n\n    if response.success:\n        print(\"[green]Database is deleted!\")\n    else:\n        print(\"[red]Database is not deleted!\")\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.retrieval_query","title":"<code>retrieval_query(msg)</code>","text":"<p>\" Handle a CypherRetrievalTool message by executing a Cypher query and returning the result. Args:     msg (CypherRetrievalTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the cypher_query.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def retrieval_query(self, msg: CypherRetrievalTool) -&gt; str:\n    \"\"\" \"\n    Handle a CypherRetrievalTool message by executing a Cypher query and\n    returning the result.\n    Args:\n        msg (CypherRetrievalTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the cypher_query.\n    \"\"\"\n    query = msg.cypher_query\n\n    logger.info(f\"Executing Cypher query: {query}\")\n    response = self.read_query(query)\n    if response.success:\n        return json.dumps(response.data)\n    else:\n        return str(response.data)\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.create_query","title":"<code>create_query(msg)</code>","text":"<p>\" Handle a CypherCreationTool message by executing a Cypher query and returning the result. Args:     msg (CypherCreationTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the cypher_query.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def create_query(self, msg: CypherCreationTool) -&gt; str:\n    \"\"\" \"\n    Handle a CypherCreationTool message by executing a Cypher query and\n    returning the result.\n    Args:\n        msg (CypherCreationTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the cypher_query.\n    \"\"\"\n    query = msg.cypher_query\n\n    logger.info(f\"Executing Cypher query: {query}\")\n    response = self.write_query(query)\n    if response.success:\n        return \"Cypher query executed successfully\"\n    else:\n        return str(response.data)\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.get_schema","title":"<code>get_schema(msg)</code>","text":"<p>Retrieves the schema of a Neo4j graph database.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>GraphSchemaTool</code> <p>An instance of GraphDatabaseSchema, typically</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The visual representation of the database schema as a string, or a</p> <code>str</code> <p>message stating that the database schema is empty or not valid.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def get_schema(self, msg: GraphSchemaTool | None) -&gt; str:\n    \"\"\"\n    Retrieves the schema of a Neo4j graph database.\n\n    Args:\n        msg (GraphSchemaTool): An instance of GraphDatabaseSchema, typically\n        containing information or parameters needed for the database query.\n\n    Returns:\n        str: The visual representation of the database schema as a string, or a\n        message stating that the database schema is empty or not valid.\n\n    Raises:\n        This function does not explicitly raise exceptions but depends on the\n        behavior of 'self.read_query' method, which might raise exceptions related\n         to database connectivity or query execution.\n    \"\"\"\n    schema_result = self.read_query(\"CALL db.schema.visualization()\")\n    if schema_result.success:\n        # ther is a possibility that the schema is empty, which is a valid response\n        # the schema.data will be: [{\"nodes\": [], \"relationships\": []}]\n        return json.dumps(schema_result.data)\n    else:\n        return f\"Failed to retrieve schema: {schema_result.data}\"\n</code></pre>"},{"location":"reference/agent/special/neo4j/utils/","title":"utils","text":"<p>langroid/agent/special/neo4j/utils/init.py </p>"},{"location":"reference/agent/special/neo4j/utils/system_message/","title":"system_message","text":"<p>langroid/agent/special/neo4j/utils/system_message.py </p>"},{"location":"reference/agent/special/sql/","title":"sql","text":"<p>langroid/agent/special/sql/init.py </p>"},{"location":"reference/agent/special/sql/sql_chat_agent/","title":"sql_chat_agent","text":"<p>langroid/agent/special/sql/sql_chat_agent.py </p> <p>Agent that allows interaction with an SQL database using SQLAlchemy library.  The agent can execute SQL queries in the database and return the result. </p> <p>Functionality includes: - adding table and column context - asking a question about a SQL schema</p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgentConfig","title":"<code>SQLChatAgentConfig</code>","text":"<p>             Bases: <code>ChatAgentConfig</code></p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgentConfig.multi_schema","title":"<code>multi_schema: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional, but strongly recommended, context descriptions for tables, columns,  and relationships. It should be a dictionary where each key is a table name  and its value is another dictionary. </p> <p>In this inner dictionary: - The 'description' key corresponds to a string description of the table. - The 'columns' key corresponds to another dictionary where each key is a  column name and its value is a string description of that column. - The 'relationships' key corresponds to another dictionary where each key  is another table name and the value is a description of the relationship to  that table.</p> <p>If multi_schema support is enabled, the tables names in the description should be of the form 'schema_name.table_name'.</p> <p>For example: {     'table1': {         'description': 'description of table1',         'columns': {             'column1': 'description of column1 in table1',             'column2': 'description of column2 in table1'         }     },     'table2': {         'description': 'description of table2',         'columns': {             'column3': 'description of column3 in table2',             'column4': 'description of column4 in table2'         }     } }</p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent","title":"<code>SQLChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a SQL database</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database information is not provided in the config.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def __init__(self, config: \"SQLChatAgentConfig\") -&gt; None:\n    \"\"\"Initialize the SQLChatAgent.\n\n    Raises:\n        ValueError: If database information is not provided in the config.\n    \"\"\"\n    self._validate_config(config)\n    self.config: SQLChatAgentConfig = config\n    self._init_database()\n    self._init_metadata()\n    self._init_table_metadata()\n    self._init_message_tools()\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.retry_query","title":"<code>retry_query(e, query)</code>","text":"<p>Generate an error message for a failed SQL query and return it.</p> <p>Parameters: e (Exception): The exception raised during the SQL query execution. query (str): The SQL query that failed.</p> <p>Returns: str: The error message.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def retry_query(self, e: Exception, query: str) -&gt; str:\n    \"\"\"\n    Generate an error message for a failed SQL query and return it.\n\n    Parameters:\n    e (Exception): The exception raised during the SQL query execution.\n    query (str): The SQL query that failed.\n\n    Returns:\n    str: The error message.\n    \"\"\"\n    logger.error(f\"SQL Query failed: {query}\\nException: {e}\")\n\n    # Optional part to be included based on `use_schema_tools`\n    optional_schema_description = \"\"\n    if not self.config.use_schema_tools:\n        optional_schema_description = f\"\"\"\\\n        This JSON schema maps SQL database structure. It outlines tables, each \n        with a description and columns. Each table is identified by a key, and holds\n        a description and a dictionary of columns, with column \n        names as keys and their descriptions as values.\n\n        ```json\n        {self.config.context_descriptions}\n        ```\"\"\"\n\n    # Construct the error message\n    error_message_template = f\"\"\"\\\n    {SQL_ERROR_MSG}: '{query}'\n    {str(e)}\n    Run a new query, correcting the errors.\n    {optional_schema_description}\"\"\"\n\n    return error_message_template\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.run_query","title":"<code>run_query(msg)</code>","text":"<p>Handle a RunQueryTool message by executing a SQL query and returning the result.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>RunQueryTool</code> <p>The tool-message to handle.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the SQL query.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def run_query(self, msg: RunQueryTool) -&gt; str:\n    \"\"\"\n    Handle a RunQueryTool message by executing a SQL query and returning the result.\n\n    Args:\n        msg (RunQueryTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the SQL query.\n    \"\"\"\n    query = msg.query\n    session = self.Session\n    response_message = \"\"\n\n    try:\n        logger.info(f\"Executing SQL query: {query}\")\n\n        query_result = session.execute(text(query))\n        session.commit()\n\n        rows = query_result.fetchall()\n        response_message = self._format_rows(rows)\n\n    except SQLAlchemyError as e:\n        session.rollback()\n        logger.error(f\"Failed to execute query: {query}\\n{e}\")\n        response_message = self.retry_query(e, query)\n    finally:\n        session.close()\n\n    return response_message\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_table_names","title":"<code>get_table_names(msg)</code>","text":"<p>Handle a GetTableNamesTool message by returning the names of all tables in the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The names of all tables in the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_table_names(self, msg: GetTableNamesTool) -&gt; str:\n    \"\"\"\n    Handle a GetTableNamesTool message by returning the names of all tables in the\n    database.\n\n    Returns:\n        str: The names of all tables in the database.\n    \"\"\"\n    if isinstance(self.metadata, list):\n        table_names = [\", \".join(md.tables.keys()) for md in self.metadata]\n        return \", \".join(table_names)\n\n    return \", \".join(self.metadata.tables.keys())\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_table_schema","title":"<code>get_table_schema(msg)</code>","text":"<p>Handle a GetTableSchemaTool message by returning the schema of all provided tables in the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The schema of all provided tables in the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_table_schema(self, msg: GetTableSchemaTool) -&gt; str:\n    \"\"\"\n    Handle a GetTableSchemaTool message by returning the schema of all provided\n    tables in the database.\n\n    Returns:\n        str: The schema of all provided tables in the database.\n    \"\"\"\n    tables = msg.tables\n    result = \"\"\n    for table_name in tables:\n        table = self.table_metadata.get(table_name)\n        if table is not None:\n            result += f\"{table_name}: {table}\\n\"\n        else:\n            result += f\"{table_name} is not a valid table name.\\n\"\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_column_descriptions","title":"<code>get_column_descriptions(msg)</code>","text":"<p>Handle a GetColumnDescriptionsTool message by returning the descriptions of all provided columns from the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The descriptions of all provided columns from the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_column_descriptions(self, msg: GetColumnDescriptionsTool) -&gt; str:\n    \"\"\"\n    Handle a GetColumnDescriptionsTool message by returning the descriptions of all\n    provided columns from the database.\n\n    Returns:\n        str: The descriptions of all provided columns from the database.\n    \"\"\"\n    table = msg.table\n    columns = msg.columns.split(\", \")\n    result = f\"\\nTABLE: {table}\"\n    descriptions = self.config.context_descriptions.get(table)\n\n    for col in columns:\n        result += f\"\\n{col} =&gt; {descriptions['columns'][col]}\"  # type: ignore\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/","title":"utils","text":"<p>langroid/agent/special/sql/utils/init.py </p>"},{"location":"reference/agent/special/sql/utils/description_extractors/","title":"description_extractors","text":"<p>langroid/agent/special/sql/utils/description_extractors.py </p>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_postgresql_descriptions","title":"<code>extract_postgresql_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts descriptions for tables and columns from a PostgreSQL database.</p> <p>This method retrieves the descriptions of tables and their columns from a PostgreSQL database using the provided SQLAlchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a PostgreSQL database.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing the table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_postgresql_descriptions(\n    engine: Engine,\n    multi_schema: bool = False,\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Extracts descriptions for tables and columns from a PostgreSQL database.\n\n    This method retrieves the descriptions of tables and their columns\n    from a PostgreSQL database using the provided SQLAlchemy engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a PostgreSQL database.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing the table description and a dictionary of\n        column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    result: Dict[str, Dict[str, Any]] = {}\n\n    def gen_schema_descriptions(schema: Optional[str] = None) -&gt; None:\n        table_names: List[str] = inspector.get_table_names(schema=schema)\n        with engine.connect() as conn:\n            for table in table_names:\n                if schema is None:\n                    table_name = table\n                else:\n                    table_name = f\"{schema}.{table}\"\n\n                table_comment = (\n                    conn.execute(\n                        text(f\"SELECT obj_description('{table_name}'::regclass)\")\n                    ).scalar()\n                    or \"\"\n                )\n\n                columns = {}\n                col_data = inspector.get_columns(table, schema=schema)\n                for idx, col in enumerate(col_data, start=1):\n                    col_comment = (\n                        conn.execute(\n                            text(\n                                f\"SELECT col_description('{table_name}'::regclass, \"\n                                f\"{idx})\"\n                            )\n                        ).scalar()\n                        or \"\"\n                    )\n                    columns[col[\"name\"]] = col_comment\n\n                result[table_name] = {\"description\": table_comment, \"columns\": columns}\n\n    if multi_schema:\n        for schema in inspector.get_schema_names():\n            gen_schema_descriptions(schema)\n    else:\n        gen_schema_descriptions()\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_mysql_descriptions","title":"<code>extract_mysql_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts descriptions for tables and columns from a MySQL database.</p> <p>This method retrieves the descriptions of tables and their columns from a MySQL database using the provided SQLAlchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a MySQL database.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing the table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_mysql_descriptions(\n    engine: Engine,\n    multi_schema: bool = False,\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Extracts descriptions for tables and columns from a MySQL database.\n\n    This method retrieves the descriptions of tables and their columns\n    from a MySQL database using the provided SQLAlchemy engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a MySQL database.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing the table description and a dictionary of\n        column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    result: Dict[str, Dict[str, Any]] = {}\n\n    def gen_schema_descriptions(schema: Optional[str] = None) -&gt; None:\n        table_names: List[str] = inspector.get_table_names(schema=schema)\n\n        with engine.connect() as conn:\n            for table in table_names:\n                if schema is None:\n                    table_name = table\n                else:\n                    table_name = f\"{schema}.{table}\"\n\n                query = text(\n                    \"SELECT table_comment FROM information_schema.tables WHERE\"\n                    \" table_schema = :schema AND table_name = :table\"\n                )\n                table_result = conn.execute(\n                    query, {\"schema\": engine.url.database, \"table\": table_name}\n                )\n                table_comment = table_result.scalar() or \"\"\n\n                columns = {}\n                for col in inspector.get_columns(table, schema=schema):\n                    columns[col[\"name\"]] = col.get(\"comment\", \"\")\n\n                result[table_name] = {\"description\": table_comment, \"columns\": columns}\n\n    if multi_schema:\n        for schema in inspector.get_schema_names():\n            gen_schema_descriptions(schema)\n    else:\n        gen_schema_descriptions()\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_default_descriptions","title":"<code>extract_default_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts default descriptions for tables and columns from a database.</p> <p>This method retrieves the table and column names from the given database and associates empty descriptions with them.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a database.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing an empty table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>empty column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_default_descriptions(\n    engine: Engine, multi_schema: bool = False\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Extracts default descriptions for tables and columns from a database.\n\n    This method retrieves the table and column names from the given database\n    and associates empty descriptions with them.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a database.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing an empty table description and a dictionary of\n        empty column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    result: Dict[str, Dict[str, Any]] = {}\n\n    def gen_schema_descriptions(schema: Optional[str] = None) -&gt; None:\n        table_names: List[str] = inspector.get_table_names(schema=schema)\n\n        for table in table_names:\n            columns = {}\n            for col in inspector.get_columns(table):\n                columns[col[\"name\"]] = \"\"\n\n            result[table] = {\"description\": \"\", \"columns\": columns}\n\n    if multi_schema:\n        for schema in inspector.get_schema_names():\n            gen_schema_descriptions(schema)\n    else:\n        gen_schema_descriptions()\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_schema_descriptions","title":"<code>extract_schema_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts the schema descriptions from the database connected to by the engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine instance.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary representation of table and column</p> <code>Dict[str, Dict[str, Any]]</code> <p>descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_schema_descriptions(\n    engine: Engine, multi_schema: bool = False\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Extracts the schema descriptions from the database connected to by the engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine instance.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary representation of table and column\n        descriptions.\n    \"\"\"\n\n    extractors = {\n        \"postgresql\": extract_postgresql_descriptions,\n        \"mysql\": extract_mysql_descriptions,\n    }\n    return extractors.get(engine.dialect.name, extract_default_descriptions)(\n        engine, multi_schema=multi_schema\n    )\n</code></pre>"},{"location":"reference/agent/special/sql/utils/populate_metadata/","title":"populate_metadata","text":"<p>langroid/agent/special/sql/utils/populate_metadata.py </p>"},{"location":"reference/agent/special/sql/utils/populate_metadata/#langroid.agent.special.sql.utils.populate_metadata.populate_metadata_with_schema_tools","title":"<code>populate_metadata_with_schema_tools(metadata, info)</code>","text":"<p>Extracts information from an SQLAlchemy database's metadata and combines it with another dictionary with context descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>MetaData</code> <p>SQLAlchemy metadata object of the database.</p> required <code>info</code> <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary with table and column                                  descriptions.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Union[str, Dict[str, str]]]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary with table and context information.</p> Source code in <code>langroid/agent/special/sql/utils/populate_metadata.py</code> <pre><code>def populate_metadata_with_schema_tools(\n    metadata: MetaData | List[MetaData],\n    info: Dict[str, Dict[str, Union[str, Dict[str, str]]]],\n) -&gt; Dict[str, Dict[str, Union[str, Dict[str, str]]]]:\n    \"\"\"\n    Extracts information from an SQLAlchemy database's metadata and combines it\n    with another dictionary with context descriptions.\n\n    Args:\n        metadata (MetaData): SQLAlchemy metadata object of the database.\n        info (Dict[str, Dict[str, Any]]): A dictionary with table and column\n                                             descriptions.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary with table and context information.\n    \"\"\"\n    db_info: Dict[str, Dict[str, Union[str, Dict[str, str]]]] = {}\n\n    def populate_metadata(md: MetaData) -&gt; None:\n        # Create empty metadata dictionary with column datatypes\n        for table_name, table in md.tables.items():\n            # Populate tables with empty descriptions\n            db_info[table_name] = {\n                \"description\": info[table_name][\"description\"] or \"\",\n                \"columns\": {},\n            }\n\n            for column in table.columns:\n                # Populate columns with datatype\n                db_info[table_name][\"columns\"][str(column.name)] = (  # type: ignore\n                    str(column.type)\n                )\n\n    if isinstance(metadata, list):\n        for md in metadata:\n            populate_metadata(md)\n    else:\n        populate_metadata(metadata)\n\n    return db_info\n</code></pre>"},{"location":"reference/agent/special/sql/utils/populate_metadata/#langroid.agent.special.sql.utils.populate_metadata.populate_metadata","title":"<code>populate_metadata(metadata, info)</code>","text":"<p>Populate metadata based on the provided database metadata and additional info.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>MetaData</code> <p>Metadata object from SQLAlchemy.</p> required <code>info</code> <code>Dict</code> <p>Additional information for database tables and columns.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Dict[str, Union[str, Dict[str, str]]]]</code> <p>A dictionary containing populated metadata information.</p> Source code in <code>langroid/agent/special/sql/utils/populate_metadata.py</code> <pre><code>def populate_metadata(\n    metadata: MetaData | List[MetaData],\n    info: Dict[str, Dict[str, Union[str, Dict[str, str]]]],\n) -&gt; Dict[str, Dict[str, Union[str, Dict[str, str]]]]:\n    \"\"\"\n    Populate metadata based on the provided database metadata and additional info.\n\n    Args:\n        metadata (MetaData): Metadata object from SQLAlchemy.\n        info (Dict): Additional information for database tables and columns.\n\n    Returns:\n        Dict: A dictionary containing populated metadata information.\n    \"\"\"\n    # Fetch basic metadata info using available tools\n    db_info: Dict[str, Dict[str, Union[str, Dict[str, str]]]] = (\n        populate_metadata_with_schema_tools(metadata=metadata, info=info)\n    )\n\n    # Iterate over tables to update column metadata\n    for table_name in db_info.keys():\n        # Update only if additional info for the table exists\n        if table_name in info:\n            for column_name in db_info[table_name][\"columns\"]:\n                # Merge and update column description if available\n                if column_name in info[table_name][\"columns\"]:\n                    db_info[table_name][\"columns\"][column_name] = (  # type: ignore\n                        db_info[table_name][\"columns\"][column_name]  # type: ignore\n                        + \"; \"\n                        + info[table_name][\"columns\"][column_name]  # type: ignore\n                    )\n\n    return db_info\n</code></pre>"},{"location":"reference/agent/special/sql/utils/system_message/","title":"system_message","text":"<p>langroid/agent/special/sql/utils/system_message.py </p>"},{"location":"reference/agent/special/sql/utils/tools/","title":"tools","text":"<p>langroid/agent/special/sql/utils/tools.py </p>"},{"location":"reference/agent/tools/","title":"tools","text":"<p>langroid/agent/tools/init.py </p>"},{"location":"reference/agent/tools/#langroid.agent.tools.AddRecipientTool","title":"<code>AddRecipientTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to add a recipient to the previous message, when it has forgotten to specify a recipient. This avoids having to re-generate the previous message (and thus saves token-cost and time).</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.AddRecipientTool.response","title":"<code>response(agent)</code>","text":"<p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; ChatDocument:\n    \"\"\"\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.recipient.\n    \"\"\"\n    print(\n        \"[red]RecipientTool: \"\n        f\"Added recipient {self.intended_recipient} to message.\"\n    )\n    if self.__class__.saved_content == \"\":\n        recipient_request_name = RecipientTool.default_value(\"request\")\n        content = f\"\"\"\n            Recipient specified but content is empty!\n            This could be because the `{self.request}` tool/function was used \n            before using `{recipient_request_name}` tool/function.\n            Resend the message using `{recipient_request_name}` tool/function.\n            \"\"\"\n    else:\n        content = self.__class__.saved_content  # use class-level attrib value\n        # erase content since we just used it.\n        self.__class__.saved_content = \"\"\n    return ChatDocument(\n        content=content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as it msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool","title":"<code>RecipientTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to send a message to a specific recipient.</p> <p>Useful in cases where an LLM is talking to 2 or more agents (or an Agent and human user), and needs to specify which agent (task) its message is intended for. The recipient name should be the name of a task (which is normally the name of the agent that the task wraps, although the task can have its own name).</p> <p>To use this tool/function-call, LLM must generate a JSON structure with these fields: {     \"request\": \"recipient_message\", # also the function name when using fn-calling     \"intended_recipient\": ,     \"content\":  } The effect of this is that <code>content</code> will be sent to the <code>intended_recipient</code> task."},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.create","title":"<code>create(recipients, default='')</code>  <code>classmethod</code>","text":"<p>Create a restricted version of RecipientTool that only allows certain recipients, and possibly sets a default recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef create(cls, recipients: List[str], default: str = \"\") -&gt; Type[\"RecipientTool\"]:\n    \"\"\"Create a restricted version of RecipientTool that\n    only allows certain recipients, and possibly sets a default recipient.\"\"\"\n\n    class RecipientToolRestricted(cls):  # type: ignore\n        allowed_recipients = recipients\n        default_recipient = default\n\n    return RecipientToolRestricted\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Generate instructions for using this tool/function. These are intended to be appended to the system message of the LLM.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Generate instructions for using this tool/function.\n    These are intended to be appended to the system message of the LLM.\n    \"\"\"\n    recipients = []\n    if has_field(cls, \"allowed_recipients\"):\n        recipients = cls.default_value(\"allowed_recipients\")\n    if len(recipients) &gt; 0:\n        recipients_str = \", \".join(recipients)\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to one of the following:\n        {recipients_str},\n        and setting the 'content' field to your message.\n        \"\"\"\n    else:\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to the name of the recipient, \n        and setting the 'content' field to your message.\n        \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.response","title":"<code>response(agent)</code>","text":"<p>When LLM has correctly used this tool, construct a ChatDocument with an explicit recipient, and make it look like it is from the LLM.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.intended_recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    When LLM has correctly used this tool,\n    construct a ChatDocument with an explicit recipient,\n    and make it look like it is from the LLM.\n\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.intended_recipient.\n    \"\"\"\n    default_recipient = self.__class__.default_value(\"default_recipient\")\n    if self.intended_recipient == \"\" and default_recipient not in [\"\", None]:\n        self.intended_recipient = default_recipient\n    elif self.intended_recipient == \"\":\n        # save the content as a class-variable, so that\n        # we can construct the ChatDocument once the LLM specifies a recipient.\n        # This avoids having to re-generate the entire message, saving time + cost.\n        AddRecipientTool.saved_content = self.content\n        agent.enable_message(AddRecipientTool)\n        return ChatDocument(\n            content=\"\"\"\n            Empty recipient field!\n            Please use the 'add_recipient' tool/function-call to specify who your \n            message is intended for.\n            DO NOT REPEAT your original message; ONLY specify the recipient via this\n            tool/function-call.\n            \"\"\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                recipient=Entity.LLM,\n            ),\n        )\n\n    print(\"[red]RecipientTool: Validated properly addressed message\")\n\n    return ChatDocument(\n        content=self.content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as if msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.handle_message_fallback","title":"<code>handle_message_fallback(agent, msg)</code>  <code>staticmethod</code>","text":"<p>Response of agent if this tool is not used, e.g. the LLM simply sends a message without using this tool. This method has two purposes: (a) Alert the LLM that it has forgotten to specify a recipient, and prod it     to use the <code>add_recipient</code> tool to specify just the recipient     (and not re-generate the entire message). (b) Save the content of the message in the agent's <code>content</code> field,     so the agent can construct a ChatDocument with this content once LLM     later specifies a recipient using the <code>add_recipient</code> tool.</p> <p>This method is used to set the agent's handle_message_fallback() method.</p> <p>Returns:</p> Type Description <code>str</code> <p>reminder to LLM to use the <code>add_recipient</code> tool.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@staticmethod\ndef handle_message_fallback(\n    agent: ChatAgent, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Response of agent if this tool is not used, e.g.\n    the LLM simply sends a message without using this tool.\n    This method has two purposes:\n    (a) Alert the LLM that it has forgotten to specify a recipient, and prod it\n        to use the `add_recipient` tool to specify just the recipient\n        (and not re-generate the entire message).\n    (b) Save the content of the message in the agent's `content` field,\n        so the agent can construct a ChatDocument with this content once LLM\n        later specifies a recipient using the `add_recipient` tool.\n\n    This method is used to set the agent's handle_message_fallback() method.\n\n    Returns:\n        (str): reminder to LLM to use the `add_recipient` tool.\n    \"\"\"\n    # Note: once the LLM specifies a missing recipient, the task loop\n    # mechanism will not allow any of the \"native\" responders to respond,\n    # since the recipient will differ from the task name.\n    # So if this method is called, we can be sure that the recipient has not\n    # been specified.\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or msg.metadata.recipient != \"\"  # there IS an explicit recipient\n    ):\n        return None\n    content = msg if isinstance(msg, str) else msg.content\n    # save the content as a class-variable, so that\n    # we can construct the ChatDocument once the LLM specifies a recipient.\n    # This avoids having to re-generate the entire message, saving time + cost.\n    AddRecipientTool.saved_content = content\n    agent.enable_message(AddRecipientTool)\n    print(\"[red]RecipientTool: Recipient not specified, asking LLM to clarify.\")\n    return ChatDocument(\n        content=\"\"\"\n        Please use the 'add_recipient' tool/function-call to specify who your \n        `intended_recipient` is.\n        DO NOT REPEAT your original message; ONLY specify the \n        `intended_recipient` via this tool/function-call.\n        \"\"\",\n        metadata=ChatDocMetaData(\n            sender=Entity.AGENT,\n            recipient=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RewindTool","title":"<code>RewindTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to rewind (i.e. backtrack) to the <code>n</code>th Assistant message and replace with a new msg.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.RewindTool.response","title":"<code>response(agent)</code>","text":"<p>Define the tool-handler method for this tool here itself, since it is a generic tool whose functionality should be the same for any agent.</p> <p>When LLM has correctly used this tool, rewind this agent's <code>message_history</code> to the <code>n</code>th assistant msg, and replace it with <code>content</code>. We need to mock it as if the LLM is sending this message.</p> <p>Within a multi-agent scenario, this also means that any other messages dependent on this message will need to be invalidated -- so go down the chain of child messages and clear each agent's history back to the <code>msg_idx</code> corresponding to the child message.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content.</p> Source code in <code>langroid/agent/tools/rewind_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    Define the tool-handler method for this tool here itself,\n    since it is a generic tool whose functionality should be the\n    same for any agent.\n\n    When LLM has correctly used this tool, rewind this agent's\n    `message_history` to the `n`th assistant msg, and replace it with `content`.\n    We need to mock it as if the LLM is sending this message.\n\n    Within a multi-agent scenario, this also means that any other messages dependent\n    on this message will need to be invalidated --\n    so go down the chain of child messages and clear each agent's history\n    back to the `msg_idx` corresponding to the child message.\n\n    Returns:\n        (ChatDocument): with content set to self.content.\n    \"\"\"\n    idx = agent.nth_message_idx_with_role(lm.Role.ASSISTANT, self.n)\n    if idx &lt; 0:\n        # set up a corrective message from AGENT\n        msg = f\"\"\"\n            Could not rewind to {self.n}th Assistant message!\n            Please check the value of `n` and try again.\n            Or it may be too early to use the `rewind_tool`.\n            \"\"\"\n        return agent.create_agent_response(msg)\n\n    parent = prune_messages(agent, idx)\n\n    # create ChatDocument with new content, to be returned as result of this tool\n    result_doc = agent.create_llm_response(self.content)\n    result_doc.metadata.parent_id = \"\" if parent is None else parent.id()\n    result_doc.metadata.agent_id = agent.id\n    result_doc.metadata.msg_idx = idx\n\n    # replace the message at idx with this new message\n    agent.message_history.append(ChatDocument.to_LLMMessage(result_doc))\n\n    # set the replaced doc's parent's child to this result_doc\n    if parent is not None:\n        # first remove the this parent's child from registry\n        ChatDocument.delete_id(parent.metadata.child_id)\n        parent.metadata.child_id = result_doc.id()\n    return result_doc\n</code></pre>"},{"location":"reference/agent/tools/duckduckgo_search_tool/","title":"duckduckgo_search_tool","text":"<p>langroid/agent/tools/duckduckgo_search_tool.py </p> <p>A tool to trigger a DuckDuckGo search for a given query, and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(DuckduckgoSearchTool)</code></p>"},{"location":"reference/agent/tools/duckduckgo_search_tool/#langroid.agent.tools.duckduckgo_search_tool.DuckduckgoSearchTool","title":"<code>DuckduckgoSearchTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/duckduckgo_search_tool/#langroid.agent.tools.duckduckgo_search_tool.DuckduckgoSearchTool.handle","title":"<code>handle()</code>","text":"<p>Conducts a search using DuckDuckGo based on the provided query and number of results by triggering a duckduckgo_search.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the titles, links, and summaries of each search result, separated by two newlines.</p> Source code in <code>langroid/agent/tools/duckduckgo_search_tool.py</code> <pre><code>def handle(self) -&gt; str:\n    \"\"\"\n    Conducts a search using DuckDuckGo based on the provided query\n    and number of results by triggering a duckduckgo_search.\n\n    Returns:\n        str: A formatted string containing the titles, links, and\n            summaries of each search result, separated by two newlines.\n    \"\"\"\n    search_results = duckduckgo_search(self.query, self.num_results)\n    # return Title, Link, Summary of each result, separated by two newlines\n    results_str = \"\\n\\n\".join(str(result) for result in search_results)\n    return f\"\"\"\n    BELOW ARE THE RESULTS FROM THE WEB SEARCH. USE THESE TO COMPOSE YOUR RESPONSE:\n    {results_str}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/google_search_tool/","title":"google_search_tool","text":"<p>langroid/agent/tools/google_search_tool.py </p> <p>A tool to trigger a Google search for a given query, and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(GoogleSearchTool)</code></p> <p>NOTE: Using this tool requires setting the GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables in your <code>.env</code> file, as explained in the README.</p>"},{"location":"reference/agent/tools/metaphor_search_tool/","title":"metaphor_search_tool","text":"<p>langroid/agent/tools/metaphor_search_tool.py </p> <p>A tool to trigger a Metaphor search for a given query, (https://docs.exa.ai/reference/getting-started) and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(MetaphorSearchTool)</code></p> <p>NOTE: To use this tool, you need to:</p> <ul> <li> <p>set the METAPHOR_API_KEY environment variables in your <code>.env</code> file, e.g. <code>METAPHOR_API_KEY=your_api_key_here</code> (Note as of 28 Jan 2023, Metaphor renamed to Exa, so you can also use <code>EXA_API_KEY=your_api_key_here</code>)</p> </li> <li> <p>install langroid with the <code>metaphor</code> extra, e.g. <code>pip install langroid[metaphor]</code> or <code>poetry add langroid[metaphor]</code> (it installs the <code>metaphor-python</code> package from pypi).</p> </li> </ul> <p>For more information, please refer to the official docs: https://metaphor.systems/</p>"},{"location":"reference/agent/tools/metaphor_search_tool/#langroid.agent.tools.metaphor_search_tool.MetaphorSearchTool","title":"<code>MetaphorSearchTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/metaphor_search_tool/#langroid.agent.tools.metaphor_search_tool.MetaphorSearchTool.handle","title":"<code>handle()</code>","text":"<p>Conducts a search using the metaphor API based on the provided query and number of results by triggering a metaphor_search.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the titles, links, and summaries of each search result, separated by two newlines.</p> Source code in <code>langroid/agent/tools/metaphor_search_tool.py</code> <pre><code>def handle(self) -&gt; str:\n    \"\"\"\n    Conducts a search using the metaphor API based on the provided query\n    and number of results by triggering a metaphor_search.\n\n    Returns:\n        str: A formatted string containing the titles, links, and\n            summaries of each search result, separated by two newlines.\n    \"\"\"\n\n    search_results = metaphor_search(self.query, self.num_results)\n    # return Title, Link, Summary of each result, separated by two newlines\n    results_str = \"\\n\\n\".join(str(result) for result in search_results)\n    return f\"\"\"\n    BELOW ARE THE RESULTS FROM THE WEB SEARCH. USE THESE TO COMPOSE YOUR RESPONSE:\n    {results_str}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/","title":"recipient_tool","text":"<p>langroid/agent/tools/recipient_tool.py </p> <p>The <code>recipient_tool</code> is used to send a message to a specific recipient. Various methods from the RecipientTool and AddRecipientTool class are inserted into the Agent as methods (see <code>langroid/agent/base.py</code>, the method <code>_get_tool_list()</code>).</p> <p>See usage examples in <code>tests/main/test_multi_agent_complex.py</code> and <code>tests/main/test_recipient_tool.py</code>.</p>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.AddRecipientTool","title":"<code>AddRecipientTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to add a recipient to the previous message, when it has forgotten to specify a recipient. This avoids having to re-generate the previous message (and thus saves token-cost and time).</p>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.AddRecipientTool.response","title":"<code>response(agent)</code>","text":"<p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; ChatDocument:\n    \"\"\"\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.recipient.\n    \"\"\"\n    print(\n        \"[red]RecipientTool: \"\n        f\"Added recipient {self.intended_recipient} to message.\"\n    )\n    if self.__class__.saved_content == \"\":\n        recipient_request_name = RecipientTool.default_value(\"request\")\n        content = f\"\"\"\n            Recipient specified but content is empty!\n            This could be because the `{self.request}` tool/function was used \n            before using `{recipient_request_name}` tool/function.\n            Resend the message using `{recipient_request_name}` tool/function.\n            \"\"\"\n    else:\n        content = self.__class__.saved_content  # use class-level attrib value\n        # erase content since we just used it.\n        self.__class__.saved_content = \"\"\n    return ChatDocument(\n        content=content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as it msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool","title":"<code>RecipientTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to send a message to a specific recipient.</p> <p>Useful in cases where an LLM is talking to 2 or more agents (or an Agent and human user), and needs to specify which agent (task) its message is intended for. The recipient name should be the name of a task (which is normally the name of the agent that the task wraps, although the task can have its own name).</p> <p>To use this tool/function-call, LLM must generate a JSON structure with these fields: {     \"request\": \"recipient_message\", # also the function name when using fn-calling     \"intended_recipient\": ,     \"content\":  } The effect of this is that <code>content</code> will be sent to the <code>intended_recipient</code> task."},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.create","title":"<code>create(recipients, default='')</code>  <code>classmethod</code>","text":"<p>Create a restricted version of RecipientTool that only allows certain recipients, and possibly sets a default recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef create(cls, recipients: List[str], default: str = \"\") -&gt; Type[\"RecipientTool\"]:\n    \"\"\"Create a restricted version of RecipientTool that\n    only allows certain recipients, and possibly sets a default recipient.\"\"\"\n\n    class RecipientToolRestricted(cls):  # type: ignore\n        allowed_recipients = recipients\n        default_recipient = default\n\n    return RecipientToolRestricted\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Generate instructions for using this tool/function. These are intended to be appended to the system message of the LLM.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Generate instructions for using this tool/function.\n    These are intended to be appended to the system message of the LLM.\n    \"\"\"\n    recipients = []\n    if has_field(cls, \"allowed_recipients\"):\n        recipients = cls.default_value(\"allowed_recipients\")\n    if len(recipients) &gt; 0:\n        recipients_str = \", \".join(recipients)\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to one of the following:\n        {recipients_str},\n        and setting the 'content' field to your message.\n        \"\"\"\n    else:\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to the name of the recipient, \n        and setting the 'content' field to your message.\n        \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.response","title":"<code>response(agent)</code>","text":"<p>When LLM has correctly used this tool, construct a ChatDocument with an explicit recipient, and make it look like it is from the LLM.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.intended_recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    When LLM has correctly used this tool,\n    construct a ChatDocument with an explicit recipient,\n    and make it look like it is from the LLM.\n\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.intended_recipient.\n    \"\"\"\n    default_recipient = self.__class__.default_value(\"default_recipient\")\n    if self.intended_recipient == \"\" and default_recipient not in [\"\", None]:\n        self.intended_recipient = default_recipient\n    elif self.intended_recipient == \"\":\n        # save the content as a class-variable, so that\n        # we can construct the ChatDocument once the LLM specifies a recipient.\n        # This avoids having to re-generate the entire message, saving time + cost.\n        AddRecipientTool.saved_content = self.content\n        agent.enable_message(AddRecipientTool)\n        return ChatDocument(\n            content=\"\"\"\n            Empty recipient field!\n            Please use the 'add_recipient' tool/function-call to specify who your \n            message is intended for.\n            DO NOT REPEAT your original message; ONLY specify the recipient via this\n            tool/function-call.\n            \"\"\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                recipient=Entity.LLM,\n            ),\n        )\n\n    print(\"[red]RecipientTool: Validated properly addressed message\")\n\n    return ChatDocument(\n        content=self.content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as if msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.handle_message_fallback","title":"<code>handle_message_fallback(agent, msg)</code>  <code>staticmethod</code>","text":"<p>Response of agent if this tool is not used, e.g. the LLM simply sends a message without using this tool. This method has two purposes: (a) Alert the LLM that it has forgotten to specify a recipient, and prod it     to use the <code>add_recipient</code> tool to specify just the recipient     (and not re-generate the entire message). (b) Save the content of the message in the agent's <code>content</code> field,     so the agent can construct a ChatDocument with this content once LLM     later specifies a recipient using the <code>add_recipient</code> tool.</p> <p>This method is used to set the agent's handle_message_fallback() method.</p> <p>Returns:</p> Type Description <code>str</code> <p>reminder to LLM to use the <code>add_recipient</code> tool.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@staticmethod\ndef handle_message_fallback(\n    agent: ChatAgent, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Response of agent if this tool is not used, e.g.\n    the LLM simply sends a message without using this tool.\n    This method has two purposes:\n    (a) Alert the LLM that it has forgotten to specify a recipient, and prod it\n        to use the `add_recipient` tool to specify just the recipient\n        (and not re-generate the entire message).\n    (b) Save the content of the message in the agent's `content` field,\n        so the agent can construct a ChatDocument with this content once LLM\n        later specifies a recipient using the `add_recipient` tool.\n\n    This method is used to set the agent's handle_message_fallback() method.\n\n    Returns:\n        (str): reminder to LLM to use the `add_recipient` tool.\n    \"\"\"\n    # Note: once the LLM specifies a missing recipient, the task loop\n    # mechanism will not allow any of the \"native\" responders to respond,\n    # since the recipient will differ from the task name.\n    # So if this method is called, we can be sure that the recipient has not\n    # been specified.\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or msg.metadata.recipient != \"\"  # there IS an explicit recipient\n    ):\n        return None\n    content = msg if isinstance(msg, str) else msg.content\n    # save the content as a class-variable, so that\n    # we can construct the ChatDocument once the LLM specifies a recipient.\n    # This avoids having to re-generate the entire message, saving time + cost.\n    AddRecipientTool.saved_content = content\n    agent.enable_message(AddRecipientTool)\n    print(\"[red]RecipientTool: Recipient not specified, asking LLM to clarify.\")\n    return ChatDocument(\n        content=\"\"\"\n        Please use the 'add_recipient' tool/function-call to specify who your \n        `intended_recipient` is.\n        DO NOT REPEAT your original message; ONLY specify the \n        `intended_recipient` via this tool/function-call.\n        \"\"\",\n        metadata=ChatDocMetaData(\n            sender=Entity.AGENT,\n            recipient=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/retrieval_tool/","title":"retrieval_tool","text":"<p>langroid/agent/tools/retrieval_tool.py </p>"},{"location":"reference/agent/tools/retrieval_tool/#langroid.agent.tools.retrieval_tool.RetrievalTool","title":"<code>RetrievalTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Retrieval tool, only to be used by a DocChatAgent.</p>"},{"location":"reference/agent/tools/rewind_tool/","title":"rewind_tool","text":"<p>langroid/agent/tools/rewind_tool.py </p> <p>The <code>rewind_tool</code> is used to rewind to the <code>n</code>th previous Assistant message and replace it with a new <code>content</code>. This is useful in several scenarios and - saves token-cost + inference time, - reduces distracting clutter in chat history, which helps improve response quality.</p> <p>This is intended to mimic how a human user might use a chat interface, where they go down a conversation path, and want to go back in history to \"edit and re-submit\" a previous message, to get a better response.</p> <p>See usage examples in <code>tests/main/test_rewind_tool.py</code>.</p>"},{"location":"reference/agent/tools/rewind_tool/#langroid.agent.tools.rewind_tool.RewindTool","title":"<code>RewindTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to rewind (i.e. backtrack) to the <code>n</code>th Assistant message and replace with a new msg.</p>"},{"location":"reference/agent/tools/rewind_tool/#langroid.agent.tools.rewind_tool.RewindTool.response","title":"<code>response(agent)</code>","text":"<p>Define the tool-handler method for this tool here itself, since it is a generic tool whose functionality should be the same for any agent.</p> <p>When LLM has correctly used this tool, rewind this agent's <code>message_history</code> to the <code>n</code>th assistant msg, and replace it with <code>content</code>. We need to mock it as if the LLM is sending this message.</p> <p>Within a multi-agent scenario, this also means that any other messages dependent on this message will need to be invalidated -- so go down the chain of child messages and clear each agent's history back to the <code>msg_idx</code> corresponding to the child message.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content.</p> Source code in <code>langroid/agent/tools/rewind_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    Define the tool-handler method for this tool here itself,\n    since it is a generic tool whose functionality should be the\n    same for any agent.\n\n    When LLM has correctly used this tool, rewind this agent's\n    `message_history` to the `n`th assistant msg, and replace it with `content`.\n    We need to mock it as if the LLM is sending this message.\n\n    Within a multi-agent scenario, this also means that any other messages dependent\n    on this message will need to be invalidated --\n    so go down the chain of child messages and clear each agent's history\n    back to the `msg_idx` corresponding to the child message.\n\n    Returns:\n        (ChatDocument): with content set to self.content.\n    \"\"\"\n    idx = agent.nth_message_idx_with_role(lm.Role.ASSISTANT, self.n)\n    if idx &lt; 0:\n        # set up a corrective message from AGENT\n        msg = f\"\"\"\n            Could not rewind to {self.n}th Assistant message!\n            Please check the value of `n` and try again.\n            Or it may be too early to use the `rewind_tool`.\n            \"\"\"\n        return agent.create_agent_response(msg)\n\n    parent = prune_messages(agent, idx)\n\n    # create ChatDocument with new content, to be returned as result of this tool\n    result_doc = agent.create_llm_response(self.content)\n    result_doc.metadata.parent_id = \"\" if parent is None else parent.id()\n    result_doc.metadata.agent_id = agent.id\n    result_doc.metadata.msg_idx = idx\n\n    # replace the message at idx with this new message\n    agent.message_history.append(ChatDocument.to_LLMMessage(result_doc))\n\n    # set the replaced doc's parent's child to this result_doc\n    if parent is not None:\n        # first remove the this parent's child from registry\n        ChatDocument.delete_id(parent.metadata.child_id)\n        parent.metadata.child_id = result_doc.id()\n    return result_doc\n</code></pre>"},{"location":"reference/agent/tools/rewind_tool/#langroid.agent.tools.rewind_tool.prune_messages","title":"<code>prune_messages(agent, idx)</code>","text":"<p>Clear the message history of agent, starting at index <code>idx</code>, taking care to first clear all dependent messages (possibly from other agents' message histories) that are linked to the message at <code>idx</code>, via the <code>child_id</code> field of the <code>metadata</code> field of the ChatDocument linked from the message at <code>idx</code>.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>ChatAgent</code> <p>The agent whose message history is to be pruned.</p> required <code>idx</code> <code>int</code> <p>The index from which to start clearing the message history.</p> required <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>The parent ChatDocument of the ChatDocument linked from the message at <code>idx</code>,</p> <code>ChatDocument | None</code> <p>if it exists, else None.</p> Source code in <code>langroid/agent/tools/rewind_tool.py</code> <pre><code>def prune_messages(agent: ChatAgent, idx: int) -&gt; ChatDocument | None:\n    \"\"\"\n    Clear the message history of agent, starting at index `idx`,\n    taking care to first clear all dependent messages (possibly from other agents'\n    message histories) that are linked to the message at `idx`, via the `child_id` field\n    of the `metadata` field of the ChatDocument linked from the message at `idx`.\n\n    Args:\n        agent (ChatAgent): The agent whose message history is to be pruned.\n        idx (int): The index from which to start clearing the message history.\n\n    Returns:\n        The parent ChatDocument of the ChatDocument linked from the message at `idx`,\n        if it exists, else None.\n\n    \"\"\"\n    assert idx &gt;= 0, \"Invalid index for message history!\"\n    chat_doc_id = agent.message_history[idx].chat_document_id\n    chat_doc = ChatDocument.from_id(chat_doc_id)\n    assert chat_doc is not None, \"ChatDocument not found in registry!\"\n\n    parent = ChatDocument.from_id(chat_doc.metadata.parent_id)  # may be None\n    # We're invaliding the msg at idx,\n    # so starting with chat_doc, go down the child links\n    # and clear history of each agent, to the msg_idx\n    curr_doc = chat_doc\n    while child_doc := curr_doc.metadata.child:\n        if child_doc.metadata.msg_idx &gt;= 0:\n            child_agent = ChatAgent.from_id(child_doc.metadata.agent_id)\n            if child_agent is not None:\n                child_agent.clear_history(child_doc.metadata.msg_idx)\n        curr_doc = child_doc\n\n    # Clear out ObjectRegistry entries for this ChatDocuments\n    # and all descendants (in case they weren't already cleared above)\n    ChatDocument.delete_id(chat_doc.id())\n\n    # Finally, clear this agent's history back to idx,\n    # and replace the msg at idx with the new content\n    agent.clear_history(idx)\n    return parent\n</code></pre>"},{"location":"reference/agent/tools/segment_extract_tool/","title":"segment_extract_tool","text":"<p>langroid/agent/tools/segment_extract_tool.py </p> <p>A tool to extract segment numbers from the last user message, containing numbered segments.</p> <p>The idea is that when an LLM wants to (or is asked to) simply extract portions of a message verbatim, it should use this tool/function to SPECIFY what should be extracted, rather than actually extracting it. The output will be in the form of a list of segment numbers or ranges. This will usually be much cheaper and faster than actually writing out the extracted text. The handler of this tool/function will then extract the text and send it back.</p>"},{"location":"reference/cachedb/","title":"cachedb","text":"<p>langroid/cachedb/init.py </p>"},{"location":"reference/cachedb/base/","title":"base","text":"<p>langroid/cachedb/base.py </p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDBConfig","title":"<code>CacheDBConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Configuration model for CacheDB.</p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB","title":"<code>CacheDB</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a cache database.</p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.store","title":"<code>store(key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Abstract method to store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.retrieve","title":"<code>retrieve(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any] | str | None</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, key: str) -&gt; Dict[str, Any] | str | None:\n    \"\"\"\n    Abstract method to retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.delete_keys","title":"<code>delete_keys(keys)</code>  <code>abstractmethod</code>","text":"<p>Delete the keys from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>The keys to delete.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef delete_keys(self, keys: List[str]) -&gt; None:\n    \"\"\"\n    Delete the keys from the cache.\n\n    Args:\n        keys (List[str]): The keys to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.delete_keys_pattern","title":"<code>delete_keys_pattern(pattern)</code>  <code>abstractmethod</code>","text":"<p>Delete all keys with the given pattern</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The pattern to match.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef delete_keys_pattern(self, pattern: str) -&gt; None:\n    \"\"\"\n    Delete all keys with the given pattern\n\n    Args:\n        prefix (str): The pattern to match.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/","title":"momento_cachedb","text":"<p>langroid/cachedb/momento_cachedb.py </p>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCacheConfig","title":"<code>MomentoCacheConfig</code>","text":"<p>             Bases: <code>CacheDBConfig</code></p> <p>Configuration model for Momento Cache.</p>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache","title":"<code>MomentoCache(config)</code>","text":"<p>             Bases: <code>CacheDB</code></p> <p>Momento implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MomentoCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def __init__(self, config: MomentoCacheConfig):\n    \"\"\"\n    Initialize a MomentoCache with the given config.\n\n    Args:\n        config (MomentoCacheConfig): The configuration to use.\n    \"\"\"\n    self.config = config\n    load_dotenv()\n\n    momento_token = os.getenv(\"MOMENTO_AUTH_TOKEN\")\n    if momento_token is None:\n        raise ValueError(\"\"\"MOMENTO_AUTH_TOKEN not set in .env file\"\"\")\n    else:\n        self.client = momento.CacheClient(\n            configuration=momento.Configurations.Laptop.v1(),\n            credential_provider=momento.CredentialProvider.from_environment_variable(\n                \"MOMENTO_AUTH_TOKEN\"\n            ),\n            default_ttl=timedelta(seconds=self.config.ttl),\n        )\n        self.client.create_cache(self.config.cachename)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear keys from current db.\"\"\"\n    self.client.flush_cache(self.config.cachename)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    self.client.set(self.config.cachename, key, json.dumps(value))\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any] | str | None</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Dict[str, Any] | str | None:\n    \"\"\"\n    Retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\n    value = self.client.get(self.config.cachename, key)\n    if isinstance(value, CacheGet.Hit):\n        return json.loads(value.value_string)  # type: ignore\n    else:\n        return None\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.delete_keys","title":"<code>delete_keys(keys)</code>","text":"<p>Delete the keys from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>The keys to delete.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def delete_keys(self, keys: List[str]) -&gt; None:\n    \"\"\"\n    Delete the keys from the cache.\n\n    Args:\n        keys (List[str]): The keys to delete.\n    \"\"\"\n    for key in keys:\n        self.client.delete(self.config.cachename, key)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.delete_keys_pattern","title":"<code>delete_keys_pattern(pattern)</code>","text":"<p>Delete the keys from the cache with the given pattern.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The pattern to match.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def delete_keys_pattern(self, pattern: str) -&gt; None:\n    \"\"\"\n    Delete the keys from the cache with the given pattern.\n\n    Args:\n        prefix (str): The pattern to match.\n    \"\"\"\n    raise NotImplementedError(\n        \"\"\"\n        MomentoCache does not support delete_keys_pattern.\n        Please use RedisCache instead.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/","title":"redis_cachedb","text":"<p>langroid/cachedb/redis_cachedb.py </p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>             Bases: <code>CacheDBConfig</code></p> <p>Configuration model for RedisCache.</p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache","title":"<code>RedisCache(config)</code>","text":"<p>             Bases: <code>CacheDB</code></p> <p>Redis implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def __init__(self, config: RedisCacheConfig):\n    \"\"\"\n    Initialize a RedisCache with the given config.\n\n    Args:\n        config (RedisCacheConfig): The configuration to use.\n    \"\"\"\n    self.config = config\n    load_dotenv()\n\n    if self.config.fake:\n        self.pool = fakeredis.FakeStrictRedis()  # type: ignore\n    else:\n        redis_password = os.getenv(\"REDIS_PASSWORD\")\n        redis_host = os.getenv(\"REDIS_HOST\")\n        redis_port = os.getenv(\"REDIS_PORT\")\n        if None in [redis_password, redis_host, redis_port]:\n            if not RedisCache._warned_password:\n                logger.warning(\n                    \"\"\"REDIS_PASSWORD, REDIS_HOST, REDIS_PORT not set in .env file,\n                    using fake redis client\"\"\"\n                )\n                RedisCache._warned_password = True\n            self.pool = fakeredis.FakeStrictRedis()  # type: ignore\n        else:\n            self.pool = redis.ConnectionPool(  # type: ignore\n                host=redis_host,\n                port=redis_port,\n                password=redis_password,\n                max_connections=500,\n                socket_timeout=5,\n                socket_keepalive=True,\n                retry_on_timeout=True,\n                health_check_interval=30,\n            )\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.redis_client","title":"<code>redis_client()</code>","text":"<p>Cleanly open and close a redis client, avoids max clients exceeded error</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>@contextmanager  # type: ignore\ndef redis_client(self) -&gt; AbstractContextManager[T]:  # type: ignore\n    \"\"\"Cleanly open and close a redis client, avoids max clients exceeded error\"\"\"\n    if isinstance(self.pool, fakeredis.FakeStrictRedis):\n        yield self.pool\n    else:\n        client: T = redis.Redis(connection_pool=self.pool)\n        try:\n            yield client\n        finally:\n            client.close()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear keys from current db.\"\"\"\n    with self.redis_client() as client:  # type: ignore\n        client.flushdb()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear_all","title":"<code>clear_all()</code>","text":"<p>Clear all keys from all dbs.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear_all(self) -&gt; None:\n    \"\"\"Clear all keys from all dbs.\"\"\"\n    with self.redis_client() as client:  # type: ignore\n        client.flushall()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            client.set(key, json.dumps(value))\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, not storing key/value\")\n            return None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] | str | None</code> <p>dict|str|None: The value associated with the key.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Dict[str, Any] | str | None:\n    \"\"\"\n    Retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict|str|None: The value associated with the key.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            value = client.get(key)\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, returning None\")\n            return None\n        return json.loads(value) if value else None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.delete_keys","title":"<code>delete_keys(keys)</code>","text":"<p>Delete the keys from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>The keys to delete.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def delete_keys(self, keys: List[str]) -&gt; None:\n    \"\"\"\n    Delete the keys from the cache.\n\n    Args:\n        keys (List[str]): The keys to delete.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            client.delete(*keys)\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, not deleting keys\")\n            return None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.delete_keys_pattern","title":"<code>delete_keys_pattern(pattern)</code>","text":"<p>Delete the keys matching the pattern from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The pattern to match.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def delete_keys_pattern(self, pattern: str) -&gt; None:\n    \"\"\"\n    Delete the keys matching the pattern from the cache.\n\n    Args:\n        prefix (str): The pattern to match.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            keys = client.keys(pattern)\n            if len(keys) &gt; 0:\n                client.delete(*keys)\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, not deleting keys\")\n            return None\n</code></pre>"},{"location":"reference/embedding_models/","title":"embedding_models","text":"<p>langroid/embedding_models/init.py </p>"},{"location":"reference/embedding_models/#langroid.embedding_models.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for an embedding model.</p>"},{"location":"reference/embedding_models/#langroid.embedding_models.EmbeddingModel.similarity","title":"<code>similarity(text1, text2)</code>","text":"<p>Compute cosine similarity between two texts.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"Compute cosine similarity between two texts.\"\"\"\n    [emb1, emb2] = self.embedding_fn()([text1, text2])\n    return float(\n        np.array(emb1)\n        @ np.array(emb2)\n        / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n    )\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.OpenAIEmbeddings","title":"<code>OpenAIEmbeddings(config=OpenAIEmbeddingsConfig())</code>","text":"<p>             Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: OpenAIEmbeddingsConfig = OpenAIEmbeddingsConfig()):\n    super().__init__()\n    self.config = config\n    load_dotenv()\n    self.config.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    self.config.organization = os.getenv(\"OPENAI_ORGANIZATION\", \"\")\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"OPENAI_API_KEY env variable must be set to use \n            OpenAIEmbeddings. Please set the OPENAI_API_KEY value \n            in your .env file.\n            \"\"\"\n        )\n    self.client = OpenAI(base_url=self.config.api_base, api_key=self.config.api_key)\n    self.tokenizer = tiktoken.encoding_for_model(self.config.model_name)\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.OpenAIEmbeddings.truncate_texts","title":"<code>truncate_texts(texts)</code>","text":"<p>Truncate texts to the embedding model's context length. TODO: Maybe we should show warning, and consider doing T5 summarization?</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def truncate_texts(self, texts: List[str]) -&gt; List[List[int]]:\n    \"\"\"\n    Truncate texts to the embedding model's context length.\n    TODO: Maybe we should show warning, and consider doing T5 summarization?\n    \"\"\"\n    return [\n        self.tokenizer.encode(text, disallowed_special=())[\n            : self.config.context_length\n        ]\n        for text in texts\n    ]\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>\"openai\" or \"sentencetransformer\" # others soon</p> <code>'openai'</code> <p>Returns:     EmbeddingModel</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n    \"\"\"\n    Args:\n        embedding_fn_type: \"openai\" or \"sentencetransformer\" # others soon\n    Returns:\n        EmbeddingModel\n    \"\"\"\n    if embedding_fn_type == \"openai\":\n        return OpenAIEmbeddings  # type: ignore\n    else:  # default sentence transformer\n        return SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/embedding_models/base/","title":"base","text":"<p>langroid/embedding_models/base.py </p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for an embedding model.</p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel.similarity","title":"<code>similarity(text1, text2)</code>","text":"<p>Compute cosine similarity between two texts.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"Compute cosine similarity between two texts.\"\"\"\n    [emb1, emb2] = self.embedding_fn()([text1, text2])\n    return float(\n        np.array(emb1)\n        @ np.array(emb2)\n        / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n    )\n</code></pre>"},{"location":"reference/embedding_models/models/","title":"models","text":"<p>langroid/embedding_models/models.py </p>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.EmbeddingFunctionCallable","title":"<code>EmbeddingFunctionCallable(model, batch_size=512)</code>","text":"<p>A callable class designed to generate embeddings for a list of texts using the OpenAI API, with automatic retries on failure.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>OpenAIEmbeddings</code> <p>An instance of OpenAIEmbeddings that provides                     configuration and utilities for generating embeddings.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>List[str]) -&gt; Embeddings: Generate embeddings for                     a list of input texts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OpenAIEmbeddings</code> <p>An instance of OpenAIEmbeddings to use for</p> required <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>512</code> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, model: \"OpenAIEmbeddings\", batch_size: int = 512):\n    \"\"\"\n    Initialize the EmbeddingFunctionCallable with a specific model.\n\n    Args:\n        model (OpenAIEmbeddings): An instance of OpenAIEmbeddings to use for\n        generating embeddings.\n        batch_size (int): Batch size\n    \"\"\"\n    self.model = model\n    self.batch_size = batch_size\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.OpenAIEmbeddings","title":"<code>OpenAIEmbeddings(config=OpenAIEmbeddingsConfig())</code>","text":"<p>             Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: OpenAIEmbeddingsConfig = OpenAIEmbeddingsConfig()):\n    super().__init__()\n    self.config = config\n    load_dotenv()\n    self.config.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    self.config.organization = os.getenv(\"OPENAI_ORGANIZATION\", \"\")\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"OPENAI_API_KEY env variable must be set to use \n            OpenAIEmbeddings. Please set the OPENAI_API_KEY value \n            in your .env file.\n            \"\"\"\n        )\n    self.client = OpenAI(base_url=self.config.api_base, api_key=self.config.api_key)\n    self.tokenizer = tiktoken.encoding_for_model(self.config.model_name)\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.OpenAIEmbeddings.truncate_texts","title":"<code>truncate_texts(texts)</code>","text":"<p>Truncate texts to the embedding model's context length. TODO: Maybe we should show warning, and consider doing T5 summarization?</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def truncate_texts(self, texts: List[str]) -&gt; List[List[int]]:\n    \"\"\"\n    Truncate texts to the embedding model's context length.\n    TODO: Maybe we should show warning, and consider doing T5 summarization?\n    \"\"\"\n    return [\n        self.tokenizer.encode(text, disallowed_special=())[\n            : self.config.context_length\n        ]\n        for text in texts\n    ]\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>\"openai\" or \"sentencetransformer\" # others soon</p> <code>'openai'</code> <p>Returns:     EmbeddingModel</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n    \"\"\"\n    Args:\n        embedding_fn_type: \"openai\" or \"sentencetransformer\" # others soon\n    Returns:\n        EmbeddingModel\n    \"\"\"\n    if embedding_fn_type == \"openai\":\n        return OpenAIEmbeddings  # type: ignore\n    else:  # default sentence transformer\n        return SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/embedding_models/remote_embeds/","title":"remote_embeds","text":"<p>langroid/embedding_models/remote_embeds.py </p> <p>If run as a script, starts an RPC server which handles remote embedding requests:</p> <p>For example: python3 -m langroid.embedding_models.remote_embeds --port <code>port</code></p> <p>where <code>port</code> is the port at which the service is exposed.  Currently, supports insecure connections only, and this should NOT be exposed to the internet.</p>"},{"location":"reference/embedding_models/remote_embeds/#langroid.embedding_models.remote_embeds.serve","title":"<code>serve(bind_address_base='localhost', port=50052, batch_size=512, data_parallel=False, device=None, devices=None, model_name='BAAI/bge-large-en-v1.5')</code>  <code>async</code>","text":"<p>Starts the RPC server.</p> Source code in <code>langroid/embedding_models/remote_embeds.py</code> <pre><code>async def serve(\n    bind_address_base: str = \"localhost\",\n    port: int = 50052,\n    batch_size: int = 512,\n    data_parallel: bool = False,\n    device: Optional[str] = None,\n    devices: Optional[list[str]] = None,\n    model_name: str = \"BAAI/bge-large-en-v1.5\",\n) -&gt; None:\n    \"\"\"Starts the RPC server.\"\"\"\n    server = grpc.aio.server()\n    embeddings_grpc.add_EmbeddingServicer_to_server(\n        RemoteEmbeddingRPCs(\n            model_name=model_name,\n            batch_size=batch_size,\n            data_parallel=data_parallel,\n            device=device,\n            devices=devices,\n        ),\n        server,\n    )  # type: ignore\n    url = f\"{bind_address_base}:{port}\"\n    server.add_insecure_port(url)\n    await server.start()\n    print(f\"Embedding server started, listening on {url}\")\n    await server.wait_for_termination()\n</code></pre>"},{"location":"reference/embedding_models/protoc/","title":"protoc","text":"<p>langroid/embedding_models/protoc/init.py </p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2/","title":"embeddings_pb2","text":"<p>langroid/embedding_models/protoc/embeddings_pb2.py </p> <p>Generated protocol buffer code.</p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/","title":"embeddings_pb2_grpc","text":"<p>langroid/embedding_models/protoc/embeddings_pb2_grpc.py </p> <p>Client and server classes corresponding to protobuf-defined services.</p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.EmbeddingStub","title":"<code>EmbeddingStub(channel)</code>","text":"<p>             Bases: <code>object</code></p> <p>Missing associated documentation comment in .proto file.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <p>A grpc.Channel.</p> required Source code in <code>langroid/embedding_models/protoc/embeddings_pb2_grpc.py</code> <pre><code>def __init__(self, channel):\n    \"\"\"Constructor.\n\n    Args:\n        channel: A grpc.Channel.\n    \"\"\"\n    self.Embed = channel.unary_unary(\n        \"/Embedding/Embed\",\n        request_serializer=embeddings__pb2.EmbeddingRequest.SerializeToString,\n        response_deserializer=embeddings__pb2.BatchEmbeds.FromString,\n    )\n</code></pre>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.EmbeddingServicer","title":"<code>EmbeddingServicer</code>","text":"<p>             Bases: <code>object</code></p> <p>Missing associated documentation comment in .proto file.</p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.EmbeddingServicer.Embed","title":"<code>Embed(request, context)</code>","text":"<p>Missing associated documentation comment in .proto file.</p> Source code in <code>langroid/embedding_models/protoc/embeddings_pb2_grpc.py</code> <pre><code>def Embed(self, request, context):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\"Method not implemented!\")\n    raise NotImplementedError(\"Method not implemented!\")\n</code></pre>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.Embedding","title":"<code>Embedding</code>","text":"<p>             Bases: <code>object</code></p> <p>Missing associated documentation comment in .proto file.</p>"},{"location":"reference/language_models/","title":"language_models","text":"<p>langroid/language_models/init.py </p>"},{"location":"reference/language_models/#langroid.language_models.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing message sent to, or received from, LLM.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMMessage.api_dict","title":"<code>api_dict()</code>","text":"<p>Convert to dictionary for API request, keeping ONLY the fields that are expected in an API call! E.g., DROP the tool_id, since it is only for use in the Assistant API,     not the completion API. Returns:     dict: dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for API request, keeping ONLY\n    the fields that are expected in an API call!\n    E.g., DROP the tool_id, since it is only for use in the Assistant API,\n        not the completion API.\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\n    d = self.dict()\n    # drop None values since API doesn't accept them\n    dict_no_none = {k: v for k, v in d.items() if v is not None}\n    if \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n        # OpenAI API does not like empty name\n        del dict_no_none[\"name\"]\n    if \"function_call\" in dict_no_none:\n        # arguments must be a string\n        if \"arguments\" in dict_no_none[\"function_call\"]:\n            dict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\n                dict_no_none[\"function_call\"][\"arguments\"]\n            )\n    # IMPORTANT! drop fields that are not expected in API call\n    dict_no_none.pop(\"tool_id\", None)\n    dict_no_none.pop(\"timestamp\", None)\n    dict_no_none.pop(\"chat_document_id\", None)\n    return dict_no_none\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicate it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMFunctionCall.from_dict","title":"<code>from_dict(message)</code>  <code>staticmethod</code>","text":"<p>Initialize from dictionary. Args:     d: dictionary containing fields to initialize</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef from_dict(message: Dict[str, Any]) -&gt; \"LLMFunctionCall\":\n    \"\"\"\n    Initialize from dictionary.\n    Args:\n        d: dictionary containing fields to initialize\n    \"\"\"\n    fun_call = LLMFunctionCall(name=message[\"name\"])\n    fun_args_str = message[\"arguments\"]\n    # sometimes may be malformed with invalid indents,\n    # so we try to be safe by removing newlines.\n    if fun_args_str is not None:\n        fun_args_str = fun_args_str.replace(\"\\n\", \"\").strip()\n        fun_args = ast.literal_eval(fun_args_str)\n    else:\n        fun_args = None\n    fun_call.arguments = fun_args\n\n    return fun_call\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing response from LLM.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMResponse.get_recipient_and_message","title":"<code>get_recipient_and_message()</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains \"TO:  \", or (b) <code>message</code> is empty and <code>function_call</code> with <code>to: &lt;name&gt;</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_recipient_and_message(\n    self,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n\n    Two cases:\n    (a) `message` contains \"TO: &lt;name&gt; &lt;content&gt;\", or\n    (b) `message` is empty and `function_call` with `to: &lt;name&gt;`\n\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n\n    \"\"\"\n\n    if self.function_call is not None:\n        # in this case we ignore message, since all information is in function_call\n        msg = \"\"\n        args = self.function_call.arguments\n        if isinstance(args, dict):\n            recipient = args.get(\"recipient\", \"\")\n        return recipient, msg\n    else:\n        msg = self.message\n\n    # It's not a function call, so continue looking to see\n    # if a recipient is specified in the message.\n\n    # First check if message contains \"TO: &lt;recipient&gt; &lt;content&gt;\"\n    recipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\n    # check if there is a top level json that specifies 'recipient',\n    # and retain the entire message as content.\n    if recipient_name == \"\":\n        recipient_name = top_level_json_field(msg, \"recipient\") if msg else \"\"\n        content = msg\n    return recipient_name, content\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Chat models</p>"},{"location":"reference/language_models/#langroid.language_models.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig","title":"<code>OpenAIGPTConfig(**kwargs)</code>","text":"<p>             Bases: <code>LLMConfig</code></p> <p>Class for any LLM with an OpenAI-like API: besides the OpenAI models this includes: (a) locally-served models behind an OpenAI-compatible API (b) non-local models, using a proxy adaptor lib like litellm that provides     an OpenAI-compatible API. We could rename this class to OpenAILikeConfig.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    local_model = \"api_base\" in kwargs and kwargs[\"api_base\"] is not None\n\n    chat_model = kwargs.get(\"chat_model\", \"\")\n    local_prefixes = [\"local/\", \"litellm/\", \"ollama/\"]\n    if any(chat_model.startswith(prefix) for prefix in local_prefixes):\n        local_model = True\n\n    warn_gpt_3_5 = (\n        \"chat_model\" not in kwargs.keys()\n        and not local_model\n        and defaultOpenAIChatModel == OpenAIChatModel.GPT3_5_TURBO\n    )\n\n    if warn_gpt_3_5:\n        existing_hook = kwargs.get(\"run_on_first_use\", noop)\n\n        def with_warning() -&gt; None:\n            existing_hook()\n            gpt_3_5_warning()\n\n        kwargs[\"run_on_first_use\"] = with_warning\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig.create","title":"<code>create(prefix)</code>  <code>classmethod</code>","text":"<p>Create a config class whose params can be set via a desired prefix from the .env file or env vars. E.g., using <pre><code>OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\nollama_config = OllamaConfig()\n</code></pre> you can have a group of params prefixed by \"OLLAMA_\", to be used with models served via <code>ollama</code>. This way, you can maintain several setting-groups in your .env file, one per model type.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@classmethod\ndef create(cls, prefix: str) -&gt; Type[\"OpenAIGPTConfig\"]:\n    \"\"\"Create a config class whose params can be set via a desired\n    prefix from the .env file or env vars.\n    E.g., using\n    ```python\n    OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\n    ollama_config = OllamaConfig()\n    ```\n    you can have a group of params prefixed by \"OLLAMA_\", to be used\n    with models served via `ollama`.\n    This way, you can maintain several setting-groups in your .env file,\n    one per model type.\n    \"\"\"\n\n    class DynamicConfig(OpenAIGPTConfig):\n        pass\n\n    DynamicConfig.Config.env_prefix = prefix.upper() + \"_\"\n\n    return DynamicConfig\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT","title":"<code>OpenAIGPT(config=OpenAIGPTConfig())</code>","text":"<p>             Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig = OpenAIGPTConfig()):\n    \"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\n    # copy the config to avoid modifying the original\n    config = config.copy()\n    super().__init__(config)\n    self.config: OpenAIGPTConfig = config\n\n    # Run the first time the model is used\n    self.run_on_first_use = cache(self.config.run_on_first_use)\n\n    # global override of chat_model,\n    # to allow quick testing with other models\n    if settings.chat_model != \"\":\n        self.config.chat_model = settings.chat_model\n        self.config.completion_model = settings.chat_model\n\n    if len(parts := self.config.chat_model.split(\"//\")) &gt; 1:\n        # there is a formatter specified, e.g.\n        # \"litellm/ollama/mistral//hf\" or\n        # \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n        formatter = parts[1]\n        self.config.chat_model = parts[0]\n        if formatter == \"hf\":\n            # e.g. \"litellm/ollama/mistral//hf\" -&gt; \"litellm/ollama/mistral\"\n            formatter = find_hf_formatter(self.config.chat_model)\n            if formatter != \"\":\n                # e.g. \"mistral\"\n                self.config.formatter = formatter\n                logging.warning(\n                    f\"\"\"\n                    Using completions (not chat) endpoint with HuggingFace \n                    chat_template for {formatter} for \n                    model {self.config.chat_model}\n                    \"\"\"\n                )\n        else:\n            # e.g. \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n            self.config.formatter = formatter\n\n    if self.config.formatter is not None:\n        self.config.hf_formatter = HFFormatter(\n            HFPromptFormatterConfig(model_name=self.config.formatter)\n        )\n\n    # if model name starts with \"litellm\",\n    # set the actual model name by stripping the \"litellm/\" prefix\n    # and set the litellm flag to True\n    if self.config.chat_model.startswith(\"litellm/\") or self.config.litellm:\n        # e.g. litellm/ollama/mistral\n        self.config.litellm = True\n        self.api_base = self.config.api_base\n        if self.config.chat_model.startswith(\"litellm/\"):\n            # strip the \"litellm/\" prefix\n            # e.g. litellm/ollama/llama2 =&gt; ollama/llama2\n            self.config.chat_model = self.config.chat_model.split(\"/\", 1)[1]\n    elif self.config.chat_model.startswith(\"local/\"):\n        # expect this to be of the form \"local/localhost:8000/v1\",\n        # depending on how the model is launched locally.\n        # In this case the model served locally behind an OpenAI-compatible API\n        # so we can just use `openai.*` methods directly,\n        # and don't need a adaptor library like litellm\n        self.config.litellm = False\n        self.config.seed = None  # some models raise an error when seed is set\n        # Extract the api_base from the model name after the \"local/\" prefix\n        self.api_base = self.config.chat_model.split(\"/\", 1)[1]\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n    elif self.config.chat_model.startswith(\"ollama/\"):\n        self.config.ollama = True\n\n        # use api_base from config if set, else fall back on OLLAMA_BASE_URL\n        self.api_base = self.config.api_base or OLLAMA_BASE_URL\n        self.api_key = OLLAMA_API_KEY\n        self.config.chat_model = self.config.chat_model.replace(\"ollama/\", \"\")\n    else:\n        self.api_base = self.config.api_base\n\n    if settings.chat_model != \"\":\n        # if we're overriding chat model globally, set completion model to same\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.formatter is not None:\n        # we want to format chats -&gt; completions using this specific formatter\n        self.config.use_completion_for_chat = True\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.use_completion_for_chat:\n        self.config.use_chat_for_completion = False\n\n    # NOTE: The api_key should be set in the .env file, or via\n    # an explicit `export OPENAI_API_KEY=xxx` or `setenv OPENAI_API_KEY xxx`\n    # Pydantic's BaseSettings will automatically pick it up from the\n    # .env file\n    # The config.api_key is ignored when not using an OpenAI model\n    if self.is_openai_completion_model() or self.is_openai_chat_model():\n        self.api_key = config.api_key\n        if self.api_key == DUMMY_API_KEY:\n            self.api_key = os.getenv(\"OPENAI_API_KEY\", DUMMY_API_KEY)\n    else:\n        self.api_key = DUMMY_API_KEY\n\n    self.is_groq = self.config.chat_model.startswith(\"groq/\")\n\n    if self.is_groq:\n        self.config.chat_model = self.config.chat_model.replace(\"groq/\", \"\")\n        self.api_key = os.getenv(\"GROQ_API_KEY\", DUMMY_API_KEY)\n        self.client = Groq(\n            api_key=self.api_key,\n        )\n        self.async_client = AsyncGroq(\n            api_key=self.api_key,\n        )\n    else:\n        self.client = OpenAI(\n            api_key=self.api_key,\n            base_url=self.api_base,\n            organization=self.config.organization,\n            timeout=Timeout(self.config.timeout),\n        )\n        self.async_client = AsyncOpenAI(\n            api_key=self.api_key,\n            organization=self.config.organization,\n            base_url=self.api_base,\n            timeout=Timeout(self.config.timeout),\n        )\n\n    self.cache: CacheDB\n    if settings.cache_type == \"momento\":\n        from langroid.cachedb.momento_cachedb import (\n            MomentoCache,\n            MomentoCacheConfig,\n        )\n\n        if config.cache_config is None or not isinstance(\n            config.cache_config,\n            MomentoCacheConfig,\n        ):\n            # switch to fresh momento config if needed\n            config.cache_config = MomentoCacheConfig()\n        self.cache = MomentoCache(config.cache_config)\n    elif \"redis\" in settings.cache_type:\n        if config.cache_config is None or not isinstance(\n            config.cache_config,\n            RedisCacheConfig,\n        ):\n            # switch to fresh redis config if needed\n            config.cache_config = RedisCacheConfig(\n                fake=\"fake\" in settings.cache_type\n            )\n        if \"fake\" in settings.cache_type:\n            # force use of fake redis if global cache_type is \"fakeredis\"\n            config.cache_config.fake = True\n        self.cache = RedisCache(config.cache_config)\n    else:\n        raise ValueError(\n            f\"Invalid cache type {settings.cache_type}. \"\n            \"Valid types are momento, redis, fakeredis\"\n        )\n\n    self.config._validate_litellm()\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.chat_context_length","title":"<code>chat_context_length()</code>","text":"<p>Context-length for chat-completion models/endpoints Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for chat-completion models/endpoints\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    model = (\n        self.config.completion_model\n        if self.config.use_completion_for_chat\n        else self.config.chat_model\n    )\n    return _context_length.get(model, super().chat_context_length())\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.completion_context_length","title":"<code>completion_context_length()</code>","text":"<p>Context-length for completion models/endpoints Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def completion_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for completion models/endpoints\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    model = (\n        self.config.chat_model\n        if self.config.use_chat_for_completion\n        else self.config.completion_model\n    )\n    return _context_length.get(model, super().completion_context_length())\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.chat_cost","title":"<code>chat_cost()</code>","text":"<p>(Prompt, Generation) cost per 1000 tokens, for chat-completion models/endpoints. Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_cost(self) -&gt; Tuple[float, float]:\n    \"\"\"\n    (Prompt, Generation) cost per 1000 tokens, for chat-completion\n    models/endpoints.\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    return _cost_per_1k_tokens.get(self.config.chat_model, super().chat_cost())\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API. Args:     stream: enable streaming output from API Returns: previous value of stream</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\n    tmp = self.config.stream\n    self.config.stream = stream\n    return tmp\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status\"\"\"\n    return self.config.stream and settings.stream\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM","title":"<code>MockLM(config=MockLMConfig())</code>","text":"<p>             Bases: <code>LanguageModel</code></p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def __init__(self, config: MockLMConfig = MockLMConfig()):\n    super().__init__(config)\n    self.config: MockLMConfig = config\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.chat","title":"<code>chat(messages, max_tokens=200, functions=None, function_call='auto')</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def chat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return self._response(last_msg)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.achat","title":"<code>achat(messages, max_tokens=200, functions=None, function_call='auto')</code>  <code>async</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def achat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return self._response(last_msg)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.generate","title":"<code>generate(prompt, max_tokens=200)</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def generate(self, prompt: str, max_tokens: int = 200) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return self._response(prompt)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.agenerate","title":"<code>agenerate(prompt, max_tokens=200)</code>  <code>async</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def agenerate(self, prompt: str, max_tokens: int = 200) -&gt; LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return self._response(prompt)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLMConfig","title":"<code>MockLMConfig</code>","text":"<p>             Bases: <code>LLMConfig</code></p> <p>Mock Language Model Configuration.</p> <p>Attributes:</p> Name Type Description <code>response_dict</code> <code>Dict[str, str]</code> <p>A \"response rule-book\", in the form of a dictionary; if last msg in dialog is x,then respond with response_dict[x]</p>"},{"location":"reference/language_models/#langroid.language_models.AzureConfig","title":"<code>AzureConfig(**kwargs)</code>","text":"<p>             Bases: <code>OpenAIGPTConfig</code></p> <p>Configuration for Azure OpenAI GPT.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>should be <code>azure.</code></p> <code>api_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_API_VERSION.</code></p> <code>deployment_name</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_DEPLOYMENT_NAME</code> and should be based the custom name you chose for your deployment when you deployed a model.</p> <code>model_name</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_GPT_MODEL_NAME</code> and should be based on the model name chosen during setup.</p> <code>model_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_MODEL_VERSION</code> and should be based on the model name chosen during setup.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    local_model = \"api_base\" in kwargs and kwargs[\"api_base\"] is not None\n\n    chat_model = kwargs.get(\"chat_model\", \"\")\n    local_prefixes = [\"local/\", \"litellm/\", \"ollama/\"]\n    if any(chat_model.startswith(prefix) for prefix in local_prefixes):\n        local_model = True\n\n    warn_gpt_3_5 = (\n        \"chat_model\" not in kwargs.keys()\n        and not local_model\n        and defaultOpenAIChatModel == OpenAIChatModel.GPT3_5_TURBO\n    )\n\n    if warn_gpt_3_5:\n        existing_hook = kwargs.get(\"run_on_first_use\", noop)\n\n        def with_warning() -&gt; None:\n            existing_hook()\n            gpt_3_5_warning()\n\n        kwargs[\"run_on_first_use\"] = with_warning\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.AzureGPT","title":"<code>AzureGPT(config)</code>","text":"<p>             Bases: <code>OpenAIGPT</code></p> <p>Class to access OpenAI LLMs via Azure. These env variables can be obtained from the file <code>.azure_env</code>. Azure OpenAI doesn't support <code>completion</code> Attributes:     config (AzureConfig): AzureConfig object     api_key (str): Azure API key     api_base (str): Azure API base url     api_version (str): Azure API version     model_name (str): the name of gpt model in your deployment     model_version (str): the version of gpt model in your deployment</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, config: AzureConfig):\n    # This will auto-populate config values from .env file\n    load_dotenv()\n    super().__init__(config)\n    self.config: AzureConfig = config\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_API_KEY not set in .env file,\n            please set it to your Azure API key.\"\"\"\n        )\n\n    if self.config.api_base == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_API_BASE not set in .env file,\n            please set it to your Azure API key.\"\"\"\n        )\n\n    if self.config.deployment_name == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_DEPLOYMENT_NAME not set in .env file,\n            please set it to your Azure openai deployment name.\"\"\"\n        )\n    self.deployment_name = self.config.deployment_name\n\n    if self.config.model_name == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_MODEL_NAME not set in .env file,\n            please set it to chat model name in your deployment.\"\"\"\n        )\n\n    # set the chat model to be the same as the model_name\n    # This corresponds to the gpt model you chose for your deployment\n    # when you deployed a model\n    self.set_chat_model()\n\n    self.client = AzureOpenAI(\n        api_key=self.config.api_key,\n        azure_endpoint=self.config.api_base,\n        api_version=self.config.api_version,\n        azure_deployment=self.config.deployment_name,\n    )\n    self.async_client = AsyncAzureOpenAI(\n        api_key=self.config.api_key,\n        azure_endpoint=self.config.api_base,\n        api_version=self.config.api_version,\n        azure_deployment=self.config.deployment_name,\n        timeout=Timeout(self.config.timeout),\n    )\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.AzureGPT.set_chat_model","title":"<code>set_chat_model()</code>","text":"<p>Sets the chat model configuration based on the model name specified in the <code>.env</code>. This function checks the <code>model_name</code> in the configuration and sets the appropriate chat model in the <code>config.chat_model</code>. It supports handling for '35-turbo' and 'gpt-4' models. For 'gpt-4', it further delegates the handling to <code>handle_gpt4_model</code> method. If the model name does not match any predefined models, it defaults to <code>OpenAIChatModel.GPT4</code>.</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def set_chat_model(self) -&gt; None:\n    \"\"\"\n    Sets the chat model configuration based on the model name specified in the\n    ``.env``. This function checks the `model_name` in the configuration and sets\n    the appropriate chat model in the `config.chat_model`. It supports handling for\n    '35-turbo' and 'gpt-4' models. For 'gpt-4', it further delegates the handling\n    to `handle_gpt4_model` method. If the model name does not match any predefined\n    models, it defaults to `OpenAIChatModel.GPT4`.\n    \"\"\"\n    MODEL_35_TURBO = \"35-turbo\"\n    MODEL_GPT4 = \"gpt-4\"\n\n    if self.config.model_name == MODEL_35_TURBO:\n        self.config.chat_model = OpenAIChatModel.GPT3_5_TURBO\n    elif self.config.model_name == MODEL_GPT4:\n        self.handle_gpt4_model()\n    else:\n        self.config.chat_model = OpenAIChatModel.GPT4\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.AzureGPT.handle_gpt4_model","title":"<code>handle_gpt4_model()</code>","text":"<p>Handles the setting of the GPT-4 model in the configuration. This function checks the <code>model_version</code> in the configuration. If the version is not set, it raises a ValueError indicating that the model version needs to be specified in the <code>.env</code> file. It sets <code>OpenAIChatModel.GPT4_TURBO</code> if the version is '1106-Preview', otherwise, it defaults to setting <code>OpenAIChatModel.GPT4</code>.</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def handle_gpt4_model(self) -&gt; None:\n    \"\"\"\n    Handles the setting of the GPT-4 model in the configuration.\n    This function checks the `model_version` in the configuration.\n    If the version is not set, it raises a ValueError indicating that the model\n    version needs to be specified in the ``.env`` file.\n    It sets `OpenAIChatModel.GPT4_TURBO` if the version is\n    '1106-Preview', otherwise, it defaults to setting `OpenAIChatModel.GPT4`.\n    \"\"\"\n    VERSION_1106_PREVIEW = \"1106-Preview\"\n\n    if self.config.model_version == \"\":\n        raise ValueError(\n            \"AZURE_OPENAI_MODEL_VERSION not set in .env file. \"\n            \"Please set it to the chat model version used in your deployment.\"\n        )\n\n    if self.config.model_version == VERSION_1106_PREVIEW:\n        self.config.chat_model = OpenAIChatModel.GPT4_TURBO\n    else:\n        self.config.chat_model = OpenAIChatModel.GPT4\n</code></pre>"},{"location":"reference/language_models/azure_openai/","title":"azure_openai","text":"<p>langroid/language_models/azure_openai.py </p>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureConfig","title":"<code>AzureConfig(**kwargs)</code>","text":"<p>             Bases: <code>OpenAIGPTConfig</code></p> <p>Configuration for Azure OpenAI GPT.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>should be <code>azure.</code></p> <code>api_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_API_VERSION.</code></p> <code>deployment_name</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_DEPLOYMENT_NAME</code> and should be based the custom name you chose for your deployment when you deployed a model.</p> <code>model_name</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_GPT_MODEL_NAME</code> and should be based on the model name chosen during setup.</p> <code>model_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_MODEL_VERSION</code> and should be based on the model name chosen during setup.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    local_model = \"api_base\" in kwargs and kwargs[\"api_base\"] is not None\n\n    chat_model = kwargs.get(\"chat_model\", \"\")\n    local_prefixes = [\"local/\", \"litellm/\", \"ollama/\"]\n    if any(chat_model.startswith(prefix) for prefix in local_prefixes):\n        local_model = True\n\n    warn_gpt_3_5 = (\n        \"chat_model\" not in kwargs.keys()\n        and not local_model\n        and defaultOpenAIChatModel == OpenAIChatModel.GPT3_5_TURBO\n    )\n\n    if warn_gpt_3_5:\n        existing_hook = kwargs.get(\"run_on_first_use\", noop)\n\n        def with_warning() -&gt; None:\n            existing_hook()\n            gpt_3_5_warning()\n\n        kwargs[\"run_on_first_use\"] = with_warning\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureGPT","title":"<code>AzureGPT(config)</code>","text":"<p>             Bases: <code>OpenAIGPT</code></p> <p>Class to access OpenAI LLMs via Azure. These env variables can be obtained from the file <code>.azure_env</code>. Azure OpenAI doesn't support <code>completion</code> Attributes:     config (AzureConfig): AzureConfig object     api_key (str): Azure API key     api_base (str): Azure API base url     api_version (str): Azure API version     model_name (str): the name of gpt model in your deployment     model_version (str): the version of gpt model in your deployment</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, config: AzureConfig):\n    # This will auto-populate config values from .env file\n    load_dotenv()\n    super().__init__(config)\n    self.config: AzureConfig = config\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_API_KEY not set in .env file,\n            please set it to your Azure API key.\"\"\"\n        )\n\n    if self.config.api_base == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_API_BASE not set in .env file,\n            please set it to your Azure API key.\"\"\"\n        )\n\n    if self.config.deployment_name == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_DEPLOYMENT_NAME not set in .env file,\n            please set it to your Azure openai deployment name.\"\"\"\n        )\n    self.deployment_name = self.config.deployment_name\n\n    if self.config.model_name == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_MODEL_NAME not set in .env file,\n            please set it to chat model name in your deployment.\"\"\"\n        )\n\n    # set the chat model to be the same as the model_name\n    # This corresponds to the gpt model you chose for your deployment\n    # when you deployed a model\n    self.set_chat_model()\n\n    self.client = AzureOpenAI(\n        api_key=self.config.api_key,\n        azure_endpoint=self.config.api_base,\n        api_version=self.config.api_version,\n        azure_deployment=self.config.deployment_name,\n    )\n    self.async_client = AsyncAzureOpenAI(\n        api_key=self.config.api_key,\n        azure_endpoint=self.config.api_base,\n        api_version=self.config.api_version,\n        azure_deployment=self.config.deployment_name,\n        timeout=Timeout(self.config.timeout),\n    )\n</code></pre>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureGPT.set_chat_model","title":"<code>set_chat_model()</code>","text":"<p>Sets the chat model configuration based on the model name specified in the <code>.env</code>. This function checks the <code>model_name</code> in the configuration and sets the appropriate chat model in the <code>config.chat_model</code>. It supports handling for '35-turbo' and 'gpt-4' models. For 'gpt-4', it further delegates the handling to <code>handle_gpt4_model</code> method. If the model name does not match any predefined models, it defaults to <code>OpenAIChatModel.GPT4</code>.</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def set_chat_model(self) -&gt; None:\n    \"\"\"\n    Sets the chat model configuration based on the model name specified in the\n    ``.env``. This function checks the `model_name` in the configuration and sets\n    the appropriate chat model in the `config.chat_model`. It supports handling for\n    '35-turbo' and 'gpt-4' models. For 'gpt-4', it further delegates the handling\n    to `handle_gpt4_model` method. If the model name does not match any predefined\n    models, it defaults to `OpenAIChatModel.GPT4`.\n    \"\"\"\n    MODEL_35_TURBO = \"35-turbo\"\n    MODEL_GPT4 = \"gpt-4\"\n\n    if self.config.model_name == MODEL_35_TURBO:\n        self.config.chat_model = OpenAIChatModel.GPT3_5_TURBO\n    elif self.config.model_name == MODEL_GPT4:\n        self.handle_gpt4_model()\n    else:\n        self.config.chat_model = OpenAIChatModel.GPT4\n</code></pre>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureGPT.handle_gpt4_model","title":"<code>handle_gpt4_model()</code>","text":"<p>Handles the setting of the GPT-4 model in the configuration. This function checks the <code>model_version</code> in the configuration. If the version is not set, it raises a ValueError indicating that the model version needs to be specified in the <code>.env</code> file. It sets <code>OpenAIChatModel.GPT4_TURBO</code> if the version is '1106-Preview', otherwise, it defaults to setting <code>OpenAIChatModel.GPT4</code>.</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def handle_gpt4_model(self) -&gt; None:\n    \"\"\"\n    Handles the setting of the GPT-4 model in the configuration.\n    This function checks the `model_version` in the configuration.\n    If the version is not set, it raises a ValueError indicating that the model\n    version needs to be specified in the ``.env`` file.\n    It sets `OpenAIChatModel.GPT4_TURBO` if the version is\n    '1106-Preview', otherwise, it defaults to setting `OpenAIChatModel.GPT4`.\n    \"\"\"\n    VERSION_1106_PREVIEW = \"1106-Preview\"\n\n    if self.config.model_version == \"\":\n        raise ValueError(\n            \"AZURE_OPENAI_MODEL_VERSION not set in .env file. \"\n            \"Please set it to the chat model version used in your deployment.\"\n        )\n\n    if self.config.model_version == VERSION_1106_PREVIEW:\n        self.config.chat_model = OpenAIChatModel.GPT4_TURBO\n    else:\n        self.config.chat_model = OpenAIChatModel.GPT4\n</code></pre>"},{"location":"reference/language_models/base/","title":"base","text":"<p>langroid/language_models/base.py </p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicate it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall.from_dict","title":"<code>from_dict(message)</code>  <code>staticmethod</code>","text":"<p>Initialize from dictionary. Args:     d: dictionary containing fields to initialize</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef from_dict(message: Dict[str, Any]) -&gt; \"LLMFunctionCall\":\n    \"\"\"\n    Initialize from dictionary.\n    Args:\n        d: dictionary containing fields to initialize\n    \"\"\"\n    fun_call = LLMFunctionCall(name=message[\"name\"])\n    fun_args_str = message[\"arguments\"]\n    # sometimes may be malformed with invalid indents,\n    # so we try to be safe by removing newlines.\n    if fun_args_str is not None:\n        fun_args_str = fun_args_str.replace(\"\\n\", \"\").strip()\n        fun_args = ast.literal_eval(fun_args_str)\n    else:\n        fun_args = None\n    fun_call.arguments = fun_args\n\n    return fun_call\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing message sent to, or received from, LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage.api_dict","title":"<code>api_dict()</code>","text":"<p>Convert to dictionary for API request, keeping ONLY the fields that are expected in an API call! E.g., DROP the tool_id, since it is only for use in the Assistant API,     not the completion API. Returns:     dict: dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for API request, keeping ONLY\n    the fields that are expected in an API call!\n    E.g., DROP the tool_id, since it is only for use in the Assistant API,\n        not the completion API.\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\n    d = self.dict()\n    # drop None values since API doesn't accept them\n    dict_no_none = {k: v for k, v in d.items() if v is not None}\n    if \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n        # OpenAI API does not like empty name\n        del dict_no_none[\"name\"]\n    if \"function_call\" in dict_no_none:\n        # arguments must be a string\n        if \"arguments\" in dict_no_none[\"function_call\"]:\n            dict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\n                dict_no_none[\"function_call\"][\"arguments\"]\n            )\n    # IMPORTANT! drop fields that are not expected in API call\n    dict_no_none.pop(\"tool_id\", None)\n    dict_no_none.pop(\"timestamp\", None)\n    dict_no_none.pop(\"chat_document_id\", None)\n    return dict_no_none\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing response from LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse.get_recipient_and_message","title":"<code>get_recipient_and_message()</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains \"TO:  \", or (b) <code>message</code> is empty and <code>function_call</code> with <code>to: &lt;name&gt;</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_recipient_and_message(\n    self,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n\n    Two cases:\n    (a) `message` contains \"TO: &lt;name&gt; &lt;content&gt;\", or\n    (b) `message` is empty and `function_call` with `to: &lt;name&gt;`\n\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n\n    \"\"\"\n\n    if self.function_call is not None:\n        # in this case we ignore message, since all information is in function_call\n        msg = \"\"\n        args = self.function_call.arguments\n        if isinstance(args, dict):\n            recipient = args.get(\"recipient\", \"\")\n        return recipient, msg\n    else:\n        msg = self.message\n\n    # It's not a function call, so continue looking to see\n    # if a recipient is specified in the message.\n\n    # First check if message contains \"TO: &lt;recipient&gt; &lt;content&gt;\"\n    recipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\n    # check if there is a top level json that specifies 'recipient',\n    # and retain the entire message as content.\n    if recipient_name == \"\":\n        recipient_name = top_level_json_field(msg, \"recipient\") if msg else \"\"\n        content = msg\n    return recipient_name, content\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel","title":"<code>LanguageModel(config=LLMConfig())</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for language models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, config: LLMConfig = LLMConfig()):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.create","title":"<code>create(config)</code>  <code>staticmethod</code>","text":"<p>Create a language model. Args:     config: configuration for language model Returns: instance of language model</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef create(config: Optional[LLMConfig]) -&gt; Optional[\"LanguageModel\"]:\n    \"\"\"\n    Create a language model.\n    Args:\n        config: configuration for language model\n    Returns: instance of language model\n    \"\"\"\n    if type(config) is LLMConfig:\n        raise ValueError(\n            \"\"\"\n            Cannot create a Language Model object from LLMConfig. \n            Please specify a specific subclass of LLMConfig e.g., \n            OpenAIGPTConfig. If you are creating a ChatAgent from \n            a ChatAgentConfig, please specify the `llm` field of this config\n            as a specific subclass of LLMConfig, e.g., OpenAIGPTConfig.\n            \"\"\"\n        )\n    from langroid.language_models.azure_openai import AzureGPT\n    from langroid.language_models.mock_lm import MockLM, MockLMConfig\n    from langroid.language_models.openai_gpt import OpenAIGPT\n\n    if config is None or config.type is None:\n        return None\n\n    if config.type == \"mock\":\n        return MockLM(cast(MockLMConfig, config))\n\n    openai: Union[Type[AzureGPT], Type[OpenAIGPT]]\n\n    if config.type == \"azure\":\n        openai = AzureGPT\n    else:\n        openai = OpenAIGPT\n    cls = dict(\n        openai=openai,\n    ).get(config.type, openai)\n    return cls(config)  # type: ignore\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.user_assistant_pairs","title":"<code>user_assistant_pairs(lst)</code>  <code>staticmethod</code>","text":"<p>Given an even-length sequence of strings, split into a sequence of pairs</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>List[str]</code> <p>sequence of strings</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>List[Tuple[str,str]]: sequence of pairs of strings</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef user_assistant_pairs(lst: List[str]) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Given an even-length sequence of strings, split into a sequence of pairs\n\n    Args:\n        lst (List[str]): sequence of strings\n\n    Returns:\n        List[Tuple[str,str]]: sequence of pairs of strings\n    \"\"\"\n    evens = lst[::2]\n    odds = lst[1::2]\n    return list(zip(evens, odds))\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_chat_history_components","title":"<code>get_chat_history_components(messages)</code>  <code>staticmethod</code>","text":"<p>From the chat history, extract system prompt, user-assistant turns, and final user msg.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>List of messages in the chat history</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[Tuple[str, str]], str]</code> <p>Tuple[str, List[Tuple[str,str]], str]: system prompt, user-assistant turns, final user msg</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef get_chat_history_components(\n    messages: List[LLMMessage],\n) -&gt; Tuple[str, List[Tuple[str, str]], str]:\n    \"\"\"\n    From the chat history, extract system prompt, user-assistant turns, and\n    final user msg.\n\n    Args:\n        messages (List[LLMMessage]): List of messages in the chat history\n\n    Returns:\n        Tuple[str, List[Tuple[str,str]], str]:\n            system prompt, user-assistant turns, final user msg\n\n    \"\"\"\n    # Handle various degenerate cases\n    messages = [m for m in messages]  # copy\n    DUMMY_SYS_PROMPT = \"You are a helpful assistant.\"\n    DUMMY_USER_PROMPT = \"Follow the instructions above.\"\n    if len(messages) == 0 or messages[0].role != Role.SYSTEM:\n        logger.warning(\"No system msg, creating dummy system prompt\")\n        messages.insert(0, LLMMessage(content=DUMMY_SYS_PROMPT, role=Role.SYSTEM))\n    system_prompt = messages[0].content\n\n    # now we have messages = [Sys,...]\n    if len(messages) == 1:\n        logger.warning(\n            \"Got only system message in chat history, creating dummy user prompt\"\n        )\n        messages.append(LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, msg, ...]\n\n    if messages[1].role != Role.USER:\n        messages.insert(1, LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, user, ...]\n    if messages[-1].role != Role.USER:\n        logger.warning(\n            \"Last message in chat history is not a user message,\"\n            \" creating dummy user prompt\"\n        )\n        messages.append(LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, user, ..., user]\n    # so we omit the first and last elements and make pairs of user-asst messages\n    conversation = [m.content for m in messages[1:-1]]\n    user_prompt = messages[-1].content\n    pairs = LanguageModel.user_assistant_pairs(conversation)\n    return system_prompt, pairs, user_prompt\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.set_stream","title":"<code>set_stream(stream)</code>  <code>abstractmethod</code>","text":"<p>Enable or disable streaming output from API. Return previous value of stream.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Return previous value of stream.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.update_usage_cost","title":"<code>update_usage_cost(chat, prompts, completions, cost)</code>","text":"<p>Update usage cost for this LLM. Args:     chat (bool): whether to update for chat or completion model     prompts (int): number of tokens used for prompts     completions (int): number of tokens used for completions     cost (float): total token cost in USD</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def update_usage_cost(\n    self, chat: bool, prompts: int, completions: int, cost: float\n) -&gt; None:\n    \"\"\"\n    Update usage cost for this LLM.\n    Args:\n        chat (bool): whether to update for chat or completion model\n        prompts (int): number of tokens used for prompts\n        completions (int): number of tokens used for completions\n        cost (float): total token cost in USD\n    \"\"\"\n    mdl = self.config.chat_model if chat else self.config.completion_model\n    if mdl is None:\n        return\n    if mdl not in self.usage_cost_dict:\n        self.usage_cost_dict[mdl] = LLMTokenUsage()\n    counter = self.usage_cost_dict[mdl]\n    counter.prompt_tokens += prompts\n    counter.completion_tokens += completions\n    counter.cost += cost\n    counter.calls += 1\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.tot_tokens_cost","title":"<code>tot_tokens_cost()</code>  <code>classmethod</code>","text":"<p>Return total tokens used and total cost across all models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@classmethod\ndef tot_tokens_cost(cls) -&gt; Tuple[int, float]:\n    \"\"\"\n    Return total tokens used and total cost across all models.\n    \"\"\"\n    total_tokens = 0\n    total_cost = 0.0\n    for counter in cls.usage_cost_dict.values():\n        total_tokens += counter.total_tokens\n        total_cost += counter.cost\n    return total_tokens, total_cost\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.followup_to_standalone","title":"<code>followup_to_standalone(chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question. Args:     chat_history: list of tuples of (question, answer)     query: follow-up question</p> <p>Returns: standalone version of the question</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def followup_to_standalone(\n    self, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n    \"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n\n    Returns: standalone version of the question\n    \"\"\"\n    history = collate_chat_history(chat_history)\n\n    prompt = f\"\"\"\n    Given the CHAT HISTORY below, and a follow-up QUESTION or SEARCH PHRASE,\n    rephrase the follow-up question/phrase as a STANDALONE QUESTION that\n    can be understood without the context of the chat history.\n\n    Chat history: {history}\n\n    Follow-up question: {question} \n    \"\"\".strip()\n    show_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-PROMPT= \")\n    standalone = self.generate(prompt=prompt, max_tokens=1024).message.strip()\n    show_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-RESPONSE= \")\n    return standalone\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.StreamingIfAllowed","title":"<code>StreamingIfAllowed(llm, stream=True)</code>","text":"<p>Context to temporarily enable or disable streaming, if allowed globally via <code>settings.stream</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, llm: LanguageModel, stream: bool = True):\n    self.llm = llm\n    self.stream = stream\n</code></pre>"},{"location":"reference/language_models/config/","title":"config","text":"<p>langroid/language_models/config.py </p>"},{"location":"reference/language_models/mock_lm/","title":"mock_lm","text":"<p>langroid/language_models/mock_lm.py </p> <p>Mock Language Model for testing</p>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLMConfig","title":"<code>MockLMConfig</code>","text":"<p>             Bases: <code>LLMConfig</code></p> <p>Mock Language Model Configuration.</p> <p>Attributes:</p> Name Type Description <code>response_dict</code> <code>Dict[str, str]</code> <p>A \"response rule-book\", in the form of a dictionary; if last msg in dialog is x,then respond with response_dict[x]</p>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM","title":"<code>MockLM(config=MockLMConfig())</code>","text":"<p>             Bases: <code>LanguageModel</code></p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def __init__(self, config: MockLMConfig = MockLMConfig()):\n    super().__init__(config)\n    self.config: MockLMConfig = config\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.chat","title":"<code>chat(messages, max_tokens=200, functions=None, function_call='auto')</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def chat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return self._response(last_msg)\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.achat","title":"<code>achat(messages, max_tokens=200, functions=None, function_call='auto')</code>  <code>async</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def achat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return self._response(last_msg)\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.generate","title":"<code>generate(prompt, max_tokens=200)</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def generate(self, prompt: str, max_tokens: int = 200) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return self._response(prompt)\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.agenerate","title":"<code>agenerate(prompt, max_tokens=200)</code>  <code>async</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def agenerate(self, prompt: str, max_tokens: int = 200) -&gt; LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return self._response(prompt)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/","title":"openai_gpt","text":"<p>langroid/language_models/openai_gpt.py </p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Chat models</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Various params that can be sent to an OpenAI API chat-completion call. When specified, any param here overrides the one with same name in the OpenAIGPTConfig.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig","title":"<code>OpenAIGPTConfig(**kwargs)</code>","text":"<p>             Bases: <code>LLMConfig</code></p> <p>Class for any LLM with an OpenAI-like API: besides the OpenAI models this includes: (a) locally-served models behind an OpenAI-compatible API (b) non-local models, using a proxy adaptor lib like litellm that provides     an OpenAI-compatible API. We could rename this class to OpenAILikeConfig.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    local_model = \"api_base\" in kwargs and kwargs[\"api_base\"] is not None\n\n    chat_model = kwargs.get(\"chat_model\", \"\")\n    local_prefixes = [\"local/\", \"litellm/\", \"ollama/\"]\n    if any(chat_model.startswith(prefix) for prefix in local_prefixes):\n        local_model = True\n\n    warn_gpt_3_5 = (\n        \"chat_model\" not in kwargs.keys()\n        and not local_model\n        and defaultOpenAIChatModel == OpenAIChatModel.GPT3_5_TURBO\n    )\n\n    if warn_gpt_3_5:\n        existing_hook = kwargs.get(\"run_on_first_use\", noop)\n\n        def with_warning() -&gt; None:\n            existing_hook()\n            gpt_3_5_warning()\n\n        kwargs[\"run_on_first_use\"] = with_warning\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig.create","title":"<code>create(prefix)</code>  <code>classmethod</code>","text":"<p>Create a config class whose params can be set via a desired prefix from the .env file or env vars. E.g., using <pre><code>OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\nollama_config = OllamaConfig()\n</code></pre> you can have a group of params prefixed by \"OLLAMA_\", to be used with models served via <code>ollama</code>. This way, you can maintain several setting-groups in your .env file, one per model type.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@classmethod\ndef create(cls, prefix: str) -&gt; Type[\"OpenAIGPTConfig\"]:\n    \"\"\"Create a config class whose params can be set via a desired\n    prefix from the .env file or env vars.\n    E.g., using\n    ```python\n    OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\n    ollama_config = OllamaConfig()\n    ```\n    you can have a group of params prefixed by \"OLLAMA_\", to be used\n    with models served via `ollama`.\n    This way, you can maintain several setting-groups in your .env file,\n    one per model type.\n    \"\"\"\n\n    class DynamicConfig(OpenAIGPTConfig):\n        pass\n\n    DynamicConfig.Config.env_prefix = prefix.upper() + \"_\"\n\n    return DynamicConfig\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIResponse","title":"<code>OpenAIResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>OpenAI response model, either completion or chat.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT","title":"<code>OpenAIGPT(config=OpenAIGPTConfig())</code>","text":"<p>             Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig = OpenAIGPTConfig()):\n    \"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\n    # copy the config to avoid modifying the original\n    config = config.copy()\n    super().__init__(config)\n    self.config: OpenAIGPTConfig = config\n\n    # Run the first time the model is used\n    self.run_on_first_use = cache(self.config.run_on_first_use)\n\n    # global override of chat_model,\n    # to allow quick testing with other models\n    if settings.chat_model != \"\":\n        self.config.chat_model = settings.chat_model\n        self.config.completion_model = settings.chat_model\n\n    if len(parts := self.config.chat_model.split(\"//\")) &gt; 1:\n        # there is a formatter specified, e.g.\n        # \"litellm/ollama/mistral//hf\" or\n        # \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n        formatter = parts[1]\n        self.config.chat_model = parts[0]\n        if formatter == \"hf\":\n            # e.g. \"litellm/ollama/mistral//hf\" -&gt; \"litellm/ollama/mistral\"\n            formatter = find_hf_formatter(self.config.chat_model)\n            if formatter != \"\":\n                # e.g. \"mistral\"\n                self.config.formatter = formatter\n                logging.warning(\n                    f\"\"\"\n                    Using completions (not chat) endpoint with HuggingFace \n                    chat_template for {formatter} for \n                    model {self.config.chat_model}\n                    \"\"\"\n                )\n        else:\n            # e.g. \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n            self.config.formatter = formatter\n\n    if self.config.formatter is not None:\n        self.config.hf_formatter = HFFormatter(\n            HFPromptFormatterConfig(model_name=self.config.formatter)\n        )\n\n    # if model name starts with \"litellm\",\n    # set the actual model name by stripping the \"litellm/\" prefix\n    # and set the litellm flag to True\n    if self.config.chat_model.startswith(\"litellm/\") or self.config.litellm:\n        # e.g. litellm/ollama/mistral\n        self.config.litellm = True\n        self.api_base = self.config.api_base\n        if self.config.chat_model.startswith(\"litellm/\"):\n            # strip the \"litellm/\" prefix\n            # e.g. litellm/ollama/llama2 =&gt; ollama/llama2\n            self.config.chat_model = self.config.chat_model.split(\"/\", 1)[1]\n    elif self.config.chat_model.startswith(\"local/\"):\n        # expect this to be of the form \"local/localhost:8000/v1\",\n        # depending on how the model is launched locally.\n        # In this case the model served locally behind an OpenAI-compatible API\n        # so we can just use `openai.*` methods directly,\n        # and don't need a adaptor library like litellm\n        self.config.litellm = False\n        self.config.seed = None  # some models raise an error when seed is set\n        # Extract the api_base from the model name after the \"local/\" prefix\n        self.api_base = self.config.chat_model.split(\"/\", 1)[1]\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n    elif self.config.chat_model.startswith(\"ollama/\"):\n        self.config.ollama = True\n\n        # use api_base from config if set, else fall back on OLLAMA_BASE_URL\n        self.api_base = self.config.api_base or OLLAMA_BASE_URL\n        self.api_key = OLLAMA_API_KEY\n        self.config.chat_model = self.config.chat_model.replace(\"ollama/\", \"\")\n    else:\n        self.api_base = self.config.api_base\n\n    if settings.chat_model != \"\":\n        # if we're overriding chat model globally, set completion model to same\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.formatter is not None:\n        # we want to format chats -&gt; completions using this specific formatter\n        self.config.use_completion_for_chat = True\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.use_completion_for_chat:\n        self.config.use_chat_for_completion = False\n\n    # NOTE: The api_key should be set in the .env file, or via\n    # an explicit `export OPENAI_API_KEY=xxx` or `setenv OPENAI_API_KEY xxx`\n    # Pydantic's BaseSettings will automatically pick it up from the\n    # .env file\n    # The config.api_key is ignored when not using an OpenAI model\n    if self.is_openai_completion_model() or self.is_openai_chat_model():\n        self.api_key = config.api_key\n        if self.api_key == DUMMY_API_KEY:\n            self.api_key = os.getenv(\"OPENAI_API_KEY\", DUMMY_API_KEY)\n    else:\n        self.api_key = DUMMY_API_KEY\n\n    self.is_groq = self.config.chat_model.startswith(\"groq/\")\n\n    if self.is_groq:\n        self.config.chat_model = self.config.chat_model.replace(\"groq/\", \"\")\n        self.api_key = os.getenv(\"GROQ_API_KEY\", DUMMY_API_KEY)\n        self.client = Groq(\n            api_key=self.api_key,\n        )\n        self.async_client = AsyncGroq(\n            api_key=self.api_key,\n        )\n    else:\n        self.client = OpenAI(\n            api_key=self.api_key,\n            base_url=self.api_base,\n            organization=self.config.organization,\n            timeout=Timeout(self.config.timeout),\n        )\n        self.async_client = AsyncOpenAI(\n            api_key=self.api_key,\n            organization=self.config.organization,\n            base_url=self.api_base,\n            timeout=Timeout(self.config.timeout),\n        )\n\n    self.cache: CacheDB\n    if settings.cache_type == \"momento\":\n        from langroid.cachedb.momento_cachedb import (\n            MomentoCache,\n            MomentoCacheConfig,\n        )\n\n        if config.cache_config is None or not isinstance(\n            config.cache_config,\n            MomentoCacheConfig,\n        ):\n            # switch to fresh momento config if needed\n            config.cache_config = MomentoCacheConfig()\n        self.cache = MomentoCache(config.cache_config)\n    elif \"redis\" in settings.cache_type:\n        if config.cache_config is None or not isinstance(\n            config.cache_config,\n            RedisCacheConfig,\n        ):\n            # switch to fresh redis config if needed\n            config.cache_config = RedisCacheConfig(\n                fake=\"fake\" in settings.cache_type\n            )\n        if \"fake\" in settings.cache_type:\n            # force use of fake redis if global cache_type is \"fakeredis\"\n            config.cache_config.fake = True\n        self.cache = RedisCache(config.cache_config)\n    else:\n        raise ValueError(\n            f\"Invalid cache type {settings.cache_type}. \"\n            \"Valid types are momento, redis, fakeredis\"\n        )\n\n    self.config._validate_litellm()\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat_context_length","title":"<code>chat_context_length()</code>","text":"<p>Context-length for chat-completion models/endpoints Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for chat-completion models/endpoints\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    model = (\n        self.config.completion_model\n        if self.config.use_completion_for_chat\n        else self.config.chat_model\n    )\n    return _context_length.get(model, super().chat_context_length())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.completion_context_length","title":"<code>completion_context_length()</code>","text":"<p>Context-length for completion models/endpoints Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def completion_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for completion models/endpoints\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    model = (\n        self.config.chat_model\n        if self.config.use_chat_for_completion\n        else self.config.completion_model\n    )\n    return _context_length.get(model, super().completion_context_length())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat_cost","title":"<code>chat_cost()</code>","text":"<p>(Prompt, Generation) cost per 1000 tokens, for chat-completion models/endpoints. Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_cost(self) -&gt; Tuple[float, float]:\n    \"\"\"\n    (Prompt, Generation) cost per 1000 tokens, for chat-completion\n    models/endpoints.\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    return _cost_per_1k_tokens.get(self.config.chat_model, super().chat_cost())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API. Args:     stream: enable streaming output from API Returns: previous value of stream</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\n    tmp = self.config.stream\n    self.config.stream = stream\n    return tmp\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status\"\"\"\n    return self.config.stream and settings.stream\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.noop","title":"<code>noop()</code>","text":"<p>Does nothing.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def noop() -&gt; None:\n    \"\"\"Does nothing.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.litellm_logging_fn","title":"<code>litellm_logging_fn(model_call_dict)</code>","text":"<p>Logging function for litellm</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def litellm_logging_fn(model_call_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Logging function for litellm\"\"\"\n    try:\n        api_input_dict = model_call_dict.get(\"additional_args\", {}).get(\n            \"complete_input_dict\"\n        )\n        if api_input_dict is not None:\n            text = escape(json.dumps(api_input_dict, indent=2))\n            print(\n                f\"[grey37]LITELLM: {text}[/grey37]\",\n            )\n    except Exception:\n        pass\n</code></pre>"},{"location":"reference/language_models/utils/","title":"utils","text":"<p>langroid/language_models/utils.py </p>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(func, initial_delay=1, exponential_base=1.3, jitter=True, max_retries=5, errors=(requests.exceptions.RequestException, openai.APITimeoutError, openai.RateLimitError, openai.AuthenticationError, openai.APIError, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def retry_with_exponential_backoff(\n    func: Callable[..., Any],\n    initial_delay: float = 1,\n    exponential_base: float = 1.3,\n    jitter: bool = True,\n    max_retries: int = 5,\n    errors: tuple = (  # type: ignore\n        requests.exceptions.RequestException,\n        openai.APITimeoutError,\n        openai.RateLimitError,\n        openai.AuthenticationError,\n        openai.APIError,\n        aiohttp.ServerTimeoutError,\n        asyncio.TimeoutError,\n    ),\n) -&gt; Callable[..., Any]:\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            except openai.BadRequestError as e:\n                # do not retry when the request itself is invalid,\n                # e.g. when context is too long\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n            except openai.AuthenticationError as e:\n                # do not retry when there's an auth error\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n\n            # Retry on specified errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                        f\" Last error: {str(e)}.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n                logger.warning(\n                    f\"\"\"OpenAI API request failed with error: \n                    {e}. \n                    Retrying in {delay} seconds...\"\"\"\n                )\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n</code></pre>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.async_retry_with_exponential_backoff","title":"<code>async_retry_with_exponential_backoff(func, initial_delay=1, exponential_base=1.3, jitter=True, max_retries=5, errors=(openai.APITimeoutError, openai.RateLimitError, openai.AuthenticationError, openai.APIError, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def async_retry_with_exponential_backoff(\n    func: Callable[..., Any],\n    initial_delay: float = 1,\n    exponential_base: float = 1.3,\n    jitter: bool = True,\n    max_retries: int = 5,\n    errors: tuple = (  # type: ignore\n        openai.APITimeoutError,\n        openai.RateLimitError,\n        openai.AuthenticationError,\n        openai.APIError,\n        aiohttp.ServerTimeoutError,\n        asyncio.TimeoutError,\n    ),\n) -&gt; Callable[..., Any]:\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    async def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or exception is raised\n        while True:\n            try:\n                result = await func(*args, **kwargs)\n                return result\n\n            except openai.BadRequestError as e:\n                # do not retry when the request itself is invalid,\n                # e.g. when context is too long\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n            except openai.AuthenticationError as e:\n                # do not retry when there's an auth error\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n            # Retry on specified errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                        f\" Last error: {str(e)}.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n                logger.warning(\n                    f\"\"\"OpenAI API request failed with error{e}. \n                    Retrying in {delay} seconds...\"\"\"\n                )\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/","title":"prompt_formatter","text":"<p>langroid/language_models/prompt_formatter/init.py </p>"},{"location":"reference/language_models/prompt_formatter/#langroid.language_models.prompt_formatter.PromptFormatter","title":"<code>PromptFormatter(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a prompt formatter</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/#langroid.language_models.prompt_formatter.PromptFormatter.format","title":"<code>format(messages)</code>  <code>abstractmethod</code>","text":"<p>Convert sequence of messages (system, user, assistant, user, assistant...user)     to a single prompt formatted according to the specific format type,     to be used in a /completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>chat history as a sequence of messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>formatted version of chat history</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>@abstractmethod\ndef format(self, messages: List[LLMMessage]) -&gt; str:\n    \"\"\"\n    Convert sequence of messages (system, user, assistant, user, assistant...user)\n        to a single prompt formatted according to the specific format type,\n        to be used in a /completions endpoint.\n\n    Args:\n        messages (List[LLMMessage]): chat history as a sequence of messages\n\n    Returns:\n        (str): formatted version of chat history\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/#langroid.language_models.prompt_formatter.Llama2Formatter","title":"<code>Llama2Formatter(config)</code>","text":"<p>             Bases: <code>PromptFormatter</code></p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/base/","title":"base","text":"<p>langroid/language_models/prompt_formatter/base.py </p>"},{"location":"reference/language_models/prompt_formatter/base/#langroid.language_models.prompt_formatter.base.PromptFormatter","title":"<code>PromptFormatter(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a prompt formatter</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/base/#langroid.language_models.prompt_formatter.base.PromptFormatter.format","title":"<code>format(messages)</code>  <code>abstractmethod</code>","text":"<p>Convert sequence of messages (system, user, assistant, user, assistant...user)     to a single prompt formatted according to the specific format type,     to be used in a /completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>chat history as a sequence of messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>formatted version of chat history</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>@abstractmethod\ndef format(self, messages: List[LLMMessage]) -&gt; str:\n    \"\"\"\n    Convert sequence of messages (system, user, assistant, user, assistant...user)\n        to a single prompt formatted according to the specific format type,\n        to be used in a /completions endpoint.\n\n    Args:\n        messages (List[LLMMessage]): chat history as a sequence of messages\n\n    Returns:\n        (str): formatted version of chat history\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/hf_formatter/","title":"hf_formatter","text":"<p>langroid/language_models/prompt_formatter/hf_formatter.py </p> <p>Prompt formatter based on HuggingFace <code>AutoTokenizer.apply_chat_template</code> method from their Transformers library. It searches the hub for a model matching the specified name, and uses the first one it finds. We assume that all matching models will have the same tokenizer, so we just use the first one.</p>"},{"location":"reference/language_models/prompt_formatter/hf_formatter/#langroid.language_models.prompt_formatter.hf_formatter.try_import_hf_modules","title":"<code>try_import_hf_modules()</code>","text":"<p>Attempts to import the AutoTokenizer class from the transformers package. Returns:     The AutoTokenizer class if successful. Raises:     ImportError: If the transformers package is not installed.</p> Source code in <code>langroid/language_models/prompt_formatter/hf_formatter.py</code> <pre><code>def try_import_hf_modules() -&gt; Tuple[Type[Any], Type[Any], Type[Any]]:\n    \"\"\"\n    Attempts to import the AutoTokenizer class from the transformers package.\n    Returns:\n        The AutoTokenizer class if successful.\n    Raises:\n        ImportError: If the transformers package is not installed.\n    \"\"\"\n    try:\n        from huggingface_hub import HfApi, ModelFilter\n        from transformers import AutoTokenizer\n\n        return AutoTokenizer, HfApi, ModelFilter\n    except ImportError:\n        raise ImportError(\n            \"\"\"\n            You are trying to use some/all of:\n            HuggingFace transformers.AutoTokenizer,\n            huggingface_hub.HfApi, \n            huggingface_hub.ModelFilter,\n            but these are not not installed \n            by default with Langroid. Please install langroid using the \n            `transformers` extra, like so:\n            pip install \"langroid[transformers]\"\n            or equivalent.\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/llama2_formatter/","title":"llama2_formatter","text":"<p>langroid/language_models/prompt_formatter/llama2_formatter.py </p>"},{"location":"reference/language_models/prompt_formatter/llama2_formatter/#langroid.language_models.prompt_formatter.llama2_formatter.Llama2Formatter","title":"<code>Llama2Formatter(config)</code>","text":"<p>             Bases: <code>PromptFormatter</code></p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/parsing/","title":"parsing","text":"<p>langroid/parsing/init.py </p>"},{"location":"reference/parsing/#langroid.parsing.Parser","title":"<code>Parser(config)</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>def __init__(self, config: ParsingConfig):\n    self.config = config\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n    except Exception:\n        self.tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n</code></pre>"},{"location":"reference/parsing/#langroid.parsing.Parser.add_window_ids","title":"<code>add_window_ids(chunks)</code>","text":"<p>Chunks may belong to multiple docs, but for each doc, they appear consecutively. Add window_ids in metadata</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def add_window_ids(self, chunks: List[Document]) -&gt; None:\n    \"\"\"Chunks may belong to multiple docs, but for each doc,\n    they appear consecutively. Add window_ids in metadata\"\"\"\n\n    # discard empty chunks\n    chunks = [c for c in chunks if c.content.strip() != \"\"]\n    if len(chunks) == 0:\n        return\n    # The original metadata.id (if any) is ignored since it will be same for all\n    # chunks and is useless. We want a distinct id for each chunk.\n    # ASSUMPTION: all chunks c of a doc have same c.metadata.id !\n    orig_ids = [c.metadata.id for c in chunks]\n    ids = [ObjectRegistry.new_id() for c in chunks]\n    id2chunk = {id: c for id, c in zip(ids, chunks)}\n\n    # group the ids by orig_id\n    # (each distinct orig_id refers to a different document)\n    orig_id_to_ids: Dict[str, List[str]] = {}\n    for orig_id, id in zip(orig_ids, ids):\n        if orig_id not in orig_id_to_ids:\n            orig_id_to_ids[orig_id] = []\n        orig_id_to_ids[orig_id].append(id)\n\n    # now each orig_id maps to a sequence of ids within a single doc\n\n    k = self.config.n_neighbor_ids\n    for orig, ids in orig_id_to_ids.items():\n        # ids are consecutive chunks in a single doc\n        n = len(ids)\n        window_ids = [ids[max(0, i - k) : min(n, i + k + 1)] for i in range(n)]\n        for i, _ in enumerate(ids):\n            c = id2chunk[ids[i]]\n            c.metadata.window_ids = window_ids[i]\n            c.metadata.id = ids[i]\n            c.metadata.is_chunk = True\n</code></pre>"},{"location":"reference/parsing/#langroid.parsing.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\n    self,\n    text: str,\n) -&gt; List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n\n    Args:\n        text: The text to split into chunks.\n\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = self.tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks &lt; self.config.max_chunks:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[: self.config.chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = self.tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text\n            # from remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        last_punctuation = max(\n            chunk_text.rfind(\".\"),\n            chunk_text.rfind(\"?\"),\n            chunk_text.rfind(\"!\"),\n            chunk_text.rfind(\"\\n\"),\n        )\n\n        # If there is a punctuation mark, and the last punctuation index is\n        # after MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation &gt; self.config.min_chunk_chars\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Remove any newline characters and strip any leading or\n        # trailing whitespace\n        chunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\n\n        if len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text_to_append)\n\n        # Remove the tokens corresponding to the chunk text\n        # from the remaining tokens\n        tokens = tokens[\n            len(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n        ]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # There may be remaining tokens, but we discard them\n    # since we have already reached the maximum number of chunks\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/agent_chats/","title":"agent_chats","text":"<p>langroid/parsing/agent_chats.py </p>"},{"location":"reference/parsing/agent_chats/#langroid.parsing.agent_chats.parse_message","title":"<code>parse_message(msg)</code>","text":"<p>Parse the intended recipient and content of a message. Message format is assumed to be TO[]:. The TO[]: part is optional. <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to parse</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>str, str: task-name of intended recipient, and content of message (if recipient is not specified, task-name is empty string)</p> Source code in <code>langroid/parsing/agent_chats.py</code> <pre><code>@no_type_check\ndef parse_message(msg: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Parse the intended recipient and content of a message.\n    Message format is assumed to be TO[&lt;recipient&gt;]:&lt;message&gt;.\n    The TO[&lt;recipient&gt;]: part is optional.\n\n    Args:\n        msg (str): message to parse\n\n    Returns:\n        str, str: task-name of intended recipient, and content of message\n            (if recipient is not specified, task-name is empty string)\n\n    \"\"\"\n    if msg is None:\n        return \"\", \"\"\n\n    # Grammar definition\n    name = Word(alphanums)\n    to_start = Literal(\"TO[\").suppress()\n    to_end = Literal(\"]:\").suppress()\n    to_field = (to_start + name(\"name\") + to_end) | Empty().suppress()\n    message = SkipTo(StringEnd())(\"text\")\n\n    # Parser definition\n    parser = to_field + message\n\n    try:\n        parsed = parser.parseString(msg)\n        return parsed.name, parsed.text\n    except ParseException:\n        return \"\", msg\n</code></pre>"},{"location":"reference/parsing/code_parser/","title":"code_parser","text":"<p>langroid/parsing/code_parser.py </p>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser","title":"<code>CodeParser(config)</code>","text":"Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def __init__(self, config: CodeParsingConfig):\n    self.config = config\n    self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.num_tokens","title":"<code>num_tokens(text)</code>","text":"<p>How many tokens are in the text, according to the tokenizer. This needs to be accurate, otherwise we may exceed the maximum number of tokens allowed by the model. Args:     text: string to tokenize Returns:     number of tokens in the text</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def num_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    How many tokens are in the text, according to the tokenizer.\n    This needs to be accurate, otherwise we may exceed the maximum\n    number of tokens allowed by the model.\n    Args:\n        text: string to tokenize\n    Returns:\n        number of tokens in the text\n    \"\"\"\n    tokens = self.tokenizer.encode(text)\n    return len(tokens)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.split","title":"<code>split(docs)</code>","text":"<p>Split the documents into chunks, according to the config.splitter. Only the documents with a language in the config.extensions are split.</p> <p>Note</p> <p>We assume the metadata in each document has at least a <code>language</code> field, which is used to determine how to chunk the code.</p> <p>Args:     docs: list of documents to split Returns:     list of documents, where each document is a chunk; the metadata of the     original document is duplicated for each chunk, so that when we retrieve a     chunk, we immediately know info about the original document.</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def split(self, docs: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Split the documents into chunks, according to the config.splitter.\n    Only the documents with a language in the config.extensions are split.\n    !!! note\n        We assume the metadata in each document has at least a `language` field,\n        which is used to determine how to chunk the code.\n    Args:\n        docs: list of documents to split\n    Returns:\n        list of documents, where each document is a chunk; the metadata of the\n        original document is duplicated for each chunk, so that when we retrieve a\n        chunk, we immediately know info about the original document.\n    \"\"\"\n    chunked_docs = [\n        [\n            Document(content=chunk, metadata=d.metadata)\n            for chunk in chunk_code(\n                d.content,\n                d.metadata.language,  # type: ignore\n                self.config.chunk_size,\n                self.num_tokens,\n            )\n            if chunk.strip() != \"\"\n        ]\n        for d in docs\n        if d.metadata.language in self.config.extensions  # type: ignore\n    ]\n    # collapse the list of lists into a single list\n    return reduce(lambda x, y: x + y, chunked_docs)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.chunk_code","title":"<code>chunk_code(code, language, max_tokens, len_fn)</code>","text":"<p>Chunk code into smaller pieces, so that we don't exceed the maximum number of tokens allowed by the embedding model. Args:     code: string of code     language: str as a file extension, e.g. \"py\", \"yml\"     max_tokens: max tokens per chunk     len_fn: function to get the length of a string in token units Returns:</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def chunk_code(\n    code: str, language: str, max_tokens: int, len_fn: Callable[[str], int]\n) -&gt; List[str]:\n    \"\"\"\n    Chunk code into smaller pieces, so that we don't exceed the maximum\n    number of tokens allowed by the embedding model.\n    Args:\n        code: string of code\n        language: str as a file extension, e.g. \"py\", \"yml\"\n        max_tokens: max tokens per chunk\n        len_fn: function to get the length of a string in token units\n    Returns:\n\n    \"\"\"\n    lexer = get_lexer_by_name(language)\n    tokens = list(lex(code, lexer))\n\n    chunks = []\n    current_chunk = \"\"\n    for token_type, token_value in tokens:\n        if token_type in Token.Text.Whitespace:\n            current_chunk += token_value\n        else:\n            token_tokens = len_fn(token_value)\n            if len_fn(current_chunk) + token_tokens &lt;= max_tokens:\n                current_chunk += token_value\n            else:\n                chunks.append(current_chunk)\n                current_chunk = token_value\n\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/document_parser/","title":"document_parser","text":"<p>langroid/parsing/document_parser.py </p>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser","title":"<code>DocumentParser(source, config)</code>","text":"<p>             Bases: <code>Parser</code></p> <p>Abstract base class for extracting text from special types of docs such as PDFs or Docx.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source, either a URL or a file path.</p> <code>doc_bytes</code> <code>BytesIO</code> <p>BytesIO object containing the doc data.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.create","title":"<code>create(source, config, doc_type=None)</code>  <code>classmethod</code>","text":"<p>Create a DocumentParser instance based on source type     and config..library specified. <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | bytes</code> <p>The source, could be a URL, file path, or bytes object.</p> required <code>config</code> <code>ParserConfig</code> <p>The parser configuration.</p> required <code>doc_type</code> <code>str | None</code> <p>The type of document, if known</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DocumentParser</code> <code>'DocumentParser'</code> <p>An instance of a DocumentParser subclass.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    source: str | bytes,\n    config: ParsingConfig,\n    doc_type: str | DocumentType | None = None,\n) -&gt; \"DocumentParser\":\n    \"\"\"\n    Create a DocumentParser instance based on source type\n        and config.&lt;source_type&gt;.library specified.\n\n    Args:\n        source (str|bytes): The source, could be a URL, file path,\n            or bytes object.\n        config (ParserConfig): The parser configuration.\n        doc_type (str|None): The type of document, if known\n\n    Returns:\n        DocumentParser: An instance of a DocumentParser subclass.\n    \"\"\"\n    if DocumentParser._document_type(source, doc_type) == DocumentType.PDF:\n        if config.pdf.library == \"fitz\":\n            return FitzPDFParser(source, config)\n        elif config.pdf.library == \"pypdf\":\n            return PyPDFParser(source, config)\n        elif config.pdf.library == \"pdfplumber\":\n            return PDFPlumberParser(source, config)\n        elif config.pdf.library == \"unstructured\":\n            return UnstructuredPDFParser(source, config)\n        elif config.pdf.library == \"pdf2image\":\n            return ImagePdfParser(source, config)\n        else:\n            raise ValueError(\n                f\"Unsupported PDF library specified: {config.pdf.library}\"\n            )\n    elif DocumentParser._document_type(source, doc_type) == DocumentType.DOCX:\n        if config.docx.library == \"unstructured\":\n            return UnstructuredDocxParser(source, config)\n        elif config.docx.library == \"python-docx\":\n            return PythonDocxParser(source, config)\n        else:\n            raise ValueError(\n                f\"Unsupported DOCX library specified: {config.docx.library}\"\n            )\n    elif DocumentParser._document_type(source, doc_type) == DocumentType.DOC:\n        return UnstructuredDocParser(source, config)\n    else:\n        source_name = source if isinstance(source, str) else \"bytes\"\n        raise ValueError(f\"Unsupported document type: {source_name}\")\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.chunks_from_path_or_bytes","title":"<code>chunks_from_path_or_bytes(source, parser, doc_type=None, lines=None)</code>  <code>staticmethod</code>","text":"<p>Get document chunks from a file path or bytes object. Args:     source (str|bytes): The source, which could be a URL, path or bytes object.     parser (Parser): The parser instance (for splitting the document).     doc_type (str|DocumentType|None): The type of document, if known.     lines (int|None): The number of lines to read from a plain text file. Returns:     List[Document]: A list of <code>Document</code> objects,         each containing a chunk of text, determined by the         chunking and splitting settings in the parser config.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>@staticmethod\ndef chunks_from_path_or_bytes(\n    source: str | bytes,\n    parser: Parser,\n    doc_type: str | DocumentType | None = None,\n    lines: int | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Get document chunks from a file path or bytes object.\n    Args:\n        source (str|bytes): The source, which could be a URL, path or bytes object.\n        parser (Parser): The parser instance (for splitting the document).\n        doc_type (str|DocumentType|None): The type of document, if known.\n        lines (int|None): The number of lines to read from a plain text file.\n    Returns:\n        List[Document]: A list of `Document` objects,\n            each containing a chunk of text, determined by the\n            chunking and splitting settings in the parser config.\n    \"\"\"\n    dtype: DocumentType = DocumentParser._document_type(source, doc_type)\n    if dtype in [DocumentType.PDF, DocumentType.DOC, DocumentType.DOCX]:\n        doc_parser = DocumentParser.create(\n            source,\n            parser.config,\n            doc_type=doc_type,\n        )\n        chunks = doc_parser.get_doc_chunks()\n        if len(chunks) == 0 and dtype == DocumentType.PDF:\n            doc_parser = ImagePdfParser(source, parser.config)\n            chunks = doc_parser.get_doc_chunks()\n        return chunks\n    else:\n        # try getting as plain text; these will be chunked downstream\n        # -- could be a bytes object or a path\n        if isinstance(source, bytes):\n            content = source.decode()\n            if lines is not None:\n                file_lines = content.splitlines()[:lines]\n                content = \"\\n\".join(line.strip() for line in file_lines)\n        else:\n            with open(source, \"r\") as f:\n                if lines is not None:\n                    file_lines = list(itertools.islice(f, lines))\n                    content = \"\\n\".join(line.strip() for line in file_lines)\n                else:\n                    content = f.read()\n        soup = BeautifulSoup(content, \"html.parser\")\n        text = soup.get_text()\n        source_name = source if isinstance(source, str) else \"bytes\"\n        doc = Document(\n            content=text,\n            metadata=DocMetaData(source=str(source_name)),\n        )\n        return parser.split([doc])\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"Yield each page in the PDF.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"Extract text from a given page.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.fix_text","title":"<code>fix_text(text)</code>","text":"<p>Fix text extracted from a PDF.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The extracted text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The fixed text.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def fix_text(self, text: str) -&gt; str:\n    \"\"\"\n    Fix text extracted from a PDF.\n\n    Args:\n        text (str): The extracted text.\n\n    Returns:\n        str: The fixed text.\n    \"\"\"\n    # Some pdf parsers introduce extra space before hyphen,\n    # so use regular expression to replace 'space-hyphen' with just 'hyphen'\n    return re.sub(r\" +\\-\", \"-\", text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_doc","title":"<code>get_doc()</code>","text":"<p>Get entire text from source as a single document.</p> <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the content of the pdf file, and metadata containing source name (URL or path)</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc(self) -&gt; Document:\n    \"\"\"\n    Get entire text from source as a single document.\n\n    Returns:\n        a `Document` object containing the content of the pdf file,\n            and metadata containing source name (URL or path)\n    \"\"\"\n\n    text = \"\".join(\n        [self.extract_text_from_page(page) for _, page in self.iterate_pages()]\n    )\n    return Document(content=text, metadata=DocMetaData(source=self.source))\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_doc_chunks","title":"<code>get_doc_chunks()</code>","text":"<p>Get document chunks from a pdf source, with page references in the document metadata.</p> <p>Adapted from https://github.com/whitead/paper-qa/blob/main/paperqa/readers.py</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: a list of <code>Document</code> objects, each containing a chunk of text</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc_chunks(self) -&gt; List[Document]:\n    \"\"\"\n    Get document chunks from a pdf source,\n    with page references in the document metadata.\n\n    Adapted from\n    https://github.com/whitead/paper-qa/blob/main/paperqa/readers.py\n\n    Returns:\n        List[Document]: a list of `Document` objects,\n            each containing a chunk of text\n    \"\"\"\n\n    split = []  # tokens in curr split\n    pages: List[str] = []\n    docs: List[Document] = []\n    # metadata.id to be shared by ALL chunks of this document\n    common_id = ObjectRegistry.new_id()\n    for i, page in self.iterate_pages():\n        page_text = self.extract_text_from_page(page)\n        split += self.tokenizer.encode(page_text)\n        pages.append(str(i + 1))\n        # split could be so long it needs to be split\n        # into multiple chunks. Or it could be so short\n        # that it needs to be combined with the next chunk.\n        while len(split) &gt; self.config.chunk_size:\n            # pretty formatting of pages (e.g. 1-3, 4, 5-7)\n            pg = \"-\".join([pages[0], pages[-1]])\n            text = self.tokenizer.decode(split[: self.config.chunk_size])\n            docs.append(\n                Document(\n                    content=text,\n                    metadata=DocMetaData(\n                        source=f\"{self.source} pages {pg}\",\n                        is_chunk=True,\n                        id=common_id,\n                    ),\n                )\n            )\n            split = split[self.config.chunk_size - self.config.overlap :]\n            pages = [str(i + 1)]\n    if len(split) &gt; self.config.overlap:\n        pg = \"-\".join([pages[0], pages[-1]])\n        text = self.tokenizer.decode(split[: self.config.chunk_size])\n        docs.append(\n            Document(\n                content=text,\n                metadata=DocMetaData(\n                    source=f\"{self.source} pages {pg}\",\n                    is_chunk=True,\n                    id=common_id,\n                ),\n            )\n        )\n    self.add_window_ids(docs)\n    return docs\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser","title":"<code>FitzPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>fitz</code> library.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>fitz</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, 'fitz.Page'], None, None]</code> <p>Generator[fitz.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, \"fitz.Page\"], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `fitz`.\n\n    Returns:\n        Generator[fitz.Page]: Generator yielding each page.\n    \"\"\"\n    if fitz is None:\n        raise LangroidImportError(\"fitz\", \"pdf-parsers\")\n    doc = fitz.open(stream=self.doc_bytes, filetype=\"pdf\")\n    for i, page in enumerate(doc):\n        yield i, page\n    doc.close()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>fitz</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The <code>fitz</code> page object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: \"fitz.Page\") -&gt; str:\n    \"\"\"\n    Extract text from a given `fitz` page.\n\n    Args:\n        page (fitz.Page): The `fitz` page object.\n\n    Returns:\n        str: Extracted text from the page.\n    \"\"\"\n    return self.fix_text(page.get_text())\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser","title":"<code>PyPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>pypdf</code> library.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>pypdf</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, PageObject], None, None]</code> <p>Generator[pypdf.pdf.PageObject]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, pypdf.PageObject], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `pypdf`.\n\n    Returns:\n        Generator[pypdf.pdf.PageObject]: Generator yielding each page.\n    \"\"\"\n    if pypdf is None:\n        raise LangroidImportError(\"pypdf\", \"pdf-parsers\")\n    reader = pypdf.PdfReader(self.doc_bytes)\n    for i, page in enumerate(reader.pages):\n        yield i, page\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>pypdf</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>PageObject</code> <p>The <code>pypdf</code> page object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: pypdf.PageObject) -&gt; str:\n    \"\"\"\n    Extract text from a given `pypdf` page.\n\n    Args:\n        page (pypdf.pdf.PageObject): The `pypdf` page object.\n\n    Returns:\n        str: Extracted text from the page.\n    \"\"\"\n    return self.fix_text(page.extract_text())\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PDFPlumberParser","title":"<code>PDFPlumberParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>pdfplumber</code> library.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PDFPlumberParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>pdfplumber</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, Page], None, None]</code> <p>Generator[pdfplumber.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(\n    self,\n) -&gt; (Generator)[Tuple[int, pdfplumber.pdf.Page], None, None]:  # type: ignore\n    \"\"\"\n    Yield each page in the PDF using `pdfplumber`.\n\n    Returns:\n        Generator[pdfplumber.Page]: Generator yielding each page.\n    \"\"\"\n    if pdfplumber is None:\n        raise LangroidImportError(\"pdfplumber\", \"pdf-parsers\")\n    with pdfplumber.open(self.doc_bytes) as pdf:\n        for i, page in enumerate(pdf.pages):\n            yield i, page\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PDFPlumberParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>pdfplumber</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The <code>pdfplumber</code> page object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: pdfplumber.pdf.Page) -&gt; str:  # type: ignore\n    \"\"\"\n    Extract text from a given `pdfplumber` page.\n\n    Args:\n        page (pdfplumber.Page): The `pdfplumber` page object.\n\n    Returns:\n        str: Extracted text from the page.\n    \"\"\"\n    return self.fix_text(page.extract_text())\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.ImagePdfParser","title":"<code>ImagePdfParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs that are images, i.e. not \"true\" PDFs.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.ImagePdfParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>pdf2image</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Image</code> <p>The PIL Image object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the image.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: \"Image\") -&gt; str:  # type: ignore\n    \"\"\"\n    Extract text from a given `pdf2image` page.\n\n    Args:\n        page (Image): The PIL Image object.\n\n    Returns:\n        str: Extracted text from the image.\n    \"\"\"\n    try:\n        import pytesseract\n    except ImportError:\n        raise LangroidImportError(\"pytesseract\", \"pdf-parsers\")\n\n    text = pytesseract.image_to_string(page)\n    return self.fix_text(text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredPDFParser","title":"<code>UnstructuredPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDF files using the <code>unstructured</code> library.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredPDFParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>unstructured</code> element.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>unstructured element</code> <p>The <code>unstructured</code> element object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the element.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"\n    Extract text from a given `unstructured` element.\n\n    Args:\n        page (unstructured element): The `unstructured` element object.\n\n    Returns:\n        str: Extracted text from the element.\n    \"\"\"\n    text = \" \".join(el.text for el in page)\n    return self.fix_text(text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredDocxParser","title":"<code>UnstructuredDocxParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing DOCX files using the <code>unstructured</code> library.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredDocxParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>unstructured</code> element.</p> Note <p>The concept of \"pages\" doesn't actually exist in the .docx file format in the same way it does in formats like .pdf. A .docx file is made up of a series of elements like paragraphs and tables, but the division into pages is done dynamically based on the rendering settings (like the page size, margin size, font size, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>unstructured element</code> <p>The <code>unstructured</code> element object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the element.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"\n    Extract text from a given `unstructured` element.\n\n    Note:\n        The concept of \"pages\" doesn't actually exist in the .docx file format in\n        the same way it does in formats like .pdf. A .docx file is made up of a\n        series of elements like paragraphs and tables, but the division into\n        pages is done dynamically based on the rendering settings (like the page\n        size, margin size, font size, etc.).\n\n    Args:\n        page (unstructured element): The `unstructured` element object.\n\n    Returns:\n        str: Extracted text from the element.\n    \"\"\"\n    text = \" \".join(el.text for el in page)\n    return self.fix_text(text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PythonDocxParser","title":"<code>PythonDocxParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing DOCX files using the <code>python-docx</code> library.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PythonDocxParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Simulate iterating through pages. In a DOCX file, pages are not explicitly defined, so we consider each paragraph as a separate 'page' for simplicity.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"\n    Simulate iterating through pages.\n    In a DOCX file, pages are not explicitly defined,\n    so we consider each paragraph as a separate 'page' for simplicity.\n    \"\"\"\n    try:\n        import docx\n    except ImportError:\n        raise LangroidImportError(\"python-docx\", \"docx\")\n\n    doc = docx.Document(self.doc_bytes)\n    for i, para in enumerate(doc.paragraphs, start=1):\n        yield i, [para]\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PythonDocxParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given 'page', which in this case is a single paragraph.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>list</code> <p>A list containing a single Paragraph object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the paragraph.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"\n    Extract text from a given 'page', which in this case is a single paragraph.\n\n    Args:\n        page (list): A list containing a single Paragraph object.\n\n    Returns:\n        str: Extracted text from the paragraph.\n    \"\"\"\n    paragraph = page[0]\n    return self.fix_text(paragraph.text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.find_last_full_char","title":"<code>find_last_full_char(possible_unicode)</code>","text":"<p>Find the index of the last full character in a byte string. Args:     possible_unicode (bytes): The bytes to check. Returns:     int: The index of the last full unicode character.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def find_last_full_char(possible_unicode: bytes) -&gt; int:\n    \"\"\"\n    Find the index of the last full character in a byte string.\n    Args:\n        possible_unicode (bytes): The bytes to check.\n    Returns:\n        int: The index of the last full unicode character.\n    \"\"\"\n\n    for i in range(len(possible_unicode) - 1, 0, -1):\n        if (possible_unicode[i] &amp; 0xC0) != 0x80:\n            return i\n    return 0\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.is_plain_text","title":"<code>is_plain_text(path_or_bytes)</code>","text":"<p>Check if a file is plain text by attempting to decode it as UTF-8. Args:     path_or_bytes (str|bytes): The file path or bytes object. Returns:     bool: True if the file is plain text, False otherwise.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def is_plain_text(path_or_bytes: str | bytes) -&gt; bool:\n    \"\"\"\n    Check if a file is plain text by attempting to decode it as UTF-8.\n    Args:\n        path_or_bytes (str|bytes): The file path or bytes object.\n    Returns:\n        bool: True if the file is plain text, False otherwise.\n    \"\"\"\n    if isinstance(path_or_bytes, str):\n        if path_or_bytes.startswith((\"http://\", \"https://\")):\n            response = requests.get(path_or_bytes)\n            response.raise_for_status()\n            content = response.content[:1024]\n        else:\n            with open(path_or_bytes, \"rb\") as f:\n                content = f.read(1024)\n    else:\n        content = path_or_bytes[:1024]\n    try:\n        # Attempt to decode the content as UTF-8\n        content = content[: find_last_full_char(content)]\n\n        _ = content.decode(\"utf-8\")\n        # Additional checks can go here, e.g., to verify that the content\n        # doesn't contain too many unusual characters for it to be considered text\n        return True\n    except UnicodeDecodeError:\n        # If decoding fails, it's likely not plain text (or not encoded in UTF-8)\n        return False\n</code></pre>"},{"location":"reference/parsing/para_sentence_split/","title":"para_sentence_split","text":"<p>langroid/parsing/para_sentence_split.py </p>"},{"location":"reference/parsing/parse_json/","title":"parse_json","text":"<p>langroid/parsing/parse_json.py </p>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.is_valid_json","title":"<code>is_valid_json(json_str)</code>","text":"<p>Check if the input string is a valid JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The input string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string is a valid JSON, False otherwise.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def is_valid_json(json_str: str) -&gt; bool:\n    \"\"\"Check if the input string is a valid JSON.\n\n    Args:\n        json_str (str): The input string to check.\n\n    Returns:\n        bool: True if the input string is a valid JSON, False otherwise.\n    \"\"\"\n    try:\n        json.loads(json_str)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.flatten","title":"<code>flatten(nested_list)</code>","text":"<p>Flatten a nested list into a single list of strings</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def flatten(nested_list) -&gt; Iterator[str]:  # type: ignore\n    \"\"\"Flatten a nested list into a single list of strings\"\"\"\n    for item in nested_list:\n        if isinstance(item, (list, tuple)):\n            for subitem in flatten(item):\n                yield subitem\n        else:\n            yield item\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.get_json_candidates","title":"<code>get_json_candidates(s)</code>","text":"<p>Get top-level JSON candidates, i.e. strings between curly braces.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def get_json_candidates(s: str) -&gt; List[str]:\n    \"\"\"Get top-level JSON candidates, i.e. strings between curly braces.\"\"\"\n    # Define the grammar for matching curly braces\n    curly_braces = originalTextFor(nestedExpr(\"{\", \"}\"))\n\n    # Parse the string\n    try:\n        results = curly_braces.searchString(s)\n        # Properly convert nested lists to strings\n        return [r[0] for r in results]\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.add_quotes","title":"<code>add_quotes(s)</code>","text":"<p>Replace accidentally un-quoted string-like keys and values in a potential json str. Intended to handle cases where a weak LLM may produce a JSON-like string containing, e.g. \"rent\": DO-NOT-KNOW, where it \"forgot\" to put quotes on the value, or city: \"New York\" where it \"forgot\" to put quotes on the key. It will even handle cases like 'address: do not know'.</p> <p>Got this fiendishly clever solution from https://stackoverflow.com/a/66053900/10940584 Far better/safer than trying to do it with regexes.</p> <p>Args: - s (str): The potential JSON string to parse.</p> <ul> <li>str: The (potential) JSON string with un-quoted string-like values     replaced by quoted values.</li> </ul> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def add_quotes(s: str) -&gt; str:\n    \"\"\"\n    Replace accidentally un-quoted string-like keys and values in a potential json str.\n    Intended to handle cases where a weak LLM may produce a JSON-like string\n    containing, e.g. \"rent\": DO-NOT-KNOW, where it \"forgot\" to put quotes on the value,\n    or city: \"New York\" where it \"forgot\" to put quotes on the key.\n    It will even handle cases like 'address: do not know'.\n\n    Got this fiendishly clever solution from\n    https://stackoverflow.com/a/66053900/10940584\n    Far better/safer than trying to do it with regexes.\n\n    Args:\n    - s (str): The potential JSON string to parse.\n\n    Returns:\n    - str: The (potential) JSON string with un-quoted string-like values\n        replaced by quoted values.\n    \"\"\"\n    if is_valid_json(s):\n        return s\n    try:\n        dct = yaml.load(s, yaml.SafeLoader)\n        return json.dumps(dct)\n    except Exception:\n        return s\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.repair_newlines","title":"<code>repair_newlines(s)</code>","text":"<p>Attempt to load as json, and if it fails, try with newlines replaced by space.    Intended to handle cases where weak LLMs produce JSON-like strings where    some string-values contain explicit newlines, e.g.:    {\"text\": \"This is a text with a newline\"}    These would not be valid JSON, so we try to clean them up here.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def repair_newlines(s: str) -&gt; str:\n    \"\"\"\n    Attempt to load as json, and if it fails, try with newlines replaced by space.\n    Intended to handle cases where weak LLMs produce JSON-like strings where\n    some string-values contain explicit newlines, e.g.:\n    {\"text\": \"This is a text\\n with a newline\"}\n    These would not be valid JSON, so we try to clean them up here.\n    \"\"\"\n    try:\n        json.loads(s)\n        return s\n    except Exception:\n        try:\n            s = s.replace(\"\\n\", \" \")\n            json.loads(s)\n            return s\n        except Exception:\n            return s\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.extract_top_level_json","title":"<code>extract_top_level_json(s)</code>","text":"<p>Extract all top-level JSON-formatted substrings from a given string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of top-level JSON-formatted substrings.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def extract_top_level_json(s: str) -&gt; List[str]:\n    \"\"\"Extract all top-level JSON-formatted substrings from a given string.\n\n    Args:\n        s (str): The input string to search for JSON substrings.\n\n    Returns:\n        List[str]: A list of top-level JSON-formatted substrings.\n    \"\"\"\n    # Find JSON object and array candidates\n    json_candidates = get_json_candidates(s)\n\n    normalized_candidates = [\n        candidate.replace(\"\\\\{\", \"{\").replace(\"\\\\}\", \"}\").replace(\"\\\\_\", \"_\")\n        for candidate in json_candidates\n    ]\n    candidates = [add_quotes(candidate) for candidate in normalized_candidates]\n    candidates = [repair_newlines(candidate) for candidate in candidates]\n    top_level_jsons = [\n        candidate for candidate in candidates if is_valid_json(candidate)\n    ]\n\n    return top_level_jsons\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.top_level_json_field","title":"<code>top_level_json_field(s, f)</code>","text":"<p>Extract the value of a field f from a top-level JSON object. If there are multiple, just return the first.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <code>f</code> <code>str</code> <p>The field to extract from the JSON object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>The value of the field f in the top-level JSON object, if any. Otherwise, return an empty string.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def top_level_json_field(s: str, f: str) -&gt; Any:\n    \"\"\"\n    Extract the value of a field f from a top-level JSON object.\n    If there are multiple, just return the first.\n\n    Args:\n        s (str): The input string to search for JSON substrings.\n        f (str): The field to extract from the JSON object.\n\n    Returns:\n        str: The value of the field f in the top-level JSON object, if any.\n            Otherwise, return an empty string.\n    \"\"\"\n\n    jsons = extract_top_level_json(s)\n    if len(jsons) == 0:\n        return \"\"\n    for j in jsons:\n        json_data = json.loads(j)\n        if f in json_data:\n            return json_data[f]\n\n    return \"\"\n</code></pre>"},{"location":"reference/parsing/parser/","title":"parser","text":"<p>langroid/parsing/parser.py </p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser","title":"<code>Parser(config)</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>def __init__(self, config: ParsingConfig):\n    self.config = config\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n    except Exception:\n        self.tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.add_window_ids","title":"<code>add_window_ids(chunks)</code>","text":"<p>Chunks may belong to multiple docs, but for each doc, they appear consecutively. Add window_ids in metadata</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def add_window_ids(self, chunks: List[Document]) -&gt; None:\n    \"\"\"Chunks may belong to multiple docs, but for each doc,\n    they appear consecutively. Add window_ids in metadata\"\"\"\n\n    # discard empty chunks\n    chunks = [c for c in chunks if c.content.strip() != \"\"]\n    if len(chunks) == 0:\n        return\n    # The original metadata.id (if any) is ignored since it will be same for all\n    # chunks and is useless. We want a distinct id for each chunk.\n    # ASSUMPTION: all chunks c of a doc have same c.metadata.id !\n    orig_ids = [c.metadata.id for c in chunks]\n    ids = [ObjectRegistry.new_id() for c in chunks]\n    id2chunk = {id: c for id, c in zip(ids, chunks)}\n\n    # group the ids by orig_id\n    # (each distinct orig_id refers to a different document)\n    orig_id_to_ids: Dict[str, List[str]] = {}\n    for orig_id, id in zip(orig_ids, ids):\n        if orig_id not in orig_id_to_ids:\n            orig_id_to_ids[orig_id] = []\n        orig_id_to_ids[orig_id].append(id)\n\n    # now each orig_id maps to a sequence of ids within a single doc\n\n    k = self.config.n_neighbor_ids\n    for orig, ids in orig_id_to_ids.items():\n        # ids are consecutive chunks in a single doc\n        n = len(ids)\n        window_ids = [ids[max(0, i - k) : min(n, i + k + 1)] for i in range(n)]\n        for i, _ in enumerate(ids):\n            c = id2chunk[ids[i]]\n            c.metadata.window_ids = window_ids[i]\n            c.metadata.id = ids[i]\n            c.metadata.is_chunk = True\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\n    self,\n    text: str,\n) -&gt; List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n\n    Args:\n        text: The text to split into chunks.\n\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = self.tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks &lt; self.config.max_chunks:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[: self.config.chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = self.tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text\n            # from remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        last_punctuation = max(\n            chunk_text.rfind(\".\"),\n            chunk_text.rfind(\"?\"),\n            chunk_text.rfind(\"!\"),\n            chunk_text.rfind(\"\\n\"),\n        )\n\n        # If there is a punctuation mark, and the last punctuation index is\n        # after MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation &gt; self.config.min_chunk_chars\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Remove any newline characters and strip any leading or\n        # trailing whitespace\n        chunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\n\n        if len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text_to_append)\n\n        # Remove the tokens corresponding to the chunk text\n        # from the remaining tokens\n        tokens = tokens[\n            len(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n        ]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # There may be remaining tokens, but we discard them\n    # since we have already reached the maximum number of chunks\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/repo_loader/","title":"repo_loader","text":"<p>langroid/parsing/repo_loader.py </p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoaderConfig","title":"<code>RepoLoaderConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Configuration for RepoLoader.</p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader","title":"<code>RepoLoader(url, config=RepoLoaderConfig())</code>","text":"<p>Class for recursively getting all file content in a repo.</p> <pre><code>config: configuration for RepoLoader\n</code></pre> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    config: RepoLoaderConfig = RepoLoaderConfig(),\n):\n    \"\"\"\n    Args:\n        url: full github url of repo, or just \"owner/repo\"\n        config: configuration for RepoLoader\n    \"\"\"\n    self.url = url\n    self.config = config\n    self.clone_path: Optional[str] = None\n    self.log_file = \".logs/repo_loader/download_log.json\"\n    os.makedirs(os.path.dirname(self.log_file), exist_ok=True)\n    if not os.path.exists(self.log_file):\n        with open(self.log_file, \"w\") as f:\n            json.dump({\"junk\": \"ignore\"}, f)\n    with open(self.log_file, \"r\") as f:\n        log = json.load(f)\n    if self.url in log and os.path.exists(log[self.url]):\n        logger.info(f\"Repo Already downloaded in {log[self.url]}\")\n        self.clone_path = log[self.url]\n\n    if \"github.com\" in self.url:\n        repo_name = self.url.split(\"github.com/\")[1]\n    else:\n        repo_name = self.url\n    load_dotenv()\n    # authenticated calls to github api have higher rate limit\n    token = os.getenv(\"GITHUB_ACCESS_TOKEN\")\n    g = Github(token)\n    self.repo = self._get_repo_with_retry(g, repo_name)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_issues","title":"<code>get_issues(k=100)</code>","text":"<p>Get up to k issues from the GitHub repo.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def get_issues(self, k: int | None = 100) -&gt; List[IssueData]:\n    \"\"\"Get up to k issues from the GitHub repo.\"\"\"\n    if k is None:\n        issues = self.repo.get_issues(state=\"all\")\n    else:\n        issues = self.repo.get_issues(state=\"all\")[:k]\n    issue_data_list = []\n    for issue in issues:\n        issue_data = IssueData(\n            state=issue.state,\n            year=issue.created_at.year,\n            month=issue.created_at.month,\n            day=issue.created_at.day,\n            assignee=issue.assignee.login if issue.assignee else None,\n            size=get_issue_size(issue.labels),\n            text=issue.body or \"No issue description body.\",\n        )\n        issue_data_list.append(issue_data)\n\n    return issue_data_list\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.clone","title":"<code>clone(path=None)</code>","text":"<p>Clone a GitHub repository to a local directory specified by <code>path</code>, if it has not already been cloned.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local directory where the repository should be cloned. If not specified, a temporary directory will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The path to the local directory where the repository was cloned.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def clone(self, path: Optional[str] = None) -&gt; Optional[str]:\n    \"\"\"\n    Clone a GitHub repository to a local directory specified by `path`,\n    if it has not already been cloned.\n\n    Args:\n        path (str): The local directory where the repository should be cloned.\n            If not specified, a temporary directory will be created.\n\n    Returns:\n        str: The path to the local directory where the repository was cloned.\n    \"\"\"\n    with open(self.log_file, \"r\") as f:\n        log: Dict[str, str] = json.load(f)\n\n    if (\n        self.url in log\n        and os.path.exists(log[self.url])\n        and _has_files(log[self.url])\n    ):\n        logger.warning(f\"Repo Already downloaded in {log[self.url]}\")\n        self.clone_path = log[self.url]\n        return self.clone_path\n\n    self.clone_path = path\n    if path is None:\n        path = self.default_clone_path()\n        self.clone_path = path\n\n    try:\n        subprocess.run([\"git\", \"clone\", self.url, path], check=True)\n        log[self.url] = path\n        with open(self.log_file, \"w\") as f:\n            json.dump(log, f)\n        return self.clone_path\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Git clone failed: {e}\")\n    except Exception as e:\n        logger.error(f\"An error occurred while trying to clone the repository:{e}\")\n\n    return self.clone_path\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_tree_from_github","title":"<code>load_tree_from_github(depth, lines=0)</code>","text":"<p>Get a nested dictionary of GitHub repository file and directory names up to a certain depth, with file contents.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth level.</p> required <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]:</p> <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>A dictionary containing file and directory names, with file contents.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_tree_from_github(\n    self, depth: int, lines: int = 0\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n    \"\"\"\n    Get a nested dictionary of GitHub repository file and directory names\n    up to a certain depth, with file contents.\n\n    Args:\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n\n    Returns:\n        Dict[str, Union[str, List[Dict]]]:\n        A dictionary containing file and directory names, with file contents.\n    \"\"\"\n    root_contents = self.repo.get_contents(\"\")\n    if not isinstance(root_contents, list):\n        root_contents = [root_contents]\n    repo_structure = {\n        \"type\": \"dir\",\n        \"name\": \"\",\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": \"\",\n    }\n\n    # A queue of tuples (current_node, current_depth, parent_structure)\n    queue = deque([(root_contents, 0, repo_structure)])\n\n    while queue:\n        current_node, current_depth, parent_structure = queue.popleft()\n\n        for content in current_node:\n            if not self._is_allowed(content):\n                continue\n            if content.type == \"dir\" and current_depth &lt; depth:\n                # Create a new sub-dictionary for this directory\n                new_dir = {\n                    \"type\": \"dir\",\n                    \"name\": content.name,\n                    \"dirs\": [],\n                    \"files\": [],\n                    \"path\": content.path,\n                }\n                parent_structure[\"dirs\"].append(new_dir)\n                contents = self.repo.get_contents(content.path)\n                if not isinstance(contents, list):\n                    contents = [contents]\n                queue.append(\n                    (\n                        contents,\n                        current_depth + 1,\n                        new_dir,\n                    )\n                )\n            elif content.type == \"file\":\n                file_content = \"\\n\".join(\n                    _get_decoded_content(content).splitlines()[:lines]\n                )\n                file_dict = {\n                    \"type\": \"file\",\n                    \"name\": content.name,\n                    \"content\": file_content,\n                    \"path\": content.path,\n                }\n                parent_structure[\"files\"].append(file_dict)\n\n    return repo_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load","title":"<code>load(path=None, depth=3, lines=0)</code>","text":"<p>From a local folder <code>path</code> (if None, the repo clone path), get:   a nested dictionary (tree) of dicts, files and contents   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path; if none, use self.clone_path()</p> <code>None</code> <code>depth</code> <code>int</code> <p>The depth level.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents, and a list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load(\n    self,\n    path: Optional[str] = None,\n    depth: int = 3,\n    lines: int = 0,\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n    \"\"\"\n    From a local folder `path` (if None, the repo clone path), get:\n      a nested dictionary (tree) of dicts, files and contents\n      a list of Document objects for each file.\n\n    Args:\n        path (str): The local folder path; if none, use self.clone_path()\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n\n    Returns:\n        Tuple of (dict, List_of_Documents):\n            A dictionary containing file and directory names, with file\n            contents, and a list of Document objects for each file.\n    \"\"\"\n    if path is None:\n        if self.clone_path is None or not _has_files(self.clone_path):\n            self.clone()\n        path = self.clone_path\n    if path is None:\n        raise ValueError(\"Unable to clone repo\")\n    return self.load_from_folder(\n        path=path,\n        depth=depth,\n        lines=lines,\n        file_types=self.config.file_types,\n        exclude_dirs=self.config.exclude_dirs,\n        url=self.url,\n    )\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_from_folder","title":"<code>load_from_folder(path, depth=3, lines=0, file_types=None, exclude_dirs=None, url='')</code>  <code>staticmethod</code>","text":"<p>From a local folder <code>path</code> (required), get:   a nested dictionary (tree) of dicts, files and contents, restricting to     desired file_types and excluding undesired directories.   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path, required.</p> required <code>depth</code> <code>int</code> <p>The depth level. Optional, default 3.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.     Optional, default 0 (no lines =&gt; empty string).</p> <code>0</code> <code>file_types</code> <code>List[str]</code> <p>The file types to include.     Optional, default None (all).</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>The directories to exclude.     Optional, default None (no exclusions).</p> <code>None</code> <code>url</code> <code>str</code> <p>Optional url, to be stored in docs as metadata. Default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents. A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef load_from_folder(\n    path: str,\n    depth: int = 3,\n    lines: int = 0,\n    file_types: Optional[List[str]] = None,\n    exclude_dirs: Optional[List[str]] = None,\n    url: str = \"\",\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n    \"\"\"\n    From a local folder `path` (required), get:\n      a nested dictionary (tree) of dicts, files and contents, restricting to\n        desired file_types and excluding undesired directories.\n      a list of Document objects for each file.\n\n    Args:\n        path (str): The local folder path, required.\n        depth (int): The depth level. Optional, default 3.\n        lines (int): The number of lines of file contents to include.\n                Optional, default 0 (no lines =&gt; empty string).\n        file_types (List[str]): The file types to include.\n                Optional, default None (all).\n        exclude_dirs (List[str]): The directories to exclude.\n                Optional, default None (no exclusions).\n        url (str): Optional url, to be stored in docs as metadata. Default \"\".\n\n    Returns:\n        Tuple of (dict, List_of_Documents):\n            A dictionary containing file and directory names, with file contents.\n            A list of Document objects for each file.\n    \"\"\"\n\n    folder_structure = {\n        \"type\": \"dir\",\n        \"name\": \"\",\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": \"\",\n    }\n    # A queue of tuples (current_path, current_depth, parent_structure)\n    queue = deque([(path, 0, folder_structure)])\n    docs = []\n    exclude_dirs = exclude_dirs or []\n    while queue:\n        current_path, current_depth, parent_structure = queue.popleft()\n\n        for item in os.listdir(current_path):\n            item_path = os.path.join(current_path, item)\n            relative_path = os.path.relpath(item_path, path)\n            if (os.path.isdir(item_path) and item in exclude_dirs) or (\n                os.path.isfile(item_path)\n                and file_types is not None\n                and RepoLoader._file_type(item) not in file_types\n            ):\n                continue\n\n            if os.path.isdir(item_path) and current_depth &lt; depth:\n                # Create a new sub-dictionary for this directory\n                new_dir = {\n                    \"type\": \"dir\",\n                    \"name\": item,\n                    \"dirs\": [],\n                    \"files\": [],\n                    \"path\": relative_path,\n                }\n                parent_structure[\"dirs\"].append(new_dir)\n                queue.append((item_path, current_depth + 1, new_dir))\n            elif os.path.isfile(item_path):\n                # Add the file to the current dictionary\n                with open(item_path, \"r\") as f:\n                    file_lines = list(itertools.islice(f, lines))\n                file_content = \"\\n\".join(line.strip() for line in file_lines)\n                if file_content == \"\":\n                    continue\n\n                file_dict = {\n                    \"type\": \"file\",\n                    \"name\": item,\n                    \"content\": file_content,\n                    \"path\": relative_path,\n                }\n                parent_structure[\"files\"].append(file_dict)\n                docs.append(\n                    Document(\n                        content=file_content,\n                        metadata=DocMetaData(\n                            repo=url,\n                            source=relative_path,\n                            url=url,\n                            filename=item,\n                            extension=RepoLoader._file_type(item),\n                            language=RepoLoader._file_type(item),\n                        ),\n                    )\n                )\n    return folder_structure, docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_documents","title":"<code>get_documents(path, parser=Parser(ParsingConfig()), file_types=None, exclude_dirs=None, depth=-1, lines=None, doc_type=None)</code>  <code>staticmethod</code>","text":"<p>Recursively get all files under a path as Document objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | bytes</code> <p>The path to the directory or file, or bytes content. The bytes option is meant to support the case where the content has already been read from a file in an upstream process (e.g. from an API or a database), and we want to avoid having to write it to a temporary file just to read it again. (which can be very slow for large files, especially in a docker container)</p> required <code>parser</code> <code>Parser</code> <p>Parser to use to parse files.</p> <code>Parser(ParsingConfig())</code> <code>file_types</code> <code>List[str]</code> <p>List of file extensions OR filenames OR file_path_names to  include. Defaults to None, which includes all files.</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>List of directories to exclude. Defaults to None, which includes all directories.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Max depth of recursion. Defaults to -1, which includes all depths.</p> <code>-1</code> <code>lines</code> <code>int</code> <p>Number of lines to read from each file. Defaults to None, which reads all lines.</p> <code>None</code> <code>doc_type</code> <code>str | DocumentType</code> <p>The type of document to parse.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of Document objects representing files.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef get_documents(\n    path: str | bytes,\n    parser: Parser = Parser(ParsingConfig()),\n    file_types: Optional[List[str]] = None,\n    exclude_dirs: Optional[List[str]] = None,\n    depth: int = -1,\n    lines: Optional[int] = None,\n    doc_type: str | DocumentType | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Recursively get all files under a path as Document objects.\n\n    Args:\n        path (str|bytes): The path to the directory or file, or bytes content.\n            The bytes option is meant to support the case where the content\n            has already been read from a file in an upstream process\n            (e.g. from an API or a database), and we want to avoid having to\n            write it to a temporary file just to read it again.\n            (which can be very slow for large files,\n            especially in a docker container)\n        parser (Parser): Parser to use to parse files.\n        file_types (List[str], optional): List of file extensions OR\n            filenames OR file_path_names to  include.\n            Defaults to None, which includes all files.\n        exclude_dirs (List[str], optional): List of directories to exclude.\n            Defaults to None, which includes all directories.\n        depth (int, optional): Max depth of recursion. Defaults to -1,\n            which includes all depths.\n        lines (int, optional): Number of lines to read from each file.\n            Defaults to None, which reads all lines.\n        doc_type (str|DocumentType, optional): The type of document to parse.\n\n    Returns:\n        List[Document]: List of Document objects representing files.\n\n    \"\"\"\n    docs = []\n    file_paths = []\n    if isinstance(path, bytes):\n        file_paths.append(path)\n    else:\n        path_obj = Path(path).resolve()\n\n        if path_obj.is_file():\n            file_paths.append(str(path_obj))\n        else:\n            path_depth = len(path_obj.parts)\n            for root, dirs, files in os.walk(path):\n                # Exclude directories if needed\n                if exclude_dirs:\n                    dirs[:] = [d for d in dirs if d not in exclude_dirs]\n\n                current_depth = len(Path(root).resolve().parts) - path_depth\n                if depth == -1 or current_depth &lt;= depth:\n                    for file in files:\n                        file_path = str(Path(root) / file)\n                        if (\n                            file_types is None\n                            or RepoLoader._file_type(file_path) in file_types\n                            or os.path.basename(file_path) in file_types\n                            or file_path in file_types\n                        ):\n                            file_paths.append(file_path)\n\n    for file_path in file_paths:\n        docs.extend(\n            DocumentParser.chunks_from_path_or_bytes(\n                file_path,\n                parser,\n                doc_type=doc_type,\n                lines=lines,\n            )\n        )\n    return docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_docs_from_github","title":"<code>load_docs_from_github(k=None, depth=None, lines=None)</code>","text":"<p>Directly from GitHub, recursively get all files in a repo that have one of the extensions, possibly up to a max number of files, max depth, and max number of lines per file (if any of these are specified).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>max number of files to load, or None for all files</p> <code>None</code> <code>depth</code> <code>int</code> <p>max depth to recurse, or None for infinite depth</p> <code>None</code> <code>lines</code> <code>int</code> <p>max number of lines to get, from a file, or None for all lines</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects, each has fields <code>content</code> and <code>metadata</code>,</p> <code>List[Document]</code> <p>and <code>metadata</code> has fields <code>url</code>, <code>filename</code>, <code>extension</code>, <code>language</code></p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_docs_from_github(\n    self,\n    k: Optional[int] = None,\n    depth: Optional[int] = None,\n    lines: Optional[int] = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Directly from GitHub, recursively get all files in a repo that have one of the\n    extensions, possibly up to a max number of files, max depth, and max number\n    of lines per file (if any of these are specified).\n\n    Args:\n        k (int): max number of files to load, or None for all files\n        depth (int): max depth to recurse, or None for infinite depth\n        lines (int): max number of lines to get, from a file, or None for all lines\n\n    Returns:\n        list of Document objects, each has fields `content` and `metadata`,\n        and `metadata` has fields `url`, `filename`, `extension`, `language`\n    \"\"\"\n    contents = self.repo.get_contents(\"\")\n    if not isinstance(contents, list):\n        contents = [contents]\n    stack = list(zip(contents, [0] * len(contents)))  # stack of (content, depth)\n    # recursively get all files in repo that have one of the extensions\n    docs = []\n    i = 0\n\n    while stack:\n        if k is not None and i == k:\n            break\n        file_content, d = stack.pop()\n        if not self._is_allowed(file_content):\n            continue\n        if file_content.type == \"dir\":\n            if depth is None or d &lt;= depth:\n                items = self.repo.get_contents(file_content.path)\n                if not isinstance(items, list):\n                    items = [items]\n                stack.extend(list(zip(items, [d + 1] * len(items))))\n        else:\n            if depth is None or d &lt;= depth:\n                # need to decode the file content, which is in bytes\n                contents = self.repo.get_contents(file_content.path)\n                if isinstance(contents, list):\n                    contents = contents[0]\n                text = _get_decoded_content(contents)\n                if lines is not None:\n                    text = \"\\n\".join(text.split(\"\\n\")[:lines])\n                i += 1\n\n                # Note `source` is important, it may be used to cite\n                # evidence for an answer.\n                # See  URLLoader\n                # TODO we should use Pydantic to enforce/standardize this\n\n                docs.append(\n                    Document(\n                        content=text,\n                        metadata=DocMetaData(\n                            repo=self.url,\n                            source=file_content.html_url,\n                            url=file_content.html_url,\n                            filename=file_content.name,\n                            extension=self._file_type(file_content.name),\n                            language=self._file_type(file_content.name),\n                        ),\n                    )\n                )\n    return docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.select","title":"<code>select(structure, includes, excludes=[])</code>  <code>staticmethod</code>","text":"<p>Filter a structure dictionary for certain directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>includes</code> <code>List[str]</code> <p>A list of desired directories and files. For files, either full file names or \"file type\" can be specified. E.g.  \"toml\" will include all files with the \".toml\" extension, or \"Makefile\" will include all files named \"Makefile\".</p> required <code>excludes</code> <code>List[str]</code> <p>A list of directories and files to exclude. Similar to <code>includes</code>, full file/dir names or \"file type\" can be specified. Optional, defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef select(\n    structure: Dict[str, Union[str, List[Dict[str, Any]]]],\n    includes: List[str],\n    excludes: List[str] = [],\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n    \"\"\"\n    Filter a structure dictionary for certain directories and files.\n\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        includes (List[str]): A list of desired directories and files.\n            For files, either full file names or \"file type\" can be specified.\n            E.g.  \"toml\" will include all files with the \".toml\" extension,\n            or \"Makefile\" will include all files named \"Makefile\".\n        excludes (List[str]): A list of directories and files to exclude.\n            Similar to `includes`, full file/dir names or \"file type\" can be\n            specified. Optional, defaults to empty list.\n\n\n    Returns:\n        Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.\n    \"\"\"\n    filtered_structure = {\n        \"type\": structure[\"type\"],\n        \"name\": structure[\"name\"],\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": structure[\"path\"],\n    }\n\n    for dir in structure[\"dirs\"]:\n        if (\n            dir[\"name\"] in includes\n            or RepoLoader._file_type(dir[\"name\"]) in includes\n        ) and (\n            dir[\"name\"] not in excludes\n            and RepoLoader._file_type(dir[\"name\"]) not in excludes\n        ):\n            # If the directory is in the select list, include the whole subtree\n            filtered_structure[\"dirs\"].append(dir)\n        else:\n            # Otherwise, filter the directory's contents\n            filtered_dir = RepoLoader.select(dir, includes)\n            if (\n                filtered_dir[\"dirs\"] or filtered_dir[\"files\"]\n            ):  # only add if not empty\n                filtered_structure[\"dirs\"].append(filtered_dir)\n\n    for file in structure[\"files\"]:\n        if (\n            file[\"name\"] in includes\n            or RepoLoader._file_type(file[\"name\"]) in includes\n        ) and (\n            file[\"name\"] not in excludes\n            and RepoLoader._file_type(file[\"name\"]) not in excludes\n        ):\n            filtered_structure[\"files\"].append(file)\n\n    return filtered_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.ls","title":"<code>ls(structure, depth=0)</code>  <code>staticmethod</code>","text":"<p>Get a list of names of files or directories up to a certain depth from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of names of files or directories.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef ls(structure: Dict[str, Union[str, List[Dict]]], depth: int = 0) -&gt; List[str]:\n    \"\"\"\n    Get a list of names of files or directories up to a certain depth from a\n    structure dictionary.\n\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        depth (int, optional): The depth level. Defaults to 0.\n\n    Returns:\n        List[str]: A list of names of files or directories.\n    \"\"\"\n    names = []\n\n    # A queue of tuples (current_structure, current_depth)\n    queue = deque([(structure, 0)])\n\n    while queue:\n        current_structure, current_depth = queue.popleft()\n\n        if current_depth &lt;= depth:\n            names.append(current_structure[\"name\"])\n\n            for dir in current_structure[\"dirs\"]:\n                queue.append((dir, current_depth + 1))\n\n            for file in current_structure[\"files\"]:\n                # add file names only if depth is less than the limit\n                if current_depth &lt; depth:\n                    names.append(file[\"name\"])\n    names = [n for n in names if n not in [\"\", None]]\n    return names\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.list_files","title":"<code>list_files(dir, depth=1, include_types=[], exclude_types=[])</code>  <code>staticmethod</code>","text":"<p>Recursively list all files in a directory, up to a certain depth.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>The directory path, relative to root.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 1.</p> <code>1</code> <code>include_types</code> <code>List[str]</code> <p>A list of file types to include. Defaults to empty list.</p> <code>[]</code> <code>exclude_types</code> <code>List[str]</code> <p>A list of file types to exclude. Defaults to empty list.</p> <code>[]</code> <p>Returns:     List[str]: A list of file names.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef list_files(\n    dir: str,\n    depth: int = 1,\n    include_types: List[str] = [],\n    exclude_types: List[str] = [],\n) -&gt; List[str]:\n    \"\"\"\n    Recursively list all files in a directory, up to a certain depth.\n\n    Args:\n        dir (str): The directory path, relative to root.\n        depth (int, optional): The depth level. Defaults to 1.\n        include_types (List[str], optional): A list of file types to include.\n            Defaults to empty list.\n        exclude_types (List[str], optional): A list of file types to exclude.\n            Defaults to empty list.\n    Returns:\n        List[str]: A list of file names.\n    \"\"\"\n    depth = depth if depth &gt;= 0 else 200\n    output = []\n\n    for root, dirs, files in os.walk(dir):\n        if root.count(os.sep) - dir.count(os.sep) &lt; depth:\n            level = root.count(os.sep) - dir.count(os.sep)\n            sub_indent = \" \" * 4 * (level + 1)\n            for d in dirs:\n                output.append(\"{}{}/\".format(sub_indent, d))\n            for f in files:\n                if include_types and RepoLoader._file_type(f) not in include_types:\n                    continue\n                if exclude_types and RepoLoader._file_type(f) in exclude_types:\n                    continue\n                output.append(\"{}{}\".format(sub_indent, f))\n    return output\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.show_file_contents","title":"<code>show_file_contents(tree)</code>  <code>staticmethod</code>","text":"<p>Print the contents of all files from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef show_file_contents(tree: Dict[str, Union[str, List[Dict[str, Any]]]]) -&gt; str:\n    \"\"\"\n    Print the contents of all files from a structure dictionary.\n\n    Args:\n        tree (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n    \"\"\"\n    contents = \"\"\n    for dir in tree[\"dirs\"]:\n        contents += RepoLoader.show_file_contents(dir)\n    for file in tree[\"files\"]:\n        path = file[\"path\"]\n        contents += f\"\"\"\n        {path}:\n        --------------------\n        {file[\"content\"]}\n\n        \"\"\"\n\n    return contents\n</code></pre>"},{"location":"reference/parsing/routing/","title":"routing","text":"<p>langroid/parsing/routing.py </p>"},{"location":"reference/parsing/routing/#langroid.parsing.routing.parse_addressed_message","title":"<code>parse_addressed_message(content, addressing='@')</code>","text":"<p>In a message-string containing possibly multiple @ occurrences, find the last addressee and extract their name, and the message content following it. <p>E.g. \"thank you @bob, now I will ask @alice again. @alice, where is the mirror?\" =&gt; (\"alice\", \"where is the mirror?\")</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content.</p> required <code>addressing</code> <code>str</code> <p>The addressing character. Defaults to \"@\".</p> <code>'@'</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Tuple[Optional[str], str]:</p> <code>str</code> <p>A tuple containing the last addressee and the subsequent message content.</p> Source code in <code>langroid/parsing/routing.py</code> <pre><code>def parse_addressed_message(\n    content: str, addressing: str = \"@\"\n) -&gt; Tuple[Optional[str], str]:\n    \"\"\"In a message-string containing possibly multiple @&lt;recipient&gt; occurrences,\n    find the last addressee and extract their name,\n    and the message content following it.\n\n    E.g. \"thank you @bob, now I will ask @alice again. @alice, where is the mirror?\" =&gt;\n    (\"alice\", \"where is the mirror?\")\n\n    Args:\n        content (str): The message content.\n        addressing (str, optional): The addressing character. Defaults to \"@\".\n\n    Returns:\n        Tuple[Optional[str], str]:\n        A tuple containing the last addressee and the subsequent message content.\n    \"\"\"\n    # Regex to find all occurrences of the pattern\n    pattern = re.compile(rf\"{re.escape(addressing)}(\\w+)[^\\w]\")\n    matches = list(pattern.finditer(content))\n\n    if not matches:\n        return None, content  # No addressee found, return None and original content\n\n    # Get the last match\n    last_match = matches[-1]\n    last_addressee = last_match.group(1)\n    # Extract content after the last addressee\n    content_after = content[last_match.end() :].strip()\n\n    return last_addressee, content_after\n</code></pre>"},{"location":"reference/parsing/search/","title":"search","text":"<p>langroid/parsing/search.py </p> <p>Utils to search for close matches in (a list of) strings. Useful for retrieval of docs/chunks relevant to a query, in the context of Retrieval-Augmented Generation (RAG), and SQLChat (e.g., to pull relevant parts of a large schema). See tests for examples: tests/main/test_string_search.py</p>"},{"location":"reference/parsing/search/#langroid.parsing.search.find_fuzzy_matches_in_docs","title":"<code>find_fuzzy_matches_in_docs(query, docs, docs_clean, k, words_before=None, words_after=None)</code>","text":"<p>Find approximate matches of the query in the docs and return surrounding characters.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search string.</p> required <code>docs</code> <code>List[Document]</code> <p>List of Document objects to search through.</p> required <code>k</code> <code>int</code> <p>Number of best matches to return.</p> required <code>words_before</code> <code>int | None</code> <p>Number of words to include before each match. Default None =&gt; return max</p> <code>None</code> <code>words_after</code> <code>int | None</code> <p>Number of words to include after each match. Default None =&gt; return max</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of Documents containing the matches, including the given number of words around the match.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def find_fuzzy_matches_in_docs(\n    query: str,\n    docs: List[Document],\n    docs_clean: List[Document],\n    k: int,\n    words_before: int | None = None,\n    words_after: int | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Find approximate matches of the query in the docs and return surrounding\n    characters.\n\n    Args:\n        query (str): The search string.\n        docs (List[Document]): List of Document objects to search through.\n        k (int): Number of best matches to return.\n        words_before (int|None): Number of words to include before each match.\n            Default None =&gt; return max\n        words_after (int|None): Number of words to include after each match.\n            Default None =&gt; return max\n\n    Returns:\n        List[Document]: List of Documents containing the matches,\n            including the given number of words around the match.\n    \"\"\"\n    if len(docs) == 0:\n        return []\n    best_matches = process.extract(\n        query,\n        [d.content for d in docs_clean],\n        limit=k,\n        scorer=fuzz.partial_ratio,\n    )\n\n    real_matches = [m for m, score in best_matches if score &gt; 50]\n    # find the original docs that corresponding to the matches\n    orig_doc_matches = []\n    for i, m in enumerate(real_matches):\n        for j, doc_clean in enumerate(docs_clean):\n            if m in doc_clean.content:\n                orig_doc_matches.append(docs[j])\n                break\n    if words_after is None and words_before is None:\n        return orig_doc_matches\n    if len(orig_doc_matches) == 0:\n        return []\n    if set(orig_doc_matches[0].__fields__) != {\"content\", \"metadata\"}:\n        # If there are fields beyond just content and metadata,\n        # we do NOT want to create new document objects with content fields\n        # based on words_before and words_after, since we don't know how to\n        # set those other fields.\n        return orig_doc_matches\n\n    contextual_matches = []\n    for match in orig_doc_matches:\n        choice_text = match.content\n        contexts = []\n        while choice_text != \"\":\n            context, start_pos, end_pos = get_context(\n                query, choice_text, words_before, words_after\n            )\n            if context == \"\" or end_pos == 0:\n                break\n            contexts.append(context)\n            words = choice_text.split()\n            end_pos = min(end_pos, len(words))\n            choice_text = \" \".join(words[end_pos:])\n        if len(contexts) &gt; 0:\n            contextual_matches.append(\n                Document(\n                    content=\" ... \".join(contexts),\n                    metadata=match.metadata,\n                )\n            )\n\n    return contextual_matches\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.preprocess_text","title":"<code>preprocess_text(text)</code>","text":"<p>Preprocesses the given text by: 1. Lowercasing all words. 2. Tokenizing (splitting the text into words). 3. Removing punctuation. 4. Removing stopwords. 5. Lemmatizing words.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The preprocessed text.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def preprocess_text(text: str) -&gt; str:\n    \"\"\"\n    Preprocesses the given text by:\n    1. Lowercasing all words.\n    2. Tokenizing (splitting the text into words).\n    3. Removing punctuation.\n    4. Removing stopwords.\n    5. Lemmatizing words.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        str: The preprocessed text.\n    \"\"\"\n    # Ensure the NLTK resources are available\n    for resource in [\"punkt\", \"wordnet\", \"stopwords\"]:\n        download_nltk_resource(resource)\n\n    # Lowercase the text\n    text = text.lower()\n\n    # Tokenize the text and remove punctuation\n    tokenizer = RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    tokens = [t for t in tokens if t not in stop_words]\n\n    # Lemmatize words\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n\n    # Join the words back into a string\n    text = \" \".join(tokens)\n\n    return text\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.find_closest_matches_with_bm25","title":"<code>find_closest_matches_with_bm25(docs, docs_clean, query, k=5)</code>","text":"<p>Finds the k closest approximate matches using the BM25 algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Documents to search through.</p> required <code>docs_clean</code> <code>List[Document]</code> <p>List of cleaned Documents</p> required <code>query</code> <code>str</code> <p>The search query.</p> required <code>k</code> <code>int</code> <p>Number of matches to retrieve. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def find_closest_matches_with_bm25(\n    docs: List[Document],\n    docs_clean: List[Document],\n    query: str,\n    k: int = 5,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Finds the k closest approximate matches using the BM25 algorithm.\n\n    Args:\n        docs (List[Document]): List of Documents to search through.\n        docs_clean (List[Document]): List of cleaned Documents\n        query (str): The search query.\n        k (int, optional): Number of matches to retrieve. Defaults to 5.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n    \"\"\"\n    if len(docs) == 0:\n        return []\n    texts = [doc.content for doc in docs_clean]\n    query = preprocess_text(query)\n\n    text_words = [text.split() for text in texts]\n\n    bm25 = BM25Okapi(text_words)\n    query_words = query.split()\n    doc_scores = bm25.get_scores(query_words)\n\n    # Get indices of top k scores\n    top_indices = sorted(range(len(doc_scores)), key=lambda i: -doc_scores[i])[:k]\n\n    # return the original docs, based on the scores from cleaned docs\n    return [(docs[i], doc_scores[i]) for i in top_indices]\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.get_context","title":"<code>get_context(query, text, words_before=100, words_after=100)</code>","text":"<p>Returns a portion of text containing the best approximate match of the query, including b words before and a words after the match.</p> <p>Args: query (str): The string to search for. text (str): The body of text in which to search. b (int): The number of words before the query to return. a (int): The number of words after the query to return.</p> <p>str: A string containing b words before, the match, and a words after     the best approximate match position of the query in the text. If no     match is found, returns empty string. int: The start position of the match in the text. int: The end position of the match in the text.</p> <p>Example:</p> <p>get_context(\"apple\", \"The quick brown fox jumps over the apple.\", 3, 2)</p>"},{"location":"reference/parsing/search/#langroid.parsing.search.get_context--fox-jumps-over-the-apple","title":"'fox jumps over the apple.'","text":"Source code in <code>langroid/parsing/search.py</code> <pre><code>def get_context(\n    query: str,\n    text: str,\n    words_before: int | None = 100,\n    words_after: int | None = 100,\n) -&gt; Tuple[str, int, int]:\n    \"\"\"\n    Returns a portion of text containing the best approximate match of the query,\n    including b words before and a words after the match.\n\n    Args:\n    query (str): The string to search for.\n    text (str): The body of text in which to search.\n    b (int): The number of words before the query to return.\n    a (int): The number of words after the query to return.\n\n    Returns:\n    str: A string containing b words before, the match, and a words after\n        the best approximate match position of the query in the text. If no\n        match is found, returns empty string.\n    int: The start position of the match in the text.\n    int: The end position of the match in the text.\n\n    Example:\n    &gt;&gt;&gt; get_context(\"apple\", \"The quick brown fox jumps over the apple.\", 3, 2)\n    # 'fox jumps over the apple.'\n    \"\"\"\n    if words_after is None and words_before is None:\n        # return entire text since we're not asked to return a bounded context\n        return text, 0, 0\n\n    # make sure there is a good enough match to the query\n    if fuzz.partial_ratio(query, text) &lt; 40:\n        return \"\", 0, 0\n\n    sequence_matcher = difflib.SequenceMatcher(None, text, query)\n    match = sequence_matcher.find_longest_match(0, len(text), 0, len(query))\n\n    if match.size == 0:\n        return \"\", 0, 0\n\n    segments = text.split()\n    n_segs = len(segments)\n\n    start_segment_pos = len(text[: match.a].split())\n\n    words_before = words_before or n_segs\n    words_after = words_after or n_segs\n    start_pos = max(0, start_segment_pos - words_before)\n    end_pos = min(len(segments), start_segment_pos + words_after + len(query.split()))\n\n    return \" \".join(segments[start_pos:end_pos]), start_pos, end_pos\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.eliminate_near_duplicates","title":"<code>eliminate_near_duplicates(passages, threshold=0.8)</code>","text":"<p>Eliminate near duplicate text passages from a given list using MinHash and LSH. TODO: this has not been tested and the datasketch lib is not a dependency. Args:     passages (List[str]): A list of text passages.     threshold (float, optional): Jaccard similarity threshold to consider two                                  passages as near-duplicates. Default is 0.8.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of passages after eliminating near duplicates.</p> Example <p>passages = [\"Hello world\", \"Hello, world!\", \"Hi there\", \"Hello world!\"] print(eliminate_near_duplicates(passages))</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def eliminate_near_duplicates(passages: List[str], threshold: float = 0.8) -&gt; List[str]:\n    \"\"\"\n    Eliminate near duplicate text passages from a given list using MinHash and LSH.\n    TODO: this has not been tested and the datasketch lib is not a dependency.\n    Args:\n        passages (List[str]): A list of text passages.\n        threshold (float, optional): Jaccard similarity threshold to consider two\n                                     passages as near-duplicates. Default is 0.8.\n\n    Returns:\n        List[str]: A list of passages after eliminating near duplicates.\n\n    Example:\n        passages = [\"Hello world\", \"Hello, world!\", \"Hi there\", \"Hello world!\"]\n        print(eliminate_near_duplicates(passages))\n        # ['Hello world', 'Hi there']\n    \"\"\"\n\n    from datasketch import MinHash, MinHashLSH\n\n    # Create LSH index\n    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n\n    # Create MinHash objects for each passage and insert to LSH\n    minhashes = {}\n    for idx, passage in enumerate(passages):\n        m = MinHash(num_perm=128)\n        for word in passage.split():\n            m.update(word.encode(\"utf-8\"))\n        lsh.insert(idx, m)\n        minhashes[idx] = m\n\n    unique_idxs = set()\n    for idx in minhashes.keys():\n        # Query for similar passages (including itself)\n        result = lsh.query(minhashes[idx])\n\n        # If only the passage itself is returned, it's unique\n        if len(result) == 1 and idx in result:\n            unique_idxs.add(idx)\n\n    return [passages[idx] for idx in unique_idxs]\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.eliminate_near_duplicates--hello-world-hi-there","title":"['Hello world', 'Hi there']","text":""},{"location":"reference/parsing/spider/","title":"spider","text":"<p>langroid/parsing/spider.py </p>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.DomainSpecificSpider","title":"<code>DomainSpecificSpider(start_url, k=20, *args, **kwargs)</code>","text":"<p>             Bases: <code>CrawlSpider</code></p> <p>Parameters:</p> Name Type Description Default <code>start_url</code> <code>str</code> <p>The starting URL.</p> required <code>k</code> <code>int</code> <p>The max desired final URLs. Defaults to 20.</p> <code>20</code> Source code in <code>langroid/parsing/spider.py</code> <pre><code>def __init__(self, start_url: str, k: int = 20, *args, **kwargs):  # type: ignore\n    \"\"\"Initialize the spider with start_url and k.\n\n    Args:\n        start_url (str): The starting URL.\n        k (int, optional): The max desired final URLs. Defaults to 20.\n    \"\"\"\n    super(DomainSpecificSpider, self).__init__(*args, **kwargs)\n    self.start_urls = [start_url]\n    self.allowed_domains = [urlparse(start_url).netloc]\n    self.k = k\n    self.visited_urls: Set[str] = set()\n</code></pre>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.DomainSpecificSpider.parse_item","title":"<code>parse_item(response)</code>","text":"<p>Extracts URLs that are within the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>The scrapy response object.</p> required Source code in <code>langroid/parsing/spider.py</code> <pre><code>def parse_item(self, response: Response):  # type: ignore\n    \"\"\"Extracts URLs that are within the same domain.\n\n    Args:\n        response: The scrapy response object.\n    \"\"\"\n    for link in LinkExtractor(allow_domains=self.allowed_domains).extract_links(\n        response\n    ):\n        if len(self.visited_urls) &lt; self.k:\n            self.visited_urls.add(link.url)\n            yield {\"url\": link.url}\n</code></pre>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.scrapy_fetch_urls","title":"<code>scrapy_fetch_urls(url, k=20)</code>","text":"<p>Fetches up to k URLs reachable from the input URL using Scrapy.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The starting URL.</p> required <code>k</code> <code>int</code> <p>The max desired final URLs. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of URLs within the same domain as the input URL.</p> Source code in <code>langroid/parsing/spider.py</code> <pre><code>@no_type_check\ndef scrapy_fetch_urls(url: str, k: int = 20) -&gt; List[str]:\n    \"\"\"Fetches up to k URLs reachable from the input URL using Scrapy.\n\n    Args:\n        url (str): The starting URL.\n        k (int, optional): The max desired final URLs. Defaults to 20.\n\n    Returns:\n        List[str]: List of URLs within the same domain as the input URL.\n    \"\"\"\n    urls = []\n\n    def _collect_urls(spider):\n        \"\"\"Handler for the spider_closed signal. Collects the visited URLs.\"\"\"\n        nonlocal urls\n        urls.extend(list(spider.visited_urls))\n\n    # Connect the spider_closed signal with our handler\n    dispatcher.connect(_collect_urls, signal=signals.spider_closed)\n\n    runner = CrawlerRunner(\n        {\n            \"USER_AGENT\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n        }\n    )\n\n    d = runner.crawl(DomainSpecificSpider, start_url=url, k=k)\n\n    # Block until crawling is done and then stop the reactor\n    crawl_deferred = defer.Deferred()\n\n    def _crawl_done(_):\n        reactor.stop()\n        crawl_deferred.callback(urls)\n\n    d.addBoth(_crawl_done)\n\n    # Start the reactor, it will stop once the crawl is done\n    reactor.run(installSignalHandlers=0)\n\n    # This will block until the deferred gets a result\n    return crawl_deferred.result\n</code></pre>"},{"location":"reference/parsing/table_loader/","title":"table_loader","text":"<p>langroid/parsing/table_loader.py </p>"},{"location":"reference/parsing/table_loader/#langroid.parsing.table_loader.read_tabular_data","title":"<code>read_tabular_data(path_or_url, sep=None)</code>","text":"<p>Reads tabular data from a file or URL and returns a pandas DataFrame. The separator is auto-detected if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_url</code> <code>str</code> <p>Path or URL to the file to be read.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from file or URL as a pandas DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data cannot be read or is misformatted.</p> Source code in <code>langroid/parsing/table_loader.py</code> <pre><code>def read_tabular_data(path_or_url: str, sep: None | str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads tabular data from a file or URL and returns a pandas DataFrame.\n    The separator is auto-detected if not specified.\n\n    Args:\n        path_or_url (str): Path or URL to the file to be read.\n\n    Returns:\n        pd.DataFrame: Data from file or URL as a pandas DataFrame.\n\n    Raises:\n        ValueError: If the data cannot be read or is misformatted.\n    \"\"\"\n    try:\n        if sep is None:\n            # Read the first few lines to guess the separator\n            with pd.io.common.get_handle(path_or_url, \"r\") as file_handler:\n                first_lines = \"\".join(file_handler.handle.readlines(5))\n                sep = Sniffer().sniff(first_lines).delimiter\n                # If it's a local file, reset to the beginning\n                if hasattr(file_handler.handle, \"seek\"):\n                    file_handler.handle.seek(0)\n\n        # Read the data\n\n        # get non-blank column names\n        with pd.io.common.get_handle(path_or_url, \"r\") as f:\n            header_line = f.handle.readline().strip()\n            valid_cols = [col for col in header_line.split(sep) if col]\n            valid_cols = [c.replace('\"', \"\").replace(\"'\", \"\") for c in valid_cols]\n            if hasattr(f.handle, \"seek\"):\n                f.handle.seek(0)\n\n        # use only those columns\n        data = pd.read_csv(path_or_url, sep=sep, usecols=valid_cols)\n        data.columns = data.columns.str.strip()  # e.g. \"  column 1  \" -&gt; \"column 1\"\n\n        return data\n\n    except Exception as e:\n        raise ValueError(\n            \"Unable to read data. \"\n            \"Please ensure it is correctly formatted. Error: \" + str(e)\n        )\n</code></pre>"},{"location":"reference/parsing/table_loader/#langroid.parsing.table_loader.describe_dataframe","title":"<code>describe_dataframe(df, filter_fields=[], n_vals=10)</code>","text":"<p>Generates a description of the columns in the dataframe, along with a listing of up to <code>n_vals</code> unique values for each column. Intended to be used to insert into an LLM context so it can generate appropriate queries or filters on the df.</p> <p>Args: df (pd.DataFrame): The dataframe to describe. filter_fields (list): A list of fields that can be used for filtering.     When non-empty, the values-list will be restricted to these. n_vals (int): How many unique values to show for each column.</p> <p>Returns: str: A description of the dataframe.</p> Source code in <code>langroid/parsing/table_loader.py</code> <pre><code>def describe_dataframe(\n    df: pd.DataFrame, filter_fields: List[str] = [], n_vals: int = 10\n) -&gt; str:\n    \"\"\"\n    Generates a description of the columns in the dataframe,\n    along with a listing of up to `n_vals` unique values for each column.\n    Intended to be used to insert into an LLM context so it can generate\n    appropriate queries or filters on the df.\n\n    Args:\n    df (pd.DataFrame): The dataframe to describe.\n    filter_fields (list): A list of fields that can be used for filtering.\n        When non-empty, the values-list will be restricted to these.\n    n_vals (int): How many unique values to show for each column.\n\n    Returns:\n    str: A description of the dataframe.\n    \"\"\"\n    description = []\n    for column in df.columns.to_list():\n        unique_values = df[column].dropna().unique()\n        unique_count = len(unique_values)\n        if column not in filter_fields:\n            values_desc = f\"{unique_count} unique values\"\n        else:\n            if unique_count &gt; n_vals:\n                displayed_values = unique_values[:n_vals]\n                more_count = unique_count - n_vals\n                values_desc = f\" Values - {displayed_values}, ... {more_count} more\"\n            else:\n                values_desc = f\" Values - {unique_values}\"\n        col_type = \"string\" if df[column].dtype == \"object\" else df[column].dtype\n        col_desc = f\"* {column} ({col_type}); {values_desc}\"\n        description.append(col_desc)\n\n    all_cols = \"\\n\".join(description)\n\n    return f\"\"\"\n        Name of each field, its type and unique values (up to {n_vals}):\n        {all_cols}\n        \"\"\"\n</code></pre>"},{"location":"reference/parsing/url_loader/","title":"url_loader","text":"<p>langroid/parsing/url_loader.py </p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.URLLoader","title":"<code>URLLoader(urls, parser=Parser(ParsingConfig()))</code>","text":"<p>Load a list of URLs and extract the text content. Alternative approaches could use <code>bs4</code> or <code>scrapy</code>.</p> <p>TODO - this currently does not handle cookie dialogs,  i.e. if there is a cookie pop-up, most/all of the extracted  content could be cookie policy text.  We could use <code>playwright</code> to simulate a user clicking  the \"accept\" button on the cookie dialog.</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, urls: List[str], parser: Parser = Parser(ParsingConfig())):\n    self.urls = urls\n    self.parser = parser\n</code></pre>"},{"location":"reference/parsing/urls/","title":"urls","text":"<p>langroid/parsing/urls.py </p>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.url_to_tempfile","title":"<code>url_to_tempfile(url)</code>","text":"<p>Fetch content from the given URL and save it to a temporary local file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the content to fetch.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the temporary file where the content is saved.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If there's any issue fetching the content.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def url_to_tempfile(url: str) -&gt; str:\n    \"\"\"\n    Fetch content from the given URL and save it to a temporary local file.\n\n    Args:\n        url (str): The URL of the content to fetch.\n\n    Returns:\n        str: The path to the temporary file where the content is saved.\n\n    Raises:\n        HTTPError: If there's any issue fetching the content.\n    \"\"\"\n\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Create a temporary file and write the content\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".tmp\") as temp_file:\n        temp_file.write(response.content)\n        return temp_file.name\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_user_input","title":"<code>get_user_input(msg, color='blue')</code>","text":"<p>Prompt the user for input. Args:     msg: printed prompt     color: color of the prompt Returns:     user input</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_user_input(msg: str, color: str = \"blue\") -&gt; str:\n    \"\"\"\n    Prompt the user for input.\n    Args:\n        msg: printed prompt\n        color: color of the prompt\n    Returns:\n        user input\n    \"\"\"\n    color_str = f\"[{color}]{msg} \" if color else msg + \" \"\n    print(color_str, end=\"\")\n    return input(\"\")\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_list_from_user","title":"<code>get_list_from_user(prompt=\"Enter input (type 'done' or hit return to finish)\", n=None)</code>","text":"<p>Prompt the user for inputs. Args:     prompt: printed prompt     n: how many inputs to prompt for. If None, then prompt until done, otherwise         quit after n inputs. Returns:     list of input strings</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_list_from_user(\n    prompt: str = \"Enter input (type 'done' or hit return to finish)\",\n    n: int | None = None,\n) -&gt; List[str]:\n    \"\"\"\n    Prompt the user for inputs.\n    Args:\n        prompt: printed prompt\n        n: how many inputs to prompt for. If None, then prompt until done, otherwise\n            quit after n inputs.\n    Returns:\n        list of input strings\n    \"\"\"\n    # Create an empty set to store the URLs.\n    input_set = set()\n\n    # Use a while loop to continuously ask the user for URLs.\n    for _ in range(n or 1000):\n        # Prompt the user for input.\n        input_str = Prompt.ask(f\"[blue]{prompt}\")\n\n        # Check if the user wants to exit the loop.\n        if input_str.lower() == \"done\" or input_str == \"\":\n            break\n\n        # if it is a URL, ask how many to crawl\n        if is_url(input_str):\n            url = input_str\n            input_str = Prompt.ask(\"[blue] How many new URLs to crawl?\", default=\"0\")\n            max_urls = int(input_str) + 1\n            tot_urls = list(find_urls(url, max_links=max_urls, max_depth=2))\n            tot_urls_str = \"\\n\".join(tot_urls)\n            print(\n                f\"\"\"\n                Found these {len(tot_urls)} links upto depth 2:\n                {tot_urls_str}\n                \"\"\"\n            )\n\n            input_set.update(tot_urls)\n        else:\n            input_set.add(input_str.strip())\n\n    return list(input_set)\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_urls_paths_bytes_indices","title":"<code>get_urls_paths_bytes_indices(inputs)</code>","text":"<p>Given a list of inputs, return a list of indices of URLs, list of indices of paths, list of indices of byte-contents. Args:     inputs: list of strings or bytes Returns:     list of Indices of URLs,     list of indices of paths,     list of indices of byte-contents</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_urls_paths_bytes_indices(\n    inputs: List[str | bytes],\n) -&gt; Tuple[List[int], List[int], List[int]]:\n    \"\"\"\n    Given a list of inputs, return a\n    list of indices of URLs, list of indices of paths, list of indices of byte-contents.\n    Args:\n        inputs: list of strings or bytes\n    Returns:\n        list of Indices of URLs,\n        list of indices of paths,\n        list of indices of byte-contents\n    \"\"\"\n    urls = []\n    paths = []\n    byte_list = []\n    for i, item in enumerate(inputs):\n        if isinstance(item, bytes):\n            byte_list.append(i)\n            continue\n        try:\n            Url(url=parse_obj_as(HttpUrl, item))\n            urls.append(i)\n        except ValidationError:\n            if os.path.exists(item):\n                paths.append(i)\n            else:\n                logger.warning(f\"{item} is neither a URL nor a path.\")\n    return urls, paths, byte_list\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.crawl_url","title":"<code>crawl_url(url, max_urls=1)</code>","text":"<p>Crawl starting at the url and return a list of URLs to be parsed, up to a maximum of <code>max_urls</code>. This has not been tested to work as intended. Ignore.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def crawl_url(url: str, max_urls: int = 1) -&gt; List[str]:\n    \"\"\"\n    Crawl starting at the url and return a list of URLs to be parsed,\n    up to a maximum of `max_urls`.\n    This has not been tested to work as intended. Ignore.\n    \"\"\"\n    if max_urls == 1:\n        # no need to crawl, just return the original list\n        return [url]\n\n    to_visit = None\n    known_urls = None\n\n    # Create a RobotFileParser object\n    robots = urllib.robotparser.RobotFileParser()\n    while True:\n        if known_urls is not None and len(known_urls) &gt;= max_urls:\n            break\n        # Set the RobotFileParser object to the website's robots.txt file\n        robots.set_url(url + \"/robots.txt\")\n        robots.read()\n\n        if robots.can_fetch(\"*\", url):\n            # Start or resume the crawl\n            to_visit, known_urls = focused_crawler(\n                url,\n                max_seen_urls=max_urls,\n                max_known_urls=max_urls,\n                todo=to_visit,\n                known_links=known_urls,\n                rules=robots,\n            )\n        if to_visit is None:\n            break\n\n    if known_urls is None:\n        return [url]\n    final_urls = [s.strip() for s in known_urls]\n    return list(final_urls)[:max_urls]\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.find_urls","title":"<code>find_urls(url='https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', max_links=20, visited=None, depth=0, max_depth=2, match_domain=True)</code>","text":"<p>Recursively find all URLs on a given page.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to start from.</p> <code>'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'</code> <code>max_links</code> <code>int</code> <p>The maximum number of links to find.</p> <code>20</code> <code>visited</code> <code>set</code> <p>A set of URLs that have already been visited.</p> <code>None</code> <code>depth</code> <code>int</code> <p>The current depth of the recursion.</p> <code>0</code> <code>max_depth</code> <code>int</code> <p>The maximum depth of the recursion.</p> <code>2</code> <code>match_domain</code> <code>bool</code> <p>Whether to only return URLs that are on the same domain.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>set</code> <code>Set[str]</code> <p>A set of URLs found on the page.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def find_urls(\n    url: str = \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\n    max_links: int = 20,\n    visited: Optional[Set[str]] = None,\n    depth: int = 0,\n    max_depth: int = 2,\n    match_domain: bool = True,\n) -&gt; Set[str]:\n    \"\"\"\n    Recursively find all URLs on a given page.\n\n    Args:\n        url (str): The URL to start from.\n        max_links (int): The maximum number of links to find.\n        visited (set): A set of URLs that have already been visited.\n        depth (int): The current depth of the recursion.\n        max_depth (int): The maximum depth of the recursion.\n        match_domain (bool): Whether to only return URLs that are on the same domain.\n\n    Returns:\n        set: A set of URLs found on the page.\n    \"\"\"\n\n    if visited is None:\n        visited = set()\n\n    if url in visited or depth &gt; max_depth:\n        return visited\n\n    visited.add(url)\n    base_domain = urlparse(url).netloc\n\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        links = [urljoin(url, a[\"href\"]) for a in soup.find_all(\"a\", href=True)]\n\n        # Defrag links: discard links that are to portions of same page\n        defragged_links = list(set(urldefrag(link).url for link in links))\n\n        # Filter links based on domain matching requirement\n        domain_matching_links = [\n            link for link in defragged_links if urlparse(link).netloc == base_domain\n        ]\n\n        # ensure url is first, since below we are taking first max_links urls\n        domain_matching_links = [url] + [x for x in domain_matching_links if x != url]\n\n        # If found links exceed max_links, return immediately\n        if len(domain_matching_links) &gt;= max_links:\n            return set(domain_matching_links[:max_links])\n\n        for link in domain_matching_links:\n            if len(visited) &gt;= max_links:\n                break\n\n            if link not in visited:\n                visited.update(\n                    find_urls(\n                        link,\n                        max_links,\n                        visited,\n                        depth + 1,\n                        max_depth,\n                        match_domain,\n                    )\n                )\n\n    except (requests.RequestException, Exception) as e:\n        print(f\"Error fetching {url}. Error: {e}\")\n\n    return set(list(visited)[:max_links])\n</code></pre>"},{"location":"reference/parsing/utils/","title":"utils","text":"<p>langroid/parsing/utils.py </p>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.batched","title":"<code>batched(iterable, n)</code>","text":"<p>Batch data into tuples of length n. The last batch may be shorter.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def batched(iterable: Iterable[T], n: int) -&gt; Iterable[Sequence[T]]:\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --&gt; ABC DEF G\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(islice(it, n)):\n        yield batch\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.closest_string","title":"<code>closest_string(query, string_list)</code>","text":"<p>Find the closest match to the query in a list of strings.</p> <p>This function is case-insensitive and ignores leading and trailing whitespace. If no match is found, it returns 'No match found'.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The string to match.</p> required <code>string_list</code> <code>List[str]</code> <p>The list of strings to search.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The closest match to the query from the list, or 'No match found'  if no match is found.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def closest_string(query: str, string_list: List[str]) -&gt; str:\n    \"\"\"Find the closest match to the query in a list of strings.\n\n    This function is case-insensitive and ignores leading and trailing whitespace.\n    If no match is found, it returns 'No match found'.\n\n    Args:\n        query (str): The string to match.\n        string_list (List[str]): The list of strings to search.\n\n    Returns:\n        str: The closest match to the query from the list, or 'No match found'\n             if no match is found.\n    \"\"\"\n    # Create a dictionary where the keys are the standardized strings and\n    # the values are the original strings.\n    str_dict = {s.lower().strip(): s for s in string_list}\n\n    # Standardize the query and find the closest match in the list of keys.\n    closest_match = difflib.get_close_matches(\n        query.lower().strip(), str_dict.keys(), n=1\n    )\n\n    # Retrieve the original string from the value in the dictionary.\n    original_closest_match = (\n        str_dict[closest_match[0]] if closest_match else \"No match found\"\n    )\n\n    return original_closest_match\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.split_paragraphs","title":"<code>split_paragraphs(text)</code>","text":"<pre><code>Split the input text into paragraphs using \"\n</code></pre> <p>\" as the delimiter.</p> <pre><code>Args:\n    text (str): The input text.\n\nReturns:\n    list: A list of paragraphs.\n</code></pre> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def split_paragraphs(text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into paragraphs using \"\\n\\n\" as the delimiter.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        list: A list of paragraphs.\n    \"\"\"\n    # Split based on a newline, followed by spaces/tabs, then another newline.\n    paras = re.split(r\"\\n[ \\t]*\\n\", text)\n    return [para.strip() for para in paras if para.strip()]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.split_newlines","title":"<code>split_newlines(text)</code>","text":"<pre><code>Split the input text into lines using \"\n</code></pre> <p>\" as the delimiter.</p> <pre><code>Args:\n    text (str): The input text.\n\nReturns:\n    list: A list of lines.\n</code></pre> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def split_newlines(text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into lines using \"\\n\" as the delimiter.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        list: A list of lines.\n    \"\"\"\n    lines = re.split(r\"\\n\", text)\n    return [line.strip() for line in lines if line.strip()]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.number_segments","title":"<code>number_segments(s, granularity=1)</code>","text":"<p>Number the segments in a given text, preserving paragraph structure. A segment is a sequence of <code>len</code> consecutive \"sentences\", where a \"sentence\" is either a normal sentence, or if there isn't enough punctuation to properly identify sentences, then we use a pseudo-sentence via heuristics (split by newline or failing that, just split every 40 words). The goal here is simply to number segments at a reasonable granularity so the LLM can identify relevant segments, in the RelevanceExtractorAgent.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text.</p> required <code>granularity</code> <code>int</code> <p>The number of sentences in a segment. If this is -1, then the entire text is treated as a single segment, and is numbered as &lt;#1#&gt;.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with segments numbered in the style &lt;#1#&gt;, &lt;#2#&gt; etc.</p> Example <p>number_segments(\"Hello world! How are you? Have a good day.\") '&lt;#1#&gt; Hello world! &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.'</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def number_segments(s: str, granularity: int = 1) -&gt; str:\n    \"\"\"\n    Number the segments in a given text, preserving paragraph structure.\n    A segment is a sequence of `len` consecutive \"sentences\", where a \"sentence\"\n    is either a normal sentence, or if there isn't enough punctuation to properly\n    identify sentences, then we use a pseudo-sentence via heuristics (split by newline\n    or failing that, just split every 40 words). The goal here is simply to number\n    segments at a reasonable granularity so the LLM can identify relevant segments,\n    in the RelevanceExtractorAgent.\n\n    Args:\n        s (str): The input text.\n        granularity (int): The number of sentences in a segment.\n            If this is -1, then the entire text is treated as a single segment,\n            and is numbered as &lt;#1#&gt;.\n\n    Returns:\n        str: The text with segments numbered in the style &lt;#1#&gt;, &lt;#2#&gt; etc.\n\n    Example:\n        &gt;&gt;&gt; number_segments(\"Hello world! How are you? Have a good day.\")\n        '&lt;#1#&gt; Hello world! &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.'\n    \"\"\"\n    if granularity &lt; 0:\n        return \"&lt;#1#&gt; \" + s\n    numbered_text = []\n    count = 0\n\n    paragraphs = split_paragraphs(s)\n    for paragraph in paragraphs:\n        sentences = nltk.sent_tokenize(paragraph)\n        # Some docs are problematic (e.g. resumes) and have no (or too few) periods,\n        # so we can't split usefully into sentences.\n        # We try a series of heuristics to split into sentences,\n        # until the avg num words per sentence is less than 40.\n        avg_words_per_sentence = sum(\n            len(nltk.word_tokenize(sentence)) for sentence in sentences\n        ) / len(sentences)\n        if avg_words_per_sentence &gt; 40:\n            sentences = split_newlines(paragraph)\n        avg_words_per_sentence = sum(\n            len(nltk.word_tokenize(sentence)) for sentence in sentences\n        ) / len(sentences)\n        if avg_words_per_sentence &gt; 40:\n            # Still too long, just split on every 40 words\n            sentences = []\n            for sentence in nltk.sent_tokenize(paragraph):\n                words = nltk.word_tokenize(sentence)\n                for i in range(0, len(words), 40):\n                    # if there are less than 20 words left after this,\n                    # just add them to the last sentence and break\n                    if len(words) - i &lt; 20:\n                        sentences.append(\" \".join(words[i:]))\n                        break\n                    else:\n                        sentences.append(\" \".join(words[i : i + 40]))\n        for i, sentence in enumerate(sentences):\n            num = count // granularity + 1\n            number_prefix = f\"&lt;#{num}#&gt;\" if count % granularity == 0 else \"\"\n            sentence = f\"{number_prefix} {sentence}\"\n            count += 1\n            sentences[i] = sentence\n        numbered_paragraph = \" \".join(sentences)\n        numbered_text.append(numbered_paragraph)\n\n    return \"  \\n\\n  \".join(numbered_text)\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.parse_number_range_list","title":"<code>parse_number_range_list(specs)</code>","text":"<p>Parse a specs string like \"3,5,7-10\" into a list of integers.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>str</code> <p>A string containing segment numbers and/or ranges          (e.g., \"3,5,7-10\").</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of segment numbers.</p> Example <p>parse_number_range_list(\"3,5,7-10\") [3, 5, 7, 8, 9, 10]</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def parse_number_range_list(specs: str) -&gt; List[int]:\n    \"\"\"\n    Parse a specs string like \"3,5,7-10\" into a list of integers.\n\n    Args:\n        specs (str): A string containing segment numbers and/or ranges\n                     (e.g., \"3,5,7-10\").\n\n    Returns:\n        List[int]: List of segment numbers.\n\n    Example:\n        &gt;&gt;&gt; parse_number_range_list(\"3,5,7-10\")\n        [3, 5, 7, 8, 9, 10]\n    \"\"\"\n    spec_indices = set()  # type: ignore\n    for part in specs.split(\",\"):\n        # some weak LLMs may generate &lt;#1#&gt; instead of 1, so extract just the digits\n        # or the \"-\"\n        part = \"\".join(char for char in part if char.isdigit() or char == \"-\")\n        if \"-\" in part:\n            start, end = map(int, part.split(\"-\"))\n            spec_indices.update(range(start, end + 1))\n        else:\n            spec_indices.add(int(part))\n\n    return sorted(list(spec_indices))\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.strip_k","title":"<code>strip_k(s, k=2)</code>","text":"<p>Strip any leading and trailing whitespaces from the input text beyond length k. This is useful for removing leading/trailing whitespaces from a text while preserving paragraph structure.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text.</p> required <code>k</code> <code>int</code> <p>The number of leading and trailing whitespaces to retain.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with leading and trailing whitespaces removed beyond length k.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def strip_k(s: str, k: int = 2) -&gt; str:\n    \"\"\"\n    Strip any leading and trailing whitespaces from the input text beyond length k.\n    This is useful for removing leading/trailing whitespaces from a text while\n    preserving paragraph structure.\n\n    Args:\n        s (str): The input text.\n        k (int): The number of leading and trailing whitespaces to retain.\n\n    Returns:\n        str: The text with leading and trailing whitespaces removed beyond length k.\n    \"\"\"\n\n    # Count leading and trailing whitespaces\n    leading_count = len(s) - len(s.lstrip())\n    trailing_count = len(s) - len(s.rstrip())\n\n    # Determine how many whitespaces to retain\n    leading_keep = min(leading_count, k)\n    trailing_keep = min(trailing_count, k)\n\n    # Use slicing to get the desired output\n    return s[leading_count - leading_keep : len(s) - (trailing_count - trailing_keep)]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.clean_whitespace","title":"<code>clean_whitespace(text)</code>","text":"<p>Remove extra whitespace from the input text, while preserving paragraph structure.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def clean_whitespace(text: str) -&gt; str:\n    \"\"\"Remove extra whitespace from the input text, while preserving\n    paragraph structure.\n    \"\"\"\n    paragraphs = split_paragraphs(text)\n    cleaned_paragraphs = [\" \".join(p.split()) for p in paragraphs if p]\n    return \"\\n\\n\".join(cleaned_paragraphs)  # Join the cleaned paragraphs.\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.extract_numbered_segments","title":"<code>extract_numbered_segments(s, specs)</code>","text":"<p>Extract specified segments from a numbered text, preserving paragraph structure.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text containing numbered segments.</p> required <code>specs</code> <code>str</code> <p>A string containing segment numbers and/or ranges          (e.g., \"3,5,7-10\").</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted segments, keeping original paragraph structures.</p> Example <p>text = \"(1) Hello world! (2) How are you? (3) Have a good day.\" extract_numbered_segments(text, \"1,3\") 'Hello world! Have a good day.'</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def extract_numbered_segments(s: str, specs: str) -&gt; str:\n    \"\"\"\n    Extract specified segments from a numbered text, preserving paragraph structure.\n\n    Args:\n        s (str): The input text containing numbered segments.\n        specs (str): A string containing segment numbers and/or ranges\n                     (e.g., \"3,5,7-10\").\n\n    Returns:\n        str: Extracted segments, keeping original paragraph structures.\n\n    Example:\n        &gt;&gt;&gt; text = \"(1) Hello world! (2) How are you? (3) Have a good day.\"\n        &gt;&gt;&gt; extract_numbered_segments(text, \"1,3\")\n        'Hello world! Have a good day.'\n    \"\"\"\n    # Use the helper function to get the list of indices from specs\n    if specs.strip() == \"\":\n        return \"\"\n    spec_indices = parse_number_range_list(specs)\n\n    # Regular expression to identify numbered segments like\n    # &lt;#1#&gt; Hello world! This is me. &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.\n    # Note we match any character between segment markers, including newlines.\n    segment_pattern = re.compile(r\"&lt;#(\\d+)#&gt;([\\s\\S]*?)(?=&lt;#\\d+#&gt;|$)\")\n\n    # Split the text into paragraphs while preserving their boundaries\n    paragraphs = split_paragraphs(s)\n\n    extracted_paragraphs = []\n\n    for paragraph in paragraphs:\n        segments_with_numbers = segment_pattern.findall(paragraph)\n\n        # Extract the desired segments from this paragraph\n        extracted_segments = [\n            segment\n            for num, segment in segments_with_numbers\n            if int(num) in spec_indices\n        ]\n\n        # If we extracted any segments from this paragraph,\n        # join them and append to results\n        if extracted_segments:\n            extracted_paragraphs.append(\" \".join(extracted_segments))\n\n    return \"\\n\\n\".join(extracted_paragraphs)\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.extract_content_from_path","title":"<code>extract_content_from_path(path, parsing, doc_type=None)</code>","text":"<p>Extract the content from a file path or URL, or a list of file paths or URLs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>bytes | str | List[str]</code> <p>The file path or URL, or a list of file paths or URLs, or bytes content. The bytes option is meant to support cases where upstream code may have already loaded the content (e.g., from a database or API) and we want to avoid having to copy the content to a temporary file.</p> required <code>parsing</code> <code>ParsingConfig</code> <p>The parsing configuration.</p> required <code>doc_type</code> <code>str | DocumentType | None</code> <p>The document type if known. If multiple paths are given, this MUST apply to ALL docs.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | List[str]</code> <p>str | List[str]: The extracted content if a single file path or URL is provided,     or a list of extracted contents if a     list of file paths or URLs is provided.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def extract_content_from_path(\n    path: bytes | str | List[bytes | str],\n    parsing: ParsingConfig,\n    doc_type: str | DocumentType | None = None,\n) -&gt; str | List[str]:\n    \"\"\"\n    Extract the content from a file path or URL, or a list of file paths or URLs.\n\n    Args:\n        path (bytes | str | List[str]): The file path or URL, or a list of file paths or\n            URLs, or bytes content. The bytes option is meant to support cases\n            where upstream code may have already loaded the content (e.g., from a\n            database or API) and we want to avoid having to copy the content to a\n            temporary file.\n        parsing (ParsingConfig): The parsing configuration.\n        doc_type (str | DocumentType | None): The document type if known.\n            If multiple paths are given, this MUST apply to ALL docs.\n\n    Returns:\n        str | List[str]: The extracted content if a single file path or URL is provided,\n                or a list of extracted contents if a\n                list of file paths or URLs is provided.\n    \"\"\"\n    if isinstance(path, str) or isinstance(path, bytes):\n        paths = [path]\n    elif isinstance(path, list) and len(path) == 0:\n        return \"\"\n    else:\n        paths = path\n\n    url_idxs, path_idxs, byte_idxs = get_urls_paths_bytes_indices(paths)\n    urls = [paths[i] for i in url_idxs]\n    path_list = [paths[i] for i in path_idxs]\n    byte_list = [paths[i] for i in byte_idxs]\n    path_list.extend(byte_list)\n    parser = Parser(parsing)\n    docs: List[Document] = []\n    try:\n        if len(urls) &gt; 0:\n            loader = URLLoader(urls=urls, parser=parser)  # type: ignore\n            docs = loader.load()\n        if len(path_list) &gt; 0:\n            for p in path_list:\n                path_docs = RepoLoader.get_documents(\n                    p, parser=parser, doc_type=doc_type\n                )\n                docs.extend(path_docs)\n    except Exception as e:\n        logger.warning(f\"Error loading path {paths}: {e}\")\n        return \"\"\n    if len(docs) == 1:\n        return docs[0].content\n    else:\n        return [d.content for d in docs]\n</code></pre>"},{"location":"reference/parsing/web_search/","title":"web_search","text":"<p>langroid/parsing/web_search.py </p> <p>Utilities for web search.</p> <p>NOTE: Using Google Search requires setting the GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables in your <code>.env</code> file, as explained in the README.</p>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.WebSearchResult","title":"<code>WebSearchResult(title, link, max_content_length=3500, max_summary_length=300)</code>","text":"<p>Class representing a Web Search result, containing the title, link, summary and full content of the result.</p> <pre><code>link (str): The link to the search result.\nmax_content_length (int): The maximum length of the full content.\nmax_summary_length (int): The maximum length of the summary.\n</code></pre> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def __init__(\n    self,\n    title: str,\n    link: str,\n    max_content_length: int = 3500,\n    max_summary_length: int = 300,\n):\n    \"\"\"\n    Args:\n        title (str): The title of the search result.\n        link (str): The link to the search result.\n        max_content_length (int): The maximum length of the full content.\n        max_summary_length (int): The maximum length of the summary.\n    \"\"\"\n    self.title = title\n    self.link = link\n    self.max_content_length = max_content_length\n    self.max_summary_length = max_summary_length\n    self.full_content = self.get_full_content()\n    self.summary = self.get_summary()\n</code></pre>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.metaphor_search","title":"<code>metaphor_search(query, num_results=5)</code>","text":"<p>Method that makes an API call by Metaphor client that queries the top num_results links that matches the query. Returns a list of WebSearchResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query body that users wants to make.</p> required <code>num_results</code> <code>int</code> <p>Number of top matching results that we want to grab</p> <code>5</code> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def metaphor_search(query: str, num_results: int = 5) -&gt; List[WebSearchResult]:\n    \"\"\"\n    Method that makes an API call by Metaphor client that queries\n    the top num_results links that matches the query. Returns a list\n    of WebSearchResult objects.\n\n    Args:\n        query (str): The query body that users wants to make.\n        num_results (int): Number of top matching results that we want\n            to grab\n    \"\"\"\n\n    load_dotenv()\n\n    api_key = os.getenv(\"METAPHOR_API_KEY\") or os.getenv(\"EXA_API_KEY\")\n    if not api_key:\n        raise ValueError(\n            \"\"\"\n            Neither METAPHOR_API_KEY nor EXA_API_KEY environment variables are set. \n            Please set one of them to your API key, and try again.\n            \"\"\"\n        )\n\n    try:\n        from metaphor_python import Metaphor\n    except ImportError:\n        raise ImportError(\n            \"You are attempting to use the `metaphor_python` library;\"\n            \"To use it, please install langroid with the `metaphor` extra, e.g. \"\n            \"`pip install langroid[metaphor]` or `poetry add langroid[metaphor]` \"\n            \"(it installs the `metaphor_python` package from pypi).\"\n        )\n\n    client = Metaphor(api_key=api_key)\n\n    response = client.search(\n        query=query,\n        num_results=num_results,\n    )\n    raw_results = response.results\n\n    return [\n        WebSearchResult(result.title, result.url, 3500, 300) for result in raw_results\n    ]\n</code></pre>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.duckduckgo_search","title":"<code>duckduckgo_search(query, num_results=5)</code>","text":"<p>Method that makes an API call by DuckDuckGo client that queries the top <code>num_results</code> links that matche the query. Returns a list of WebSearchResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query body that users wants to make.</p> required <code>num_results</code> <code>int</code> <p>Number of top matching results that we want to grab</p> <code>5</code> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def duckduckgo_search(query: str, num_results: int = 5) -&gt; List[WebSearchResult]:\n    \"\"\"\n    Method that makes an API call by DuckDuckGo client that queries\n    the top `num_results` links that matche the query. Returns a list\n    of WebSearchResult objects.\n\n    Args:\n        query (str): The query body that users wants to make.\n        num_results (int): Number of top matching results that we want\n            to grab\n    \"\"\"\n\n    with DDGS() as ddgs:\n        search_results = [r for r in ddgs.text(query, max_results=num_results)]\n\n    return [\n        WebSearchResult(\n            title=result[\"title\"],\n            link=result[\"href\"],\n            max_content_length=3500,\n            max_summary_length=300,\n        )\n        for result in search_results\n    ]\n</code></pre>"},{"location":"reference/prompts/","title":"prompts","text":"<p>langroid/prompts/init.py </p>"},{"location":"reference/prompts/dialog/","title":"dialog","text":"<p>langroid/prompts/dialog.py </p>"},{"location":"reference/prompts/dialog/#langroid.prompts.dialog.collate_chat_history","title":"<code>collate_chat_history(inputs)</code>","text":"<p>Collate (human, ai) pairs into a single, string Args:     inputs: Returns:</p> Source code in <code>langroid/prompts/dialog.py</code> <pre><code>def collate_chat_history(inputs: List[tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Collate (human, ai) pairs into a single, string\n    Args:\n        inputs:\n    Returns:\n    \"\"\"\n    pairs = [\n        f\"\"\"Human:{human}\n        AI:{ai}\n        \"\"\"\n        for human, ai in inputs\n    ]\n    return \"\\n\".join(pairs)\n</code></pre>"},{"location":"reference/prompts/prompts_config/","title":"prompts_config","text":"<p>langroid/prompts/prompts_config.py </p>"},{"location":"reference/prompts/templates/","title":"templates","text":"<p>langroid/prompts/templates.py </p>"},{"location":"reference/pydantic_v1/","title":"pydantic_v1","text":"<p>langroid/pydantic_v1/init.py </p> <p>If we're on Pydantic v2, use the v1 namespace, else just use the main namespace.</p> <p>This allows compatibility with both Pydantic v1 and v2</p>"},{"location":"reference/pydantic_v1/main/","title":"main","text":"<p>langroid/pydantic_v1/main.py </p>"},{"location":"reference/utils/","title":"utils","text":"<p>langroid/utils/init.py </p>"},{"location":"reference/utils/configuration/","title":"configuration","text":"<p>langroid/utils/configuration.py </p>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.update_global_settings","title":"<code>update_global_settings(cfg, keys)</code>","text":"<p>Update global settings so modules can access them via (as an example): <pre><code>from langroid.utils.configuration import settings\nif settings.debug...\n</code></pre> Caution we do not want to have too many such global settings! Args:     cfg: pydantic config, typically from a main script     keys: which keys from cfg to use, to update the global settings object</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def update_global_settings(cfg: BaseSettings, keys: List[str]) -&gt; None:\n    \"\"\"\n    Update global settings so modules can access them via (as an example):\n    ```\n    from langroid.utils.configuration import settings\n    if settings.debug...\n    ```\n    Caution we do not want to have too many such global settings!\n    Args:\n        cfg: pydantic config, typically from a main script\n        keys: which keys from cfg to use, to update the global settings object\n    \"\"\"\n    config_dict = cfg.dict()\n\n    # Filter the config_dict based on the keys\n    filtered_config = {key: config_dict[key] for key in keys if key in config_dict}\n\n    # create a new Settings() object to let pydantic validate it\n    new_settings = Settings(**filtered_config)\n\n    # Update the unique global settings object\n    settings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.set_global","title":"<code>set_global(key_vals)</code>","text":"<p>Update the unique global settings object</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def set_global(key_vals: Settings) -&gt; None:\n    \"\"\"Update the unique global settings object\"\"\"\n    settings.__dict__.update(key_vals.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.temporary_settings","title":"<code>temporary_settings(temp_settings)</code>","text":"<p>Temporarily update the global settings and restore them afterward.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>@contextmanager\ndef temporary_settings(temp_settings: Settings) -&gt; Iterator[None]:\n    \"\"\"Temporarily update the global settings and restore them afterward.\"\"\"\n    original_settings = copy.deepcopy(settings)\n\n    set_global(temp_settings)\n\n    try:\n        yield\n    finally:\n        settings.__dict__.update(original_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.quiet_mode","title":"<code>quiet_mode(quiet=True)</code>","text":"<p>Temporarily set quiet=True in global settings and restore afterward.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>@contextmanager\ndef quiet_mode(quiet: bool = True) -&gt; Iterator[None]:\n    \"\"\"Temporarily set quiet=True in global settings and restore afterward.\"\"\"\n    original_settings = copy.deepcopy(settings)\n    if quiet:\n        temp_settings = original_settings.copy(update={\"quiet\": True})\n        set_global(temp_settings)\n\n    try:\n        yield\n    finally:\n        if quiet:\n            settings.__dict__.update(original_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.set_env","title":"<code>set_env(settings)</code>","text":"<p>Set environment variables from a BaseSettings instance Args:     settings (BaseSettings): desired settings Returns:</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def set_env(settings: BaseSettings) -&gt; None:\n    \"\"\"\n    Set environment variables from a BaseSettings instance\n    Args:\n        settings (BaseSettings): desired settings\n    Returns:\n    \"\"\"\n    for field_name, field in settings.__class__.__fields__.items():\n        env_var_name = field.field_info.extra.get(\"env\", field_name).upper()\n        os.environ[env_var_name] = str(settings.dict()[field_name])\n</code></pre>"},{"location":"reference/utils/constants/","title":"constants","text":"<p>langroid/utils/constants.py </p>"},{"location":"reference/utils/globals/","title":"globals","text":"<p>langroid/utils/globals.py </p>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState","title":"<code>GlobalState</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A base Pydantic model for global states.</p>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Get the global instance of the specific subclass.</p> <p>Returns:</p> Type Description <code>GlobalState</code> <p>The global instance of the subclass.</p> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef get_instance(cls: Type[\"GlobalState\"]) -&gt; \"GlobalState\":\n    \"\"\"\n    Get the global instance of the specific subclass.\n\n    Returns:\n        The global instance of the subclass.\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = cls()\n    return cls._instance\n</code></pre>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.set_values","title":"<code>set_values(**kwargs)</code>  <code>classmethod</code>","text":"<p>Set values on the global instance of the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The fields and their values to set.</p> <code>{}</code> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef set_values(cls: Type[T], **kwargs: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Set values on the global instance of the specific subclass.\n\n    Args:\n        **kwargs: The fields and their values to set.\n    \"\"\"\n    instance = cls.get_instance()\n    for key, value in kwargs.items():\n        setattr(instance, key, value)\n</code></pre>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.get_value","title":"<code>get_value(name)</code>  <code>classmethod</code>","text":"<p>Retrieve the value of a specific field from the global instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field to retrieve.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>The value of the specified field.</p> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef get_value(cls: Type[T], name: str) -&gt; Any:\n    \"\"\"\n    Retrieve the value of a specific field from the global instance.\n\n    Args:\n        name (str): The name of the field to retrieve.\n\n    Returns:\n        str: The value of the specified field.\n    \"\"\"\n    instance = cls.get_instance()\n    return getattr(instance, name)\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>langroid/utils/logging.py </p>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_logger","title":"<code>setup_logger(name, level=logging.INFO, terminal=False)</code>","text":"<p>Set up a logger of module <code>name</code> at a desired level. Args:     name: module name     level: desired logging level Returns:     logger</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_logger(\n    name: str,\n    level: int = logging.INFO,\n    terminal: bool = False,\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up a logger of module `name` at a desired level.\n    Args:\n        name: module name\n        level: desired logging level\n    Returns:\n        logger\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    if not logger.hasHandlers() and terminal:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_loggers_for_package","title":"<code>setup_loggers_for_package(package_name, level)</code>","text":"<p>Set up loggers for all modules in a package. This ensures that log-levels of modules outside the package are not affected. Args:     package_name: main package name     level: desired logging level Returns:</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_loggers_for_package(package_name: str, level: int) -&gt; None:\n    \"\"\"\n    Set up loggers for all modules in a package.\n    This ensures that log-levels of modules outside the package are not affected.\n    Args:\n        package_name: main package name\n        level: desired logging level\n    Returns:\n    \"\"\"\n    import importlib\n    import pkgutil\n\n    package = importlib.import_module(package_name)\n    for _, module_name, _ in pkgutil.walk_packages(\n        package.__path__, package.__name__ + \".\"\n    ):\n        module = importlib.import_module(module_name)\n        setup_logger(module.__name__, level)\n</code></pre>"},{"location":"reference/utils/object_registry/","title":"object_registry","text":"<p>langroid/utils/object_registry.py </p>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry","title":"<code>ObjectRegistry</code>","text":"<p>A global registry to hold id -&gt; object mappings.</p>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.add","title":"<code>add(obj)</code>  <code>classmethod</code>","text":"<p>Adds an object to the registry, returning the object's ID.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef add(cls, obj: ObjWithId) -&gt; str:\n    \"\"\"Adds an object to the registry, returning the object's ID.\"\"\"\n    object_id = obj.id() if callable(obj.id) else obj.id\n    cls.registry[object_id] = obj\n    return object_id\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.get","title":"<code>get(obj_id)</code>  <code>classmethod</code>","text":"<p>Retrieves an object by ID if it still exists.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef get(cls, obj_id: str) -&gt; Optional[ObjWithId]:\n    \"\"\"Retrieves an object by ID if it still exists.\"\"\"\n    return cls.registry.get(obj_id)\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.register_object","title":"<code>register_object(obj)</code>  <code>classmethod</code>","text":"<p>Registers an object in the registry, returning the object's ID.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef register_object(cls, obj: ObjWithId) -&gt; str:\n    \"\"\"Registers an object in the registry, returning the object's ID.\"\"\"\n    return cls.add(obj)\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.remove","title":"<code>remove(obj_id)</code>  <code>classmethod</code>","text":"<p>Removes an object from the registry.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef remove(cls, obj_id: str) -&gt; None:\n    \"\"\"Removes an object from the registry.\"\"\"\n    if obj_id in cls.registry:\n        del cls.registry[obj_id]\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.cleanup","title":"<code>cleanup()</code>  <code>classmethod</code>","text":"<p>Cleans up the registry by removing entries where the object is None.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef cleanup(cls) -&gt; None:\n    \"\"\"Cleans up the registry by removing entries where the object is None.\"\"\"\n    to_remove = [key for key, value in cls.registry.items() if value is None]\n    for key in to_remove:\n        del cls.registry[key]\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.new_id","title":"<code>new_id()</code>  <code>staticmethod</code>","text":"<p>Generates a new unique ID.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@staticmethod\ndef new_id() -&gt; str:\n    \"\"\"Generates a new unique ID.\"\"\"\n    return str(uuid4())\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.scheduled_cleanup","title":"<code>scheduled_cleanup(interval=600)</code>","text":"<p>Periodically cleans up the global registry every 'interval' seconds.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>def scheduled_cleanup(interval: int = 600) -&gt; None:\n    \"\"\"Periodically cleans up the global registry every 'interval' seconds.\"\"\"\n    while True:\n        ObjectRegistry.cleanup()\n        time.sleep(interval)\n</code></pre>"},{"location":"reference/utils/pandas_utils/","title":"pandas_utils","text":"<p>langroid/utils/pandas_utils.py </p>"},{"location":"reference/utils/pydantic_utils/","title":"pydantic_utils","text":"<p>langroid/utils/pydantic_utils.py </p>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.has_field","title":"<code>has_field(model_class, field_name)</code>","text":"<p>Check if a Pydantic model class has a field with the given name.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def has_field(model_class: Type[BaseModel], field_name: str) -&gt; bool:\n    \"\"\"Check if a Pydantic model class has a field with the given name.\"\"\"\n    return field_name in model_class.__fields__\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_pydantic_model","title":"<code>flatten_pydantic_model(model, base_model=BaseModel)</code>","text":"<p>Given a possibly nested Pydantic class, return a flattened version of it, by constructing top-level fields, whose names are formed from the path through the nested structure, separated by double underscores.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to flatten.</p> required <code>base_model</code> <code>Type[BaseModel]</code> <p>The base model to use for the flattened model. Defaults to BaseModel.</p> <code>BaseModel</code> <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Type[BaseModel]: The flattened Pydantic model.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_pydantic_model(\n    model: Type[BaseModel],\n    base_model: Type[BaseModel] = BaseModel,\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Given a possibly nested Pydantic class, return a flattened version of it,\n    by constructing top-level fields, whose names are formed from the path\n    through the nested structure, separated by double underscores.\n\n    Args:\n        model (Type[BaseModel]): The Pydantic model to flatten.\n        base_model (Type[BaseModel], optional): The base model to use for the\n            flattened model. Defaults to BaseModel.\n\n    Returns:\n        Type[BaseModel]: The flattened Pydantic model.\n    \"\"\"\n\n    flattened_fields: Dict[str, Any] = {}\n    models_to_process = [(model, \"\")]\n\n    while models_to_process:\n        current_model, current_prefix = models_to_process.pop()\n\n        for name, field in current_model.__fields__.items():\n            if isinstance(field.outer_type_, type) and issubclass(\n                field.outer_type_, BaseModel\n            ):\n                new_prefix = (\n                    f\"{current_prefix}{name}__\" if current_prefix else f\"{name}__\"\n                )\n                models_to_process.append((field.outer_type_, new_prefix))\n            else:\n                flattened_name = f\"{current_prefix}{name}\"\n\n                if field.default_factory is not field.default_factory:\n                    flattened_fields[flattened_name] = (\n                        field.outer_type_,\n                        field.default_factory,\n                    )\n                elif field.default is not field.default:\n                    flattened_fields[flattened_name] = (\n                        field.outer_type_,\n                        field.default,\n                    )\n                else:\n                    flattened_fields[flattened_name] = (field.outer_type_, ...)\n\n    return create_model(\"FlatModel\", __base__=base_model, **flattened_fields)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.get_field_names","title":"<code>get_field_names(model)</code>","text":"<p>Get all field names from a possibly nested Pydantic model.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def get_field_names(model: Type[BaseModel]) -&gt; List[str]:\n    \"\"\"Get all field names from a possibly nested Pydantic model.\"\"\"\n    mdl = flatten_pydantic_model(model)\n    fields = list(mdl.__fields__.keys())\n    # fields may be like a__b__c , so we only want the last part\n    return [f.split(\"__\")[-1] for f in fields]\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.generate_simple_schema","title":"<code>generate_simple_schema(model, exclude=[])</code>","text":"<p>Generates a JSON schema for a Pydantic model, with options to exclude specific fields.</p> <p>This function traverses the Pydantic model's fields, including nested models, to generate a dictionary representing the JSON schema. Fields specified in the exclude list will not be included in the generated schema.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model class to generate the schema for.</p> required <code>exclude</code> <code>List[str]</code> <p>A list of string field names to be excluded from the                  generated schema. Defaults to an empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary representing the JSON schema of the provided model,             with specified fields excluded.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def generate_simple_schema(\n    model: Type[BaseModel], exclude: List[str] = []\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generates a JSON schema for a Pydantic model,\n    with options to exclude specific fields.\n\n    This function traverses the Pydantic model's fields, including nested models,\n    to generate a dictionary representing the JSON schema. Fields specified in\n    the exclude list will not be included in the generated schema.\n\n    Args:\n        model (Type[BaseModel]): The Pydantic model class to generate the schema for.\n        exclude (List[str]): A list of string field names to be excluded from the\n                             generated schema. Defaults to an empty list.\n\n    Returns:\n        Dict[str, Any]: A dictionary representing the JSON schema of the provided model,\n                        with specified fields excluded.\n    \"\"\"\n    if hasattr(model, \"__fields__\"):\n        output: Dict[str, Any] = {}\n        for field_name, field in model.__fields__.items():\n            if field_name in exclude:\n                continue  # Skip excluded fields\n\n            field_type = field.type_\n            if issubclass(field_type, BaseModel):\n                # Recursively generate schema for nested models\n                output[field_name] = generate_simple_schema(field_type, exclude)\n            else:\n                # Represent the type as a string here\n                output[field_name] = {\"type\": field_type.__name__}\n        return output\n    else:\n        # Non-model type, return a simplified representation\n        return {\"type\": model.__name__}\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_pydantic_instance","title":"<code>flatten_pydantic_instance(instance, prefix='', force_str=False)</code>","text":"<p>Given a possibly nested Pydantic instance, return a flattened version of it, as a dict where nested traversal paths are translated to keys a__b__c.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>BaseModel</code> <p>The Pydantic instance to flatten.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the top-level fields.</p> <code>''</code> <code>force_str</code> <code>bool</code> <p>Whether to force all values to be strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The flattened dict.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_pydantic_instance(\n    instance: BaseModel,\n    prefix: str = \"\",\n    force_str: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a possibly nested Pydantic instance, return a flattened version of it,\n    as a dict where nested traversal paths are translated to keys a__b__c.\n\n    Args:\n        instance (BaseModel): The Pydantic instance to flatten.\n        prefix (str, optional): The prefix to use for the top-level fields.\n        force_str (bool, optional): Whether to force all values to be strings.\n\n    Returns:\n        Dict[str, Any]: The flattened dict.\n\n    \"\"\"\n    flat_data: Dict[str, Any] = {}\n    for name, value in instance.dict().items():\n        # Assuming nested pydantic model will be a dict here\n        if isinstance(value, dict):\n            nested_flat_data = flatten_pydantic_instance(\n                instance.__fields__[name].type_(**value),\n                prefix=f\"{prefix}{name}__\",\n                force_str=force_str,\n            )\n            flat_data.update(nested_flat_data)\n        else:\n            flat_data[f\"{prefix}{name}\"] = str(value) if force_str else value\n    return flat_data\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.extract_fields","title":"<code>extract_fields(doc, fields)</code>","text":"<p>Extract specified fields from a Pydantic object. Supports dotted field names, e.g. \"metadata.author\". Dotted fields are matched exactly according to the corresponding path. Non-dotted fields are matched against the last part of the path. Clashes ignored. Args:     doc (BaseModel): The Pydantic object.     fields (List[str]): The list of fields to extract.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of field names and values.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def extract_fields(doc: BaseModel, fields: List[str]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract specified fields from a Pydantic object.\n    Supports dotted field names, e.g. \"metadata.author\".\n    Dotted fields are matched exactly according to the corresponding path.\n    Non-dotted fields are matched against the last part of the path.\n    Clashes ignored.\n    Args:\n        doc (BaseModel): The Pydantic object.\n        fields (List[str]): The list of fields to extract.\n\n    Returns:\n        Dict[str, Any]: A dictionary of field names and values.\n\n    \"\"\"\n\n    def get_value(obj: BaseModel, path: str) -&gt; Any | None:\n        for part in path.split(\".\"):\n            if hasattr(obj, part):\n                obj = getattr(obj, part)\n            else:\n                return None\n        return obj\n\n    def traverse(obj: BaseModel, result: Dict[str, Any], prefix: str = \"\") -&gt; None:\n        for k, v in obj.__dict__.items():\n            key = f\"{prefix}.{k}\" if prefix else k\n            if isinstance(v, BaseModel):\n                traverse(v, result, key)\n            else:\n                result[key] = v\n\n    result: Dict[str, Any] = {}\n\n    # Extract values for dotted field names and use last part as key\n    for field in fields:\n        if \".\" in field:\n            value = get_value(doc, field)\n            if value is not None:\n                key = field.split(\".\")[-1]\n                result[key] = value\n\n    # Traverse the object to get non-dotted fields\n    all_fields: Dict[str, Any] = {}\n    traverse(doc, all_fields)\n\n    # Add non-dotted fields to the result,\n    # avoid overwriting if already present from dotted names\n    for field in [f for f in fields if \".\" not in f]:\n        for key, value in all_fields.items():\n            if key.split(\".\")[-1] == field and field not in result:\n                result[field] = value\n\n    return result\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.nested_dict_from_flat","title":"<code>nested_dict_from_flat(flat_data, sub_dict='')</code>","text":"<p>Given a flattened version of a nested dict, reconstruct the nested dict. Field names in the flattened dict are assumed to be of the form \"field1__field2__field3\", going from top level down.</p> <p>Parameters:</p> Name Type Description Default <code>flat_data</code> <code>Dict[str, Any]</code> <p>The flattened dict.</p> required <code>sub_dict</code> <code>str</code> <p>The name of the sub-dict to extract from the flattened dict. Defaults to \"\" (extract the whole dict).</p> <code>''</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The nested dict.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def nested_dict_from_flat(\n    flat_data: Dict[str, Any],\n    sub_dict: str = \"\",\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a flattened version of a nested dict, reconstruct the nested dict.\n    Field names in the flattened dict are assumed to be of the form\n    \"field1__field2__field3\", going from top level down.\n\n    Args:\n        flat_data (Dict[str, Any]): The flattened dict.\n        sub_dict (str, optional): The name of the sub-dict to extract from the\n            flattened dict. Defaults to \"\" (extract the whole dict).\n\n    Returns:\n        Dict[str, Any]: The nested dict.\n\n    \"\"\"\n    nested_data: Dict[str, Any] = {}\n    for key, value in flat_data.items():\n        if sub_dict != \"\" and not key.startswith(sub_dict + \"__\"):\n            continue\n        keys = key.split(\"__\")\n        d = nested_data\n        for k in keys[:-1]:\n            d = d.setdefault(k, {})\n        d[keys[-1]] = value\n    if sub_dict != \"\":  # e.g. \"payload\"\n        nested_data = nested_data[sub_dict]\n    return nested_data\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.pydantic_obj_from_flat_dict","title":"<code>pydantic_obj_from_flat_dict(flat_data, model, sub_dict='')</code>","text":"<p>Flattened dict with a__b__c style keys -&gt; nested dict -&gt; pydantic object</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def pydantic_obj_from_flat_dict(\n    flat_data: Dict[str, Any],\n    model: Type[BaseModel],\n    sub_dict: str = \"\",\n) -&gt; BaseModel:\n    \"\"\"Flattened dict with a__b__c style keys -&gt; nested dict -&gt; pydantic object\"\"\"\n    nested_data = nested_dict_from_flat(flat_data, sub_dict)\n    return model(**nested_data)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.clean_schema","title":"<code>clean_schema(model, excludes=[])</code>","text":"<p>Generate a simple schema for a given Pydantic model, including inherited fields, with an option to exclude certain fields. Handles cases where fields are Lists or other generic types and includes field descriptions if available.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model class.</p> required <code>excludes</code> <code>List[str]</code> <p>A list of field names to exclude.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary representing the simple schema.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def clean_schema(model: Type[BaseModel], excludes: List[str] = []) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a simple schema for a given Pydantic model,\n    including inherited fields, with an option to exclude certain fields.\n    Handles cases where fields are Lists or other generic types and includes\n    field descriptions if available.\n\n    Args:\n        model (Type[BaseModel]): The Pydantic model class.\n        excludes (List[str]): A list of field names to exclude.\n\n    Returns:\n        Dict[str, Any]: A dictionary representing the simple schema.\n    \"\"\"\n    schema = {}\n\n    for field_name, field_info in model.__fields__.items():\n        if field_name in excludes:\n            continue\n\n        field_type = field_info.outer_type_\n        description = field_info.field_info.description or \"\"\n\n        # Handle generic types like List[...]\n        if get_origin(field_type):\n            inner_types = get_args(field_type)\n            inner_type_names = [\n                t.__name__ if hasattr(t, \"__name__\") else str(t) for t in inner_types\n            ]\n            field_type_str = (\n                f\"{get_origin(field_type).__name__}\" f'[{\", \".join(inner_type_names)}]'\n            )\n            schema[field_name] = {\"type\": field_type_str, \"description\": description}\n        elif issubclass(field_type, BaseModel):\n            # Directly use the nested model's schema,\n            # integrating it into the current level\n            nested_schema = clean_schema(field_type, excludes)\n            schema[field_name] = {**nested_schema, \"description\": description}\n        else:\n            # For basic types, use 'type'\n            schema[field_name] = {\n                \"type\": field_type.__name__,\n                \"description\": description,\n            }\n\n    return schema\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.temp_params","title":"<code>temp_params(config, field, temp)</code>","text":"<p>Context manager to temporarily override <code>field</code> in a <code>config</code></p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>@contextmanager\ndef temp_params(config: T, field: str, temp: T) -&gt; Generator[None, None, None]:\n    \"\"\"Context manager to temporarily override `field` in a `config`\"\"\"\n    original_vals = getattr(config, field)\n    try:\n        # Apply temporary settings\n        setattr(config, field, temp)\n        yield\n    finally:\n        # Revert to original settings\n        setattr(config, field, original_vals)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.numpy_to_python_type","title":"<code>numpy_to_python_type(numpy_type)</code>","text":"<p>Converts a numpy data type to its Python equivalent.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def numpy_to_python_type(numpy_type: Type[Any]) -&gt; Type[Any]:\n    \"\"\"Converts a numpy data type to its Python equivalent.\"\"\"\n    type_mapping = {\n        np.float64: float,\n        np.float32: float,\n        np.int64: int,\n        np.int32: int,\n        np.bool_: bool,\n        # Add other numpy types as necessary\n    }\n    return type_mapping.get(numpy_type, numpy_type)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_pydantic_model","title":"<code>dataframe_to_pydantic_model(df)</code>","text":"<p>Make a Pydantic model from a dataframe.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_pydantic_model(df: pd.DataFrame) -&gt; Type[BaseModel]:\n    \"\"\"Make a Pydantic model from a dataframe.\"\"\"\n    fields = {col: (type(df[col].iloc[0]), ...) for col in df.columns}\n    return create_model(\"DataFrameModel\", __base__=BaseModel, **fields)  # type: ignore\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_pydantic_objects","title":"<code>dataframe_to_pydantic_objects(df)</code>","text":"<p>Make a list of Pydantic objects from a dataframe.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_pydantic_objects(df: pd.DataFrame) -&gt; List[BaseModel]:\n    \"\"\"Make a list of Pydantic objects from a dataframe.\"\"\"\n    Model = dataframe_to_pydantic_model(df)\n    return [Model(**row.to_dict()) for index, row in df.iterrows()]\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.first_non_null","title":"<code>first_non_null(series)</code>","text":"<p>Find the first non-null item in a pandas Series.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def first_non_null(series: pd.Series) -&gt; Any | None:\n    \"\"\"Find the first non-null item in a pandas Series.\"\"\"\n    for item in series:\n        if item is not None:\n            return item\n    return None\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_document_model","title":"<code>dataframe_to_document_model(df, content='content', metadata=[], exclude=[])</code>","text":"<p>Make a subclass of Document from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe.</p> required <code>content</code> <code>str</code> <p>The name of the column containing the content, which will map to the Document.content field.</p> <code>'content'</code> <code>metadata</code> <code>List[str]</code> <p>A list of column names containing metadata; these will be included in the Document.metadata field.</p> <code>[]</code> <code>exclude</code> <code>List[str]</code> <p>A list of column names to exclude from the model. (e.g. \"vector\" when lance is used to add an embedding vector to the df)</p> <code>[]</code> <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Type[BaseModel]: A pydantic model subclassing Document.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_document_model(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n    exclude: List[str] = [],\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Make a subclass of Document from a dataframe.\n\n    Args:\n        df (pd.DataFrame): The dataframe.\n        content (str): The name of the column containing the content,\n            which will map to the Document.content field.\n        metadata (List[str]): A list of column names containing metadata;\n            these will be included in the Document.metadata field.\n        exclude (List[str]): A list of column names to exclude from the model.\n            (e.g. \"vector\" when lance is used to add an embedding vector to the df)\n\n    Returns:\n        Type[BaseModel]: A pydantic model subclassing Document.\n    \"\"\"\n\n    # Remove excluded columns\n    df = df.drop(columns=exclude, inplace=False)\n    # Check if metadata_cols is empty\n\n    if metadata:\n        # Define fields for the dynamic subclass of DocMetaData\n        metadata_fields = {\n            col: (\n                Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n                None,  # Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n            )\n            for col in metadata\n        }\n        DynamicMetaData = create_model(  # type: ignore\n            \"DynamicMetaData\", __base__=DocMetaData, **metadata_fields\n        )\n    else:\n        # Use the base DocMetaData class directly\n        DynamicMetaData = DocMetaData\n\n    # Define additional top-level fields for DynamicDocument\n    additional_fields = {\n        col: (\n            Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n            None,  # Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n        )\n        for col in df.columns\n        if col not in metadata and col != content\n    }\n\n    # Create a dynamic subclass of Document\n    DynamicDocumentFields = {\n        **{\"metadata\": (DynamicMetaData, ...)},\n        **additional_fields,\n    }\n    DynamicDocument = create_model(  # type: ignore\n        \"DynamicDocument\", __base__=Document, **DynamicDocumentFields\n    )\n\n    def from_df_row(\n        cls: type[BaseModel],\n        row: pd.Series,\n        content: str = \"content\",\n        metadata: List[str] = [],\n    ) -&gt; BaseModel | None:\n        content_val = row[content] if (content and content in row) else \"\"\n        metadata_values = (\n            {col: row[col] for col in metadata if col in row} if metadata else {}\n        )\n        additional_values = {\n            col: row[col] for col in additional_fields if col in row and col != content\n        }\n        metadata = DynamicMetaData(**metadata_values)\n        return cls(content=content_val, metadata=metadata, **additional_values)\n\n    # Bind the method to the class\n    DynamicDocument.from_df_row = classmethod(from_df_row)\n\n    return DynamicDocument  # type: ignore\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_documents","title":"<code>dataframe_to_documents(df, content='content', metadata=[], doc_cls=None)</code>","text":"<p>Make a list of Document objects from a dataframe. Args:     df (pd.DataFrame): The dataframe.     content (str): The name of the column containing the content,         which will map to the Document.content field.     metadata (List[str]): A list of column names containing metadata;         these will be included in the Document.metadata field.     doc_cls (Type[BaseModel], optional): A Pydantic model subclassing         Document. Defaults to None. Returns:     List[Document]: The list of Document objects.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_documents(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n    doc_cls: Type[BaseModel] | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Make a list of Document objects from a dataframe.\n    Args:\n        df (pd.DataFrame): The dataframe.\n        content (str): The name of the column containing the content,\n            which will map to the Document.content field.\n        metadata (List[str]): A list of column names containing metadata;\n            these will be included in the Document.metadata field.\n        doc_cls (Type[BaseModel], optional): A Pydantic model subclassing\n            Document. Defaults to None.\n    Returns:\n        List[Document]: The list of Document objects.\n    \"\"\"\n    Model = doc_cls or dataframe_to_document_model(df, content, metadata)\n    docs = [\n        Model.from_df_row(row, content, metadata)  # type: ignore\n        for _, row in df.iterrows()\n    ]\n    return [m for m in docs if m is not None]\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.extra_metadata","title":"<code>extra_metadata(document, doc_cls=Document)</code>","text":"<p>Checks for extra fields in a document's metadata that are not defined in the original metadata schema.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>The document instance to check for extra fields.</p> required <code>doc_cls</code> <code>Type[Document]</code> <p>The class type derived from Document, used as a reference to identify extra fields in the document's metadata.</p> <code>Document</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings representing the keys of the extra fields found</p> <code>List[str]</code> <p>in the document's metadata.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def extra_metadata(document: Document, doc_cls: Type[Document] = Document) -&gt; List[str]:\n    \"\"\"\n    Checks for extra fields in a document's metadata that are not defined in the\n    original metadata schema.\n\n    Args:\n        document (Document): The document instance to check for extra fields.\n        doc_cls (Type[Document]): The class type derived from Document, used\n            as a reference to identify extra fields in the document's metadata.\n\n    Returns:\n        List[str]: A list of strings representing the keys of the extra fields found\n        in the document's metadata.\n    \"\"\"\n    # Convert metadata to dict, including extra fields.\n    metadata_fields = set(document.metadata.dict().keys())\n\n    # Get defined fields in the metadata of doc_cls\n    defined_fields = set(doc_cls.__fields__[\"metadata\"].type_.__fields__.keys())\n\n    # Identify extra fields not in defined fields.\n    extra_fields = list(metadata_fields - defined_fields)\n\n    return extra_fields\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.extend_document_class","title":"<code>extend_document_class(d)</code>","text":"<p>Generates a new pydantic class based on a given document instance.</p> <p>This function dynamically creates a new pydantic class with additional fields based on the \"extra\" metadata fields present in the given document instance. The new class is a subclass of the original Document class, with the original metadata fields retained and extra fields added as normal fields to the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Document</code> <p>An instance of the Document class.</p> required <p>Returns:</p> Type Description <code>Type[Document]</code> <p>A new subclass of the Document class that includes the additional fields</p> <code>Type[Document]</code> <p>found in the metadata of the given document instance.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def extend_document_class(d: Document) -&gt; Type[Document]:\n    \"\"\"Generates a new pydantic class based on a given document instance.\n\n    This function dynamically creates a new pydantic class with additional\n    fields based on the \"extra\" metadata fields present in the given document\n    instance. The new class is a subclass of the original Document class, with\n    the original metadata fields retained and extra fields added as normal\n    fields to the metadata.\n\n    Args:\n        d: An instance of the Document class.\n\n    Returns:\n        A new subclass of the Document class that includes the additional fields\n        found in the metadata of the given document instance.\n    \"\"\"\n    # Extract the fields from the original metadata class, including types,\n    # correctly handling special types like List[str].\n    original_metadata_fields = {\n        k: (v.outer_type_ if v.shape != 1 else v.type_, ...)\n        for k, v in DocMetaData.__fields__.items()\n    }\n    # Extract extra fields from the metadata instance with their types\n    extra_fields = {\n        k: (type(v), ...)\n        for k, v in d.metadata.__dict__.items()\n        if k not in DocMetaData.__fields__\n    }\n\n    # Combine original and extra fields for the new metadata class\n    combined_fields = {**original_metadata_fields, **extra_fields}\n\n    # Create a new metadata class with combined fields\n    NewMetadataClass = create_model(  # type: ignore\n        \"ExtendedDocMetadata\", **combined_fields, __base__=DocMetaData\n    )\n    # NewMetadataClass.__config__.arbitrary_types_allowed = True\n\n    # Create a new document class using the new metadata class\n    NewDocumentClass = create_model(\n        \"ExtendedDocument\",\n        content=(str, ...),\n        metadata=(NewMetadataClass, ...),\n        __base__=Document,\n    )\n\n    return NewDocumentClass\n</code></pre>"},{"location":"reference/utils/system/","title":"system","text":"<p>langroid/utils/system.py </p>"},{"location":"reference/utils/system/#langroid.utils.system.LazyLoad","title":"<code>LazyLoad(import_path)</code>","text":"<p>Lazy loading of modules or classes.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def __init__(self, import_path: str) -&gt; None:\n    self.import_path = import_path\n    self._target = None\n    self._is_target_loaded = False\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.rmdir","title":"<code>rmdir(path)</code>","text":"<p>Remove a directory recursively. Args:     path (str): path to directory to remove Returns:     True if a dir was removed, false otherwise. Raises error if failed to remove.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def rmdir(path: str) -&gt; bool:\n    \"\"\"\n    Remove a directory recursively.\n    Args:\n        path (str): path to directory to remove\n    Returns:\n        True if a dir was removed, false otherwise. Raises error if failed to remove.\n    \"\"\"\n    if not any([path.startswith(p) for p in DELETION_ALLOWED_PATHS]):\n        raise ValueError(\n            f\"\"\"\n        Removing Dir '{path}' not allowed. \n        Must start with one of {DELETION_ALLOWED_PATHS}\n        This is a safety measure to prevent accidental deletion of files.\n        If you are sure you want to delete this directory, please add it \n        to the `DELETION_ALLOWED_PATHS` list in langroid/utils/system.py and \n        re-run the command.\n        \"\"\"\n        )\n\n    try:\n        shutil.rmtree(path)\n    except FileNotFoundError:\n        logger.warning(f\"Directory '{path}' does not exist. No action taken.\")\n        return False\n    except Exception as e:\n        logger.error(f\"Error while removing directory '{path}': {e}\")\n    return True\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.caller_name","title":"<code>caller_name()</code>","text":"<p>Who called the function?</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def caller_name() -&gt; str:\n    \"\"\"\n    Who called the function?\n    \"\"\"\n    frame = inspect.currentframe()\n    if frame is None:\n        return \"\"\n\n    caller_frame = frame.f_back\n\n    # If there's no caller frame, the function was called from the global scope\n    if caller_frame is None:\n        return \"\"\n\n    return caller_frame.f_code.co_name\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.generate_user_id","title":"<code>generate_user_id(org='')</code>","text":"<p>Generate a unique user ID based on the username and machine name. Returns:</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def generate_user_id(org: str = \"\") -&gt; str:\n    \"\"\"\n    Generate a unique user ID based on the username and machine name.\n    Returns:\n    \"\"\"\n    # Get the username\n    username = getpass.getuser()\n\n    # Get the machine's name\n    machine_name = socket.gethostname()\n\n    org_pfx = f\"{org}_\" if org else \"\"\n\n    # Create a consistent unique ID based on the username and machine name\n    unique_string = f\"{org_pfx}{username}@{machine_name}\"\n\n    # Generate a SHA-256 hash of the unique string\n    user_id = hashlib.sha256(unique_string.encode()).hexdigest()\n\n    return user_id\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.update_hash","title":"<code>update_hash(hash=None, s='')</code>","text":"<p>Takes a SHA256 hash string and a new string, updates the hash with the new string, and returns the updated hash string.</p> <p>Parameters:</p> Name Type Description Default <code>hash</code> <code>str</code> <p>A SHA256 hash string.</p> <code>None</code> <code>s</code> <code>str</code> <p>A new string to update the hash with.</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The updated hash in hexadecimal format.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def update_hash(hash: str | None = None, s: str = \"\") -&gt; str:\n    \"\"\"\n    Takes a SHA256 hash string and a new string, updates the hash with the new string,\n    and returns the updated hash string.\n\n    Args:\n        hash (str): A SHA256 hash string.\n        s (str): A new string to update the hash with.\n\n    Returns:\n        The updated hash in hexadecimal format.\n    \"\"\"\n    # Create a new hash object if no hash is provided\n    if hash is None:\n        hash_obj = hashlib.sha256()\n    else:\n        # Convert the hexadecimal hash string to a byte object\n        hash_bytes = bytes.fromhex(hash)\n        hash_obj = hashlib.sha256(hash_bytes)\n\n    # Update the hash with the new string\n    hash_obj.update(s.encode(\"utf-8\"))\n\n    # Return the updated hash in hexadecimal format and the original string\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.hash","title":"<code>hash(s)</code>","text":"<p>Generate a SHA256 hash of a string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to hash.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The SHA256 hash of the string.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def hash(s: str) -&gt; str:\n    \"\"\"\n    Generate a SHA256 hash of a string.\n\n    Args:\n        s (str): The string to hash.\n\n    Returns:\n        str: The SHA256 hash of the string.\n    \"\"\"\n    return update_hash(s=s)\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.generate_unique_id","title":"<code>generate_unique_id()</code>","text":"<p>Generate a unique ID using UUID4.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def generate_unique_id() -&gt; str:\n    \"\"\"Generate a unique ID using UUID4.\"\"\"\n    return str(uuid.uuid4())\n</code></pre>"},{"location":"reference/utils/algorithms/","title":"algorithms","text":"<p>langroid/utils/algorithms/init.py </p>"},{"location":"reference/utils/algorithms/graph/","title":"graph","text":"<p>langroid/utils/algorithms/graph.py </p> <p>Graph algos.</p>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.topological_sort","title":"<code>topological_sort(order)</code>","text":"<p>Given a directed adjacency matrix, return a topological sort of the nodes. order[i,j] = -1 means there is an edge from i to j. order[i,j] = 0 means there is no edge from i to j. order[i,j] = 1 means there is an edge from j to i.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>array</code> <p>The adjacency matrix.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: The topological sort of the nodes.</p> Source code in <code>langroid/utils/algorithms/graph.py</code> <pre><code>@no_type_check\ndef topological_sort(order: np.array) -&gt; List[int]:\n    \"\"\"\n    Given a directed adjacency matrix, return a topological sort of the nodes.\n    order[i,j] = -1 means there is an edge from i to j.\n    order[i,j] = 0 means there is no edge from i to j.\n    order[i,j] = 1 means there is an edge from j to i.\n\n    Args:\n        order (np.array): The adjacency matrix.\n\n    Returns:\n        List[int]: The topological sort of the nodes.\n\n    \"\"\"\n    n = order.shape[0]\n\n    # Calculate the in-degrees\n    in_degree = [0] * n\n    for i in range(n):\n        for j in range(n):\n            if order[i, j] == -1:\n                in_degree[j] += 1\n\n    # Initialize the queue with nodes of in-degree 0\n    queue = [i for i in range(n) if in_degree[i] == 0]\n    result = []\n\n    while queue:\n        node = queue.pop(0)\n        result.append(node)\n\n        for i in range(n):\n            if order[node, i] == -1:\n                in_degree[i] -= 1\n                if in_degree[i] == 0:\n                    queue.append(i)\n\n    assert len(result) == n, \"Cycle detected\"\n    return result\n</code></pre>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.components","title":"<code>components(order)</code>","text":"<p>Find the connected components in an undirected graph represented by a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>ndarray</code> <p>A matrix with values 0 or 1 indicating undirected graph edges. <code>order[i][j] = 1</code> means an edge between <code>i</code> and <code>j</code>, and <code>0</code> means no edge.</p> required <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int]]: A list of List where each List contains the indices of nodes in the same connected component.</p> Example <p>order = np.array([     [1, 1, 0, 0],     [1, 1, 1, 0],     [0, 1, 1, 0],     [0, 0, 0, 1] ]) components(order)</p> Source code in <code>langroid/utils/algorithms/graph.py</code> <pre><code>@no_type_check\ndef components(order: np.ndarray) -&gt; List[List[int]]:\n    \"\"\"\n    Find the connected components in an undirected graph represented by a matrix.\n\n    Args:\n        order (np.ndarray): A matrix with values 0 or 1 indicating\n            undirected graph edges. `order[i][j] = 1` means an edge between `i`\n            and `j`, and `0` means no edge.\n\n    Returns:\n        List[List[int]]: A list of List where each List contains the indices of\n            nodes in the same connected component.\n\n    Example:\n        order = np.array([\n            [1, 1, 0, 0],\n            [1, 1, 1, 0],\n            [0, 1, 1, 0],\n            [0, 0, 0, 1]\n        ])\n        components(order)\n        # [[0, 1, 2], [3]]\n    \"\"\"\n\n    i2g: Dict[int, int] = {}  # index to group mapping\n    next_group = 0\n    n = order.shape[0]\n    for i in range(n):\n        connected_groups = {i2g[j] for j in np.nonzero(order[i, :])[0] if j in i2g}\n\n        # If the node is not part of any group\n        # and is not connected to any groups, assign a new group\n        if not connected_groups:\n            i2g[i] = next_group\n            next_group += 1\n        else:\n            # If the node is connected to multiple groups, we merge them\n            main_group = min(connected_groups)\n            for j in np.nonzero(order[i, :])[0]:\n                if i2g.get(j) in connected_groups:\n                    i2g[j] = main_group\n            i2g[i] = main_group\n\n    # Convert i2g to a list of Lists\n    groups: Dict[int, List[int]] = {}\n    for index, group in i2g.items():\n        if group not in groups:\n            groups[group] = []\n        groups[group].append(index)\n\n    return list(groups.values())\n</code></pre>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.components--0-1-2-3","title":"[[0, 1, 2], [3]]","text":""},{"location":"reference/utils/output/","title":"output","text":"<p>langroid/utils/output/init.py </p>"},{"location":"reference/utils/output/#langroid.utils.output.PrintColored","title":"<code>PrintColored(color)</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>def __init__(self, color: str):\n    self.color = color\n</code></pre>"},{"location":"reference/utils/output/citations/","title":"citations","text":"<p>langroid/utils/output/citations.py </p>"},{"location":"reference/utils/output/citations/#langroid.utils.output.citations.extract_markdown_references","title":"<code>extract_markdown_references(md_string)</code>","text":"<p>Extracts markdown references (e.g., [^1], [^2]) from a string and returns them as a sorted list of integers.</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>The markdown string containing references.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: A sorted list of unique integers from the markdown references.</p> Source code in <code>langroid/utils/output/citations.py</code> <pre><code>def extract_markdown_references(md_string: str) -&gt; list[int]:\n    \"\"\"\n    Extracts markdown references (e.g., [^1], [^2]) from a string and returns\n    them as a sorted list of integers.\n\n    Args:\n        md_string (str): The markdown string containing references.\n\n    Returns:\n        list[int]: A sorted list of unique integers from the markdown references.\n    \"\"\"\n    import re\n\n    # Regex to find all occurrences of [^&lt;number&gt;]\n    matches = re.findall(r\"\\[\\^(\\d+)\\]\", md_string)\n    # Convert matches to integers, remove duplicates with set, and sort\n    return sorted(set(int(match) for match in matches))\n</code></pre>"},{"location":"reference/utils/output/citations/#langroid.utils.output.citations.format_footnote_text","title":"<code>format_footnote_text(content, width=80)</code>","text":"<p>Formats the content part of a footnote (i.e. not the first line that appears right after the reference [^4]) It wraps the text so that no line is longer than the specified width and indents lines as necessary for markdown footnotes.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text of the footnote to be formatted.</p> required <code>width</code> <code>int</code> <p>Maximum width of the text lines.</p> <code>80</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Properly formatted markdown footnote text.</p> Source code in <code>langroid/utils/output/citations.py</code> <pre><code>def format_footnote_text(content: str, width: int = 80) -&gt; str:\n    \"\"\"\n    Formats the content part of a footnote (i.e. not the first line that\n    appears right after the reference [^4])\n    It wraps the text so that no line is longer than the specified width and indents\n    lines as necessary for markdown footnotes.\n\n    Args:\n        content (str): The text of the footnote to be formatted.\n        width (int): Maximum width of the text lines.\n\n    Returns:\n        str: Properly formatted markdown footnote text.\n    \"\"\"\n    import textwrap\n\n    # Wrap the text to the specified width\n    wrapped_lines = textwrap.wrap(content, width)\n    if len(wrapped_lines) == 0:\n        return \"\"\n    indent = \"    \"  # Indentation for markdown footnotes\n    return indent + (\"\\n\" + indent).join(wrapped_lines)\n</code></pre>"},{"location":"reference/utils/output/printing/","title":"printing","text":"<p>langroid/utils/output/printing.py </p>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.PrintColored","title":"<code>PrintColored(color)</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>def __init__(self, color: str):\n    self.color = color\n</code></pre>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.silence_stdout","title":"<code>silence_stdout()</code>","text":"<p>Temporarily silence all output to stdout and from rich.print.</p> <p>This context manager redirects all output written to stdout (which includes outputs from the built-in print function and rich.print) to /dev/null on UNIX-like systems or NUL on Windows. Once the context block exits, stdout is restored to its original state.</p> Example <p>with silence_stdout_and_rich():     print(\"This won't be printed\")     rich.print(\"This also won't be printed\")</p> Note <p>This suppresses both standard print functions and the rich library outputs.</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>@contextmanager\ndef silence_stdout() -&gt; Iterator[None]:\n    \"\"\"\n    Temporarily silence all output to stdout and from rich.print.\n\n    This context manager redirects all output written to stdout (which includes\n    outputs from the built-in print function and rich.print) to /dev/null on\n    UNIX-like systems or NUL on Windows. Once the context block exits, stdout is\n    restored to its original state.\n\n    Example:\n        with silence_stdout_and_rich():\n            print(\"This won't be printed\")\n            rich.print(\"This also won't be printed\")\n\n    Note:\n        This suppresses both standard print functions and the rich library outputs.\n    \"\"\"\n    platform_null = \"/dev/null\" if sys.platform != \"win32\" else \"NUL\"\n    original_stdout = sys.stdout\n    fnull = open(platform_null, \"w\")\n    sys.stdout = fnull\n    try:\n        yield\n    finally:\n        sys.stdout = original_stdout\n        fnull.close()\n</code></pre>"},{"location":"reference/utils/output/status/","title":"status","text":"<p>langroid/utils/output/status.py </p>"},{"location":"reference/utils/output/status/#langroid.utils.output.status.status","title":"<code>status(msg, log_if_quiet=True)</code>","text":"<p>Displays a rich spinner if not in quiet mode, else optionally logs the message.</p> Source code in <code>langroid/utils/output/status.py</code> <pre><code>def status(\n    msg: str,\n    log_if_quiet: bool = True,\n) -&gt; AbstractContextManager[Any]:\n    \"\"\"\n    Displays a rich spinner if not in quiet mode, else optionally logs the message.\n    \"\"\"\n    stack = ExitStack()\n    logged = False\n    if settings.quiet and log_if_quiet:\n        logged = True\n        logger.info(msg)\n\n    if not settings.quiet:\n        try:\n            stack.enter_context(console.status(msg))\n        except LiveError:\n            if not logged:\n                logger.info(msg)\n\n    # When using rich spinner, we enforce quiet mode\n    # (since output will be messy otherwise);\n    # We make an exception to this when debug is enabled.\n    stack.enter_context(quiet_mode(not settings.debug))\n\n    return stack\n</code></pre>"},{"location":"reference/vector_store/","title":"vector_store","text":"<p>langroid/vector_store/init.py </p>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore","title":"<code>VectorStore(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def __init__(self, config: VectorStoreConfig):\n    self.config = config\n    self.embedding_model = EmbeddingModel.create(config.embedding)\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n    \"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>  <code>abstractmethod</code>","text":"<p>Clear all collections in the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>really</code> <code>bool</code> <p>Whether to really clear all collections. Defaults to False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>Prefix of collections to clear.</p> <code>''</code> <p>Returns:     int: Number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"\n    Clear all collections in the vector store.\n\n    Args:\n        really (bool, optional): Whether to really clear all collections.\n            Defaults to False.\n        prefix (str, optional): Prefix of collections to clear.\n    Returns:\n        int: Number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.list_collections","title":"<code>list_collections(empty=False)</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store (only non empty collections if empty=False).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"List all collections in the vector store\n    (only non empty collections if empty=False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the collection if it         already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\n\n    self.config.collection_name = collection_name\n    if collection_name not in self.list_collections() or replace:\n        self.create_collection(collection_name, replace=replace)\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the         collection if it already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.compute_from_docs","title":"<code>compute_from_docs(docs, calc)</code>","text":"<p>Compute a result on a set of documents, using a dataframe calc string like <code>df.groupby('state')['income'].mean()</code>.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def compute_from_docs(self, docs: List[Document], calc: str) -&gt; str:\n    \"\"\"Compute a result on a set of documents,\n    using a dataframe calc string like `df.groupby('state')['income'].mean()`.\n    \"\"\"\n    dicts = [doc.dict() for doc in docs]\n    df = pd.DataFrame(dicts)\n\n    try:\n        result = pd.eval(  # safer than eval but limited to single expression\n            calc,\n            engine=\"python\",\n            parser=\"pandas\",\n            local_dict={\"df\": df},\n        )\n    except Exception as e:\n        # return error message so LLM can fix the calc string if needed\n        err = f\"\"\"\n        Error encountered in pandas eval: {str(e)}\n        \"\"\"\n        if isinstance(e, KeyError) and \"not in index\" in str(e):\n            # Pd.eval sometimes fails on a perfectly valid exprn like\n            # df.loc[..., 'column'] with a KeyError.\n            err += \"\"\"\n            Maybe try a different way, e.g. \n            instead of df.loc[..., 'column'], try df.loc[...]['column']\n            \"\"\"\n        return err\n    return stringify(result)\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.maybe_add_ids","title":"<code>maybe_add_ids(documents)</code>","text":"<p>Add ids to metadata if absent, since some vecdbs don't like having blank ids.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def maybe_add_ids(self, documents: Sequence[Document]) -&gt; None:\n    \"\"\"Add ids to metadata if absent, since some\n    vecdbs don't like having blank ids.\"\"\"\n    for d in documents:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.similar_texts_with_scores","title":"<code>similar_texts_with_scores(text, k=1, where=None)</code>  <code>abstractmethod</code>","text":"<p>Find k most similar texts to the given text, in terms of vector distance metric (e.g., cosine similarity).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to find similar texts for.</p> required <code>k</code> <code>int</code> <p>Number of similar texts to retrieve. Defaults to 1.</p> <code>1</code> <code>where</code> <code>Optional[str]</code> <p>Where clause to filter the search.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef similar_texts_with_scores(\n    self,\n    text: str,\n    k: int = 1,\n    where: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Find k most similar texts to the given text, in terms of vector distance metric\n    (e.g., cosine similarity).\n\n    Args:\n        text (str): The text to find similar texts for.\n        k (int, optional): Number of similar texts to retrieve. Defaults to 1.\n        where (Optional[str], optional): Where clause to filter the search.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.add_context_window","title":"<code>add_context_window(docs_scores, neighbors=0)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. These window_ids may overlap, so we - coalesce each overlapping groups into a single window (maintaining ordering), - create a new document for each part, preserving metadata,</p> <p>We may have stored a longer set of window_ids than we need during chunking. Now, we just want <code>neighbors</code> on each side of the center of the window_ids list.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <code>neighbors</code> <code>int</code> <p>Number of neighbors on \"each side\" of match to retrieve. Defaults to 0. \"Each side\" here means before and after the match, in the original text.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def add_context_window(\n    self, docs_scores: List[Tuple[Document, float]], neighbors: int = 0\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk.\n    These window_ids may overlap, so we\n    - coalesce each overlapping groups into a single window (maintaining ordering),\n    - create a new document for each part, preserving metadata,\n\n    We may have stored a longer set of window_ids than we need during chunking.\n    Now, we just want `neighbors` on each side of the center of the window_ids list.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n        neighbors (int, optional): Number of neighbors on \"each side\" of match to\n            retrieve. Defaults to 0.\n            \"Each side\" here means before and after the match,\n            in the original text.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    # We return a larger context around each match, i.e.\n    # a window of `neighbors` on each side of the match.\n    docs = [d for d, s in docs_scores]\n    scores = [s for d, s in docs_scores]\n    if neighbors == 0:\n        return docs_scores\n    doc_chunks = [d for d in docs if d.metadata.is_chunk]\n    if len(doc_chunks) == 0:\n        return docs_scores\n    window_ids_list = []\n    id2metadata = {}\n    # id -&gt; highest score of a doc it appears in\n    id2max_score: Dict[int | str, float] = {}\n    for i, d in enumerate(docs):\n        window_ids = d.metadata.window_ids\n        if len(window_ids) == 0:\n            window_ids = [d.id()]\n        id2metadata.update({id: d.metadata for id in window_ids})\n\n        id2max_score.update(\n            {id: max(id2max_score.get(id, 0), scores[i]) for id in window_ids}\n        )\n        n = len(window_ids)\n        chunk_idx = window_ids.index(d.id())\n        neighbor_ids = window_ids[\n            max(0, chunk_idx - neighbors) : min(n, chunk_idx + neighbors + 1)\n        ]\n        window_ids_list += [neighbor_ids]\n\n    # window_ids could be from different docs,\n    # and they may overlap, so we coalesce overlapping groups into\n    # separate windows.\n    window_ids_list = self.remove_overlaps(window_ids_list)\n    final_docs = []\n    final_scores = []\n    for w in window_ids_list:\n        metadata = copy.deepcopy(id2metadata[w[0]])\n        metadata.window_ids = w\n        document = Document(\n            content=\" \".join([d.content for d in self.get_documents_by_ids(w)]),\n            metadata=metadata,\n        )\n        # make a fresh id since content is in general different\n        document.metadata.id = ObjectRegistry.new_id()\n        final_docs += [document]\n        final_scores += [max(id2max_score[id] for id in w)]\n    return list(zip(final_docs, final_scores))\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.remove_overlaps","title":"<code>remove_overlaps(windows)</code>  <code>staticmethod</code>","text":"<p>Given a collection of windows, where each window is a sequence of ids, identify groups of overlapping windows, and for each overlapping group, order the chunk-ids using topological sort so they appear in the original order in the text.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>List[int | str]</code> <p>List of windows, where each window is a sequence of ids.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[int|str]: List of windows, where each window is a sequence of ids, and no two windows overlap.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@staticmethod\ndef remove_overlaps(windows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Given a collection of windows, where each window is a sequence of ids,\n    identify groups of overlapping windows, and for each overlapping group,\n    order the chunk-ids using topological sort so they appear in the original\n    order in the text.\n\n    Args:\n        windows (List[int|str]): List of windows, where each window is a\n            sequence of ids.\n\n    Returns:\n        List[int|str]: List of windows, where each window is a sequence of ids,\n            and no two windows overlap.\n    \"\"\"\n    ids = set(id for w in windows for id in w)\n    # id -&gt; {win -&gt; # pos}\n    id2win2pos: Dict[str, Dict[int, int]] = {id: {} for id in ids}\n\n    for i, w in enumerate(windows):\n        for j, id in enumerate(w):\n            id2win2pos[id][i] = j\n\n    n = len(windows)\n    # relation between windows:\n    order = np.zeros((n, n), dtype=np.int8)\n    for i, w in enumerate(windows):\n        for j, x in enumerate(windows):\n            if i == j:\n                continue\n            if len(set(w).intersection(x)) == 0:\n                continue\n            id = list(set(w).intersection(x))[0]  # any common id\n            if id2win2pos[id][i] &gt; id2win2pos[id][j]:\n                order[i, j] = -1  # win i is before win j\n            else:\n                order[i, j] = 1  # win i is after win j\n\n    # find groups of windows that overlap, like connected components in a graph\n    groups = components(np.abs(order))\n\n    # order the chunk-ids in each group using topological sort\n    new_windows = []\n    for g in groups:\n        # find total ordering among windows in group based on order matrix\n        # (this is a topological sort)\n        _g = np.array(g)\n        order_matrix = order[_g][:, _g]\n        ordered_window_indices = topological_sort(order_matrix)\n        ordered_window_ids = [windows[i] for i in _g[ordered_window_indices]]\n        flattened = [id for w in ordered_window_ids for id in w]\n        flattened_deduped = list(dict.fromkeys(flattened))\n        # Note we are not going to split these, and instead we'll return\n        # larger windows from concatenating the connected groups.\n        # This ensures context is retained for LLM q/a\n        new_windows += [flattened_deduped]\n\n    return new_windows\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.get_all_documents","title":"<code>get_all_documents(where='')</code>  <code>abstractmethod</code>","text":"<p>Get all documents in the current collection, possibly filtered by <code>where</code>.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_all_documents(self, where: str = \"\") -&gt; List[Document]:\n    \"\"\"\n    Get all documents in the current collection, possibly filtered by `where`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids. Args:     ids (List[str]): List of document ids.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB","title":"<code>QdrantDB(config=QdrantDBConfig())</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def __init__(self, config: QdrantDBConfig = QdrantDBConfig()):\n    super().__init__(config)\n    self.config: QdrantDBConfig = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    if self.config.use_sparse_embeddings:\n        try:\n            from transformers import AutoModelForMaskedLM, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"\"\"\n                To use sparse embeddings, \n                you must install langroid with the [transformers] extra, e.g.:\n                pip install \"langroid[transformers]\"\n                \"\"\"\n            )\n\n        self.sparse_tokenizer = AutoTokenizer.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n        self.sparse_model = AutoModelForMaskedLM.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    key = os.getenv(\"QDRANT_API_KEY\")\n    url = os.getenv(\"QDRANT_API_URL\")\n    if config.cloud and None in [key, url]:\n        logger.warning(\n            f\"\"\"QDRANT_API_KEY, QDRANT_API_URL env variable must be set to use \n            QdrantDB in cloud mode. Please set these values \n            in your .env file. \n            Switching to local storage at {config.storage_path} \n            \"\"\"\n        )\n        config.cloud = False\n    if config.cloud:\n        self.client = QdrantClient(\n            url=url,\n            api_key=key,\n            timeout=config.timeout,\n        )\n    else:\n        try:\n            self.client = QdrantClient(\n                path=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local QdrantDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = QdrantClient(\n                path=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        info = self.client.get_collection(collection_name=name)\n        points_count = from_optional(info.points_count, 0)\n\n        n_empty_deletes += points_count == 0\n        n_non_empty_deletes += points_count &gt; 0\n        self.client.delete_collection(collection_name=name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    colls = list(self.client.get_collections())[0][1]\n    if empty:\n        return [coll.name for coll in colls]\n    counts = []\n    for coll in colls:\n        try:\n            counts.append(\n                from_optional(\n                    self.client.get_collection(\n                        collection_name=coll.name\n                    ).points_count,\n                    0,\n                )\n            )\n        except Exception:\n            logger.warning(f\"Error getting collection {coll.name}\")\n            counts.append(0)\n    return [coll.name for coll, count in zip(colls, counts) if (count or 0) &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    if self.client.collection_exists(collection_name=collection_name):\n        coll = self.client.get_collection(collection_name=collection_name)\n        if (\n            coll.status == CollectionStatus.GREEN\n            and from_optional(coll.points_count, 0) &gt; 0\n        ):\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n        self.client.delete_collection(collection_name=collection_name)\n\n    vectors_config = {\n        \"\": VectorParams(\n            size=self.embedding_dim,\n            distance=Distance.COSINE,\n        )\n    }\n    sparse_vectors_config = None\n    if self.config.use_sparse_embeddings:\n        sparse_vectors_config = {\n            \"text-sparse\": SparseVectorParams(index=SparseIndexParams())\n        }\n    self.client.create_collection(\n        collection_name=collection_name,\n        vectors_config=vectors_config,\n        sparse_vectors_config=sparse_vectors_config,\n    )\n    collection_info = self.client.get_collection(collection_name=collection_name)\n    assert collection_info.status == CollectionStatus.GREEN\n    assert collection_info.vectors_count in [0, None]\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/base/","title":"base","text":"<p>langroid/vector_store/base.py </p>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore","title":"<code>VectorStore(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def __init__(self, config: VectorStoreConfig):\n    self.config = config\n    self.embedding_model = EmbeddingModel.create(config.embedding)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n    \"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>  <code>abstractmethod</code>","text":"<p>Clear all collections in the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>really</code> <code>bool</code> <p>Whether to really clear all collections. Defaults to False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>Prefix of collections to clear.</p> <code>''</code> <p>Returns:     int: Number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"\n    Clear all collections in the vector store.\n\n    Args:\n        really (bool, optional): Whether to really clear all collections.\n            Defaults to False.\n        prefix (str, optional): Prefix of collections to clear.\n    Returns:\n        int: Number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.list_collections","title":"<code>list_collections(empty=False)</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store (only non empty collections if empty=False).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"List all collections in the vector store\n    (only non empty collections if empty=False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the collection if it         already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\n\n    self.config.collection_name = collection_name\n    if collection_name not in self.list_collections() or replace:\n        self.create_collection(collection_name, replace=replace)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the         collection if it already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.compute_from_docs","title":"<code>compute_from_docs(docs, calc)</code>","text":"<p>Compute a result on a set of documents, using a dataframe calc string like <code>df.groupby('state')['income'].mean()</code>.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def compute_from_docs(self, docs: List[Document], calc: str) -&gt; str:\n    \"\"\"Compute a result on a set of documents,\n    using a dataframe calc string like `df.groupby('state')['income'].mean()`.\n    \"\"\"\n    dicts = [doc.dict() for doc in docs]\n    df = pd.DataFrame(dicts)\n\n    try:\n        result = pd.eval(  # safer than eval but limited to single expression\n            calc,\n            engine=\"python\",\n            parser=\"pandas\",\n            local_dict={\"df\": df},\n        )\n    except Exception as e:\n        # return error message so LLM can fix the calc string if needed\n        err = f\"\"\"\n        Error encountered in pandas eval: {str(e)}\n        \"\"\"\n        if isinstance(e, KeyError) and \"not in index\" in str(e):\n            # Pd.eval sometimes fails on a perfectly valid exprn like\n            # df.loc[..., 'column'] with a KeyError.\n            err += \"\"\"\n            Maybe try a different way, e.g. \n            instead of df.loc[..., 'column'], try df.loc[...]['column']\n            \"\"\"\n        return err\n    return stringify(result)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.maybe_add_ids","title":"<code>maybe_add_ids(documents)</code>","text":"<p>Add ids to metadata if absent, since some vecdbs don't like having blank ids.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def maybe_add_ids(self, documents: Sequence[Document]) -&gt; None:\n    \"\"\"Add ids to metadata if absent, since some\n    vecdbs don't like having blank ids.\"\"\"\n    for d in documents:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.similar_texts_with_scores","title":"<code>similar_texts_with_scores(text, k=1, where=None)</code>  <code>abstractmethod</code>","text":"<p>Find k most similar texts to the given text, in terms of vector distance metric (e.g., cosine similarity).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to find similar texts for.</p> required <code>k</code> <code>int</code> <p>Number of similar texts to retrieve. Defaults to 1.</p> <code>1</code> <code>where</code> <code>Optional[str]</code> <p>Where clause to filter the search.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef similar_texts_with_scores(\n    self,\n    text: str,\n    k: int = 1,\n    where: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Find k most similar texts to the given text, in terms of vector distance metric\n    (e.g., cosine similarity).\n\n    Args:\n        text (str): The text to find similar texts for.\n        k (int, optional): Number of similar texts to retrieve. Defaults to 1.\n        where (Optional[str], optional): Where clause to filter the search.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.add_context_window","title":"<code>add_context_window(docs_scores, neighbors=0)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. These window_ids may overlap, so we - coalesce each overlapping groups into a single window (maintaining ordering), - create a new document for each part, preserving metadata,</p> <p>We may have stored a longer set of window_ids than we need during chunking. Now, we just want <code>neighbors</code> on each side of the center of the window_ids list.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <code>neighbors</code> <code>int</code> <p>Number of neighbors on \"each side\" of match to retrieve. Defaults to 0. \"Each side\" here means before and after the match, in the original text.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def add_context_window(\n    self, docs_scores: List[Tuple[Document, float]], neighbors: int = 0\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk.\n    These window_ids may overlap, so we\n    - coalesce each overlapping groups into a single window (maintaining ordering),\n    - create a new document for each part, preserving metadata,\n\n    We may have stored a longer set of window_ids than we need during chunking.\n    Now, we just want `neighbors` on each side of the center of the window_ids list.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n        neighbors (int, optional): Number of neighbors on \"each side\" of match to\n            retrieve. Defaults to 0.\n            \"Each side\" here means before and after the match,\n            in the original text.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    # We return a larger context around each match, i.e.\n    # a window of `neighbors` on each side of the match.\n    docs = [d for d, s in docs_scores]\n    scores = [s for d, s in docs_scores]\n    if neighbors == 0:\n        return docs_scores\n    doc_chunks = [d for d in docs if d.metadata.is_chunk]\n    if len(doc_chunks) == 0:\n        return docs_scores\n    window_ids_list = []\n    id2metadata = {}\n    # id -&gt; highest score of a doc it appears in\n    id2max_score: Dict[int | str, float] = {}\n    for i, d in enumerate(docs):\n        window_ids = d.metadata.window_ids\n        if len(window_ids) == 0:\n            window_ids = [d.id()]\n        id2metadata.update({id: d.metadata for id in window_ids})\n\n        id2max_score.update(\n            {id: max(id2max_score.get(id, 0), scores[i]) for id in window_ids}\n        )\n        n = len(window_ids)\n        chunk_idx = window_ids.index(d.id())\n        neighbor_ids = window_ids[\n            max(0, chunk_idx - neighbors) : min(n, chunk_idx + neighbors + 1)\n        ]\n        window_ids_list += [neighbor_ids]\n\n    # window_ids could be from different docs,\n    # and they may overlap, so we coalesce overlapping groups into\n    # separate windows.\n    window_ids_list = self.remove_overlaps(window_ids_list)\n    final_docs = []\n    final_scores = []\n    for w in window_ids_list:\n        metadata = copy.deepcopy(id2metadata[w[0]])\n        metadata.window_ids = w\n        document = Document(\n            content=\" \".join([d.content for d in self.get_documents_by_ids(w)]),\n            metadata=metadata,\n        )\n        # make a fresh id since content is in general different\n        document.metadata.id = ObjectRegistry.new_id()\n        final_docs += [document]\n        final_scores += [max(id2max_score[id] for id in w)]\n    return list(zip(final_docs, final_scores))\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.remove_overlaps","title":"<code>remove_overlaps(windows)</code>  <code>staticmethod</code>","text":"<p>Given a collection of windows, where each window is a sequence of ids, identify groups of overlapping windows, and for each overlapping group, order the chunk-ids using topological sort so they appear in the original order in the text.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>List[int | str]</code> <p>List of windows, where each window is a sequence of ids.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[int|str]: List of windows, where each window is a sequence of ids, and no two windows overlap.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@staticmethod\ndef remove_overlaps(windows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Given a collection of windows, where each window is a sequence of ids,\n    identify groups of overlapping windows, and for each overlapping group,\n    order the chunk-ids using topological sort so they appear in the original\n    order in the text.\n\n    Args:\n        windows (List[int|str]): List of windows, where each window is a\n            sequence of ids.\n\n    Returns:\n        List[int|str]: List of windows, where each window is a sequence of ids,\n            and no two windows overlap.\n    \"\"\"\n    ids = set(id for w in windows for id in w)\n    # id -&gt; {win -&gt; # pos}\n    id2win2pos: Dict[str, Dict[int, int]] = {id: {} for id in ids}\n\n    for i, w in enumerate(windows):\n        for j, id in enumerate(w):\n            id2win2pos[id][i] = j\n\n    n = len(windows)\n    # relation between windows:\n    order = np.zeros((n, n), dtype=np.int8)\n    for i, w in enumerate(windows):\n        for j, x in enumerate(windows):\n            if i == j:\n                continue\n            if len(set(w).intersection(x)) == 0:\n                continue\n            id = list(set(w).intersection(x))[0]  # any common id\n            if id2win2pos[id][i] &gt; id2win2pos[id][j]:\n                order[i, j] = -1  # win i is before win j\n            else:\n                order[i, j] = 1  # win i is after win j\n\n    # find groups of windows that overlap, like connected components in a graph\n    groups = components(np.abs(order))\n\n    # order the chunk-ids in each group using topological sort\n    new_windows = []\n    for g in groups:\n        # find total ordering among windows in group based on order matrix\n        # (this is a topological sort)\n        _g = np.array(g)\n        order_matrix = order[_g][:, _g]\n        ordered_window_indices = topological_sort(order_matrix)\n        ordered_window_ids = [windows[i] for i in _g[ordered_window_indices]]\n        flattened = [id for w in ordered_window_ids for id in w]\n        flattened_deduped = list(dict.fromkeys(flattened))\n        # Note we are not going to split these, and instead we'll return\n        # larger windows from concatenating the connected groups.\n        # This ensures context is retained for LLM q/a\n        new_windows += [flattened_deduped]\n\n    return new_windows\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_all_documents","title":"<code>get_all_documents(where='')</code>  <code>abstractmethod</code>","text":"<p>Get all documents in the current collection, possibly filtered by <code>where</code>.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_all_documents(self, where: str = \"\") -&gt; List[Document]:\n    \"\"\"\n    Get all documents in the current collection, possibly filtered by `where`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids. Args:     ids (List[str]): List of document ids.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/chromadb/","title":"chromadb","text":"<p>langroid/vector_store/chromadb.py </p>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB","title":"<code>ChromaDB(config=ChromaDBConfig())</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def __init__(self, config: ChromaDBConfig = ChromaDBConfig()):\n    super().__init__(config)\n    try:\n        import chromadb\n    except ImportError:\n        raise LangroidImportError(\"chromadb\", \"chromadb\")\n    self.config = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn = emb_model.embedding_fn()\n    self.client = chromadb.Client(\n        chromadb.config.Settings(\n            # chroma_db_impl=\"duckdb+parquet\",\n            persist_directory=config.storage_path,\n        )\n    )\n    if self.config.collection_name is not None:\n        self.create_collection(\n            self.config.collection_name,\n            replace=self.config.replace_collection,\n        )\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections in the vector store with the given prefix.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections in the vector store with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll = [c for c in self.client.list_collections() if c.name.startswith(prefix)]\n    if len(coll) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for c in coll:\n        n_empty_deletes += c.count() == 0\n        n_non_empty_deletes += c.count() &gt; 0\n        self.client.delete_collection(name=c.name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>List non-empty collections in the vector store. Args:     empty (bool, optional): Whether to list empty collections. Returns:     List[str]: List of non-empty collection names.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    List non-empty collections in the vector store.\n    Args:\n        empty (bool, optional): Whether to list empty collections.\n    Returns:\n        List[str]: List of non-empty collection names.\n    \"\"\"\n    colls = self.client.list_collections()\n    if empty:\n        return [coll.name for coll in colls]\n    return [coll.name for coll in colls if coll.count() &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection in the vector store, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create or replace.     replace (bool, optional): Whether to replace an existing collection.         Defaults to False.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection in the vector store, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create or replace.\n        replace (bool, optional): Whether to replace an existing collection.\n            Defaults to False.\n\n    \"\"\"\n    self.config.collection_name = collection_name\n    if collection_name in self.list_collections(empty=True) and replace:\n        logger.warning(f\"Replacing existing collection {collection_name}\")\n        self.client.delete_collection(collection_name)\n    self.collection = self.client.create_collection(\n        name=self.config.collection_name,\n        embedding_function=self.embedding_fn,\n        get_or_create=not replace,\n    )\n</code></pre>"},{"location":"reference/vector_store/lancedb/","title":"lancedb","text":"<p>langroid/vector_store/lancedb.py </p>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB","title":"<code>LanceDB(config=LanceDBConfig())</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def __init__(self, config: LanceDBConfig = LanceDBConfig()):\n    super().__init__(config)\n    if not has_lancedb:\n        raise LangroidImportError(\"lancedb\", \"lancedb\")\n\n    self.config: LanceDBConfig = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    self.host = config.host\n    self.port = config.port\n    self.is_from_dataframe = False  # were docs ingested from a dataframe?\n    self.df_metadata_columns: List[str] = []  # metadata columns from dataframe\n    self._setup_schemas(config.document_class)\n\n    load_dotenv()\n    if self.config.cloud:\n        logger.warning(\n            \"LanceDB Cloud is not available yet. Switching to local storage.\"\n        )\n        config.cloud = False\n    else:\n        try:\n            self.client = lancedb.connect(\n                uri=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local LanceDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = lancedb.connect(\n                uri=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        nr = self.client.open_table(name).head(1).shape[0]\n        n_empty_deletes += nr == 0\n        n_non_empty_deletes += nr &gt; 0\n        self.client.drop_table(name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    colls = self.client.table_names(limit=None)\n    if len(colls) == 0:\n        return []\n    if empty:  # include empty tbls\n        return colls  # type: ignore\n    counts = [self.client.open_table(coll).head(1).shape[0] for coll in colls]\n    return [coll for coll, count in zip(colls, counts) if count &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    collections = self.list_collections()\n    if collection_name in collections:\n        coll = self.client.open_table(collection_name)\n        if coll.head().shape[0] &gt; 0:\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n    try:\n        self.client.create_table(\n            collection_name, schema=self.schema, mode=\"overwrite\"\n        )\n    except (AttributeError, TypeError) as e:\n        pydantic_version = pydantic_major_version()\n        if pydantic_version &gt; 1:\n            raise ValueError(\n                f\"\"\"\n                {e}\n                ====\n                You are using Pydantic v{pydantic_version},\n                which is not yet compatible with Langroid's LanceDB integration.\n                To use Lancedb with Langroid, please install the \n                latest pydantic 1.x instead of pydantic v2, e.g. \n                pip install \"pydantic&lt;2.0.0\"\n                \"\"\"\n            )\n        else:\n            raise e\n\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.add_dataframe","title":"<code>add_dataframe(df, content='content', metadata=[])</code>","text":"<p>Add a dataframe to the collection. Args:     df (pd.DataFrame): A dataframe     content (str): The name of the column in the dataframe that contains the         text content to be embedded using the embedding model.     metadata (List[str]): A list of column names in the dataframe that contain         metadata to be stored in the database. Defaults to [].</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def add_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; None:\n    \"\"\"\n    Add a dataframe to the collection.\n    Args:\n        df (pd.DataFrame): A dataframe\n        content (str): The name of the column in the dataframe that contains the\n            text content to be embedded using the embedding model.\n        metadata (List[str]): A list of column names in the dataframe that contain\n            metadata to be stored in the database. Defaults to [].\n    \"\"\"\n    self.is_from_dataframe = True\n    actual_metadata = metadata.copy()\n    self.df_metadata_columns = actual_metadata  # could be updated below\n    # get content column\n    content_values = df[content].values.tolist()\n    embedding_vecs = self.embedding_fn(content_values)\n\n    # add vector column\n    df[\"vector\"] = embedding_vecs\n    if content != \"content\":\n        # rename content column to \"content\", leave existing column intact\n        df = df.rename(columns={content: \"content\"}, inplace=False)\n\n    if \"id\" not in df.columns:\n        docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n        ids = [str(d.id()) for d in docs]\n        df[\"id\"] = ids\n\n    if \"id\" not in actual_metadata:\n        actual_metadata += [\"id\"]\n\n    colls = self.list_collections(empty=True)\n    coll_name = self.config.collection_name\n    if (\n        coll_name not in colls\n        or self.client.open_table(coll_name).head(1).shape[0] == 0\n    ):\n        # collection either doesn't exist or is empty, so replace it\n        # and set new schema from df\n        self.client.create_table(\n            self.config.collection_name,\n            data=df,\n            mode=\"overwrite\",\n        )\n        doc_cls = dataframe_to_document_model(\n            df,\n            content=content,\n            metadata=actual_metadata,\n            exclude=[\"vector\"],\n        )\n        self.config.document_class = doc_cls  # type: ignore\n        self._setup_schemas(doc_cls)  # type: ignore\n    else:\n        # collection exists and is not empty, so append to it\n        tbl = self.client.open_table(self.config.collection_name)\n        tbl.add(df)\n</code></pre>"},{"location":"reference/vector_store/meilisearch/","title":"meilisearch","text":"<p>langroid/vector_store/meilisearch.py </p> <p>MeiliSearch as a pure document store, without its (experimental) vector-store functionality. We aim to use MeiliSearch for fast lexical search. Note that what we call \"Collection\" in Langroid is referred to as \"Index\" in MeiliSearch. Each data-store has its own terminology, but for uniformity we use the Langroid terminology here.</p>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch","title":"<code>MeiliSearch(config=MeiliSearchConfig())</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def __init__(self, config: MeiliSearchConfig = MeiliSearchConfig()):\n    super().__init__(config)\n    try:\n        import meilisearch_python_sdk as meilisearch\n    except ImportError:\n        raise LangroidImportError(\"meilisearch\", \"meilisearch\")\n\n    self.config: MeiliSearchConfig = config\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    self.key = os.getenv(\"MEILISEARCH_API_KEY\") or \"masterKey\"\n    self.url = os.getenv(\"MEILISEARCH_API_URL\") or f\"http://{self.host}:{self.port}\"\n    if config.cloud and None in [self.key, self.url]:\n        logger.warning(\n            f\"\"\"MEILISEARCH_API_KEY, MEILISEARCH_API_URL env variable must be set \n            to use MeiliSearch in cloud mode. Please set these values \n            in your .env file. Switching to local MeiliSearch at \n            {self.url} \n            \"\"\"\n        )\n        config.cloud = False\n\n    self.client: Callable[[], meilisearch.AsyncClient] = lambda: (\n        meilisearch.AsyncClient(url=self.url, api_key=self.key)\n    )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of db until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.clear_empty_collections","title":"<code>clear_empty_collections()</code>","text":"<p>All collections are treated as non-empty in MeiliSearch, so this is a no-op</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def clear_empty_collections(self) -&gt; int:\n    \"\"\"All collections are treated as non-empty in MeiliSearch, so this is a\n    no-op\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Delete all indices whose names start with <code>prefix</code></p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Delete all indices whose names start with `prefix`\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [c for c in self.list_collections() if c.startswith(prefix)]\n    deletes = asyncio.run(self._async_delete_indices(coll_names))\n    n_deletes = sum(deletes)\n    logger.warning(f\"Deleted {n_deletes} indices in MeiliSearch\")\n    return n_deletes\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of index names stored. We treat any existing index as non-empty.</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of index names stored. We treat any existing index as non-empty.\n    \"\"\"\n    indexes = asyncio.run(self._async_get_indexes())\n    if len(indexes) == 0:\n        return []\n    else:\n        return [ind.uid for ind in indexes]\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    collections = self.list_collections()\n    if collection_name in collections:\n        logger.warning(\n            f\"MeiliSearch Non-empty Index {collection_name} already exists\"\n        )\n        if not replace:\n            logger.warning(\"Not replacing collection\")\n            return\n        else:\n            logger.warning(\"Recreating fresh collection\")\n            asyncio.run(self._async_delete_index(collection_name))\n    asyncio.run(self._async_create_index(collection_name))\n    collection_info = asyncio.run(self._async_get_index(collection_name))\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/momento/","title":"momento","text":"<p>langroid/vector_store/momento.py </p> <p>Momento Vector Index. https://docs.momentohq.com/vector-index/develop/api-reference</p>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI","title":"<code>MomentoVI(config=MomentoVIConfig())</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def __init__(self, config: MomentoVIConfig = MomentoVIConfig()):\n    super().__init__(config)\n    if not has_momento:\n        raise LangroidImportError(\"momento\", \"momento\")\n    self.distance = SimilarityMetric.COSINE_SIMILARITY\n    self.config: MomentoVIConfig = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    api_key = os.getenv(\"MOMENTO_API_KEY\")\n    if config.cloud:\n        if api_key is None:\n            raise ValueError(\n                \"\"\"MOMENTO_API_KEY env variable must be set to \n                MomentoVI hosted service. Please set this in your .env file. \n                \"\"\"\n            )\n        self.client = PreviewVectorIndexClient(\n            configuration=VectorIndexConfigurations.Default.latest(),\n            credential_provider=CredentialProvider.from_string(api_key),\n        )\n    else:\n        raise NotImplementedError(\"MomentoVI local not available yet\")\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = self.list_collections(empty=False)\n    coll_names = [name for name in coll_names if name.startswith(prefix)]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    for name in coll_names:\n        self.delete_collection(name)\n    logger.warning(\n        f\"\"\"\n        Deleted {len(coll_names)} indices from Momento VI\n        \"\"\"\n    )\n    return len(coll_names)\n</code></pre>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    if not has_momento:\n        raise LangroidImportError(\"momento\", \"momento\")\n    response = self.client.list_indexes()\n    if isinstance(response, mvi_response.ListIndexes.Success):\n        return [ind.name for ind in response.indexes]\n    elif isinstance(response, mvi_response.ListIndexes.Error):\n        raise ValueError(f\"Error listing collections: {response.message}\")\n    else:\n        raise ValueError(f\"Unexpected response: {response}\")\n</code></pre>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    if not has_momento:\n        raise LangroidImportError(\"momento\", \"momento\")\n    self.config.collection_name = collection_name\n    response = self.client.create_index(\n        index_name=collection_name,\n        num_dimensions=self.embedding_dim,\n        similarity_metric=self.distance,\n    )\n    if isinstance(response, mvi_response.CreateIndex.Success):\n        logger.info(f\"Created collection {collection_name}\")\n    elif isinstance(response, mvi_response.CreateIndex.IndexAlreadyExists):\n        logger.warning(f\"Collection {collection_name} already exists\")\n    elif isinstance(response, mvi_response.CreateIndex.Error):\n        raise ValueError(\n            f\"Error creating collection {collection_name}: {response.message}\"\n        )\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(f\"Collection {collection_name} created\")\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/","title":"qdrantdb","text":"<p>langroid/vector_store/qdrantdb.py </p>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB","title":"<code>QdrantDB(config=QdrantDBConfig())</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def __init__(self, config: QdrantDBConfig = QdrantDBConfig()):\n    super().__init__(config)\n    self.config: QdrantDBConfig = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    if self.config.use_sparse_embeddings:\n        try:\n            from transformers import AutoModelForMaskedLM, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"\"\"\n                To use sparse embeddings, \n                you must install langroid with the [transformers] extra, e.g.:\n                pip install \"langroid[transformers]\"\n                \"\"\"\n            )\n\n        self.sparse_tokenizer = AutoTokenizer.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n        self.sparse_model = AutoModelForMaskedLM.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    key = os.getenv(\"QDRANT_API_KEY\")\n    url = os.getenv(\"QDRANT_API_URL\")\n    if config.cloud and None in [key, url]:\n        logger.warning(\n            f\"\"\"QDRANT_API_KEY, QDRANT_API_URL env variable must be set to use \n            QdrantDB in cloud mode. Please set these values \n            in your .env file. \n            Switching to local storage at {config.storage_path} \n            \"\"\"\n        )\n        config.cloud = False\n    if config.cloud:\n        self.client = QdrantClient(\n            url=url,\n            api_key=key,\n            timeout=config.timeout,\n        )\n    else:\n        try:\n            self.client = QdrantClient(\n                path=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local QdrantDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = QdrantClient(\n                path=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        info = self.client.get_collection(collection_name=name)\n        points_count = from_optional(info.points_count, 0)\n\n        n_empty_deletes += points_count == 0\n        n_non_empty_deletes += points_count &gt; 0\n        self.client.delete_collection(collection_name=name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    colls = list(self.client.get_collections())[0][1]\n    if empty:\n        return [coll.name for coll in colls]\n    counts = []\n    for coll in colls:\n        try:\n            counts.append(\n                from_optional(\n                    self.client.get_collection(\n                        collection_name=coll.name\n                    ).points_count,\n                    0,\n                )\n            )\n        except Exception:\n            logger.warning(f\"Error getting collection {coll.name}\")\n            counts.append(0)\n    return [coll.name for coll, count in zip(colls, counts) if (count or 0) &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    if self.client.collection_exists(collection_name=collection_name):\n        coll = self.client.get_collection(collection_name=collection_name)\n        if (\n            coll.status == CollectionStatus.GREEN\n            and from_optional(coll.points_count, 0) &gt; 0\n        ):\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n        self.client.delete_collection(collection_name=collection_name)\n\n    vectors_config = {\n        \"\": VectorParams(\n            size=self.embedding_dim,\n            distance=Distance.COSINE,\n        )\n    }\n    sparse_vectors_config = None\n    if self.config.use_sparse_embeddings:\n        sparse_vectors_config = {\n            \"text-sparse\": SparseVectorParams(index=SparseIndexParams())\n        }\n    self.client.create_collection(\n        collection_name=collection_name,\n        vectors_config=vectors_config,\n        sparse_vectors_config=sparse_vectors_config,\n    )\n    collection_info = self.client.get_collection(collection_name=collection_name)\n    assert collection_info.status == CollectionStatus.GREEN\n    assert collection_info.vectors_count in [0, None]\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.is_valid_uuid","title":"<code>is_valid_uuid(uuid_to_test)</code>","text":"<p>Check if a given string is a valid UUID.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def is_valid_uuid(uuid_to_test: str) -&gt; bool:\n    \"\"\"\n    Check if a given string is a valid UUID.\n    \"\"\"\n    try:\n        uuid_obj = uuid.UUID(uuid_to_test)\n        return str(uuid_obj) == uuid_to_test\n    except Exception:\n        pass\n    # Check for valid unsigned 64-bit integer\n    try:\n        int_value = int(uuid_to_test)\n        return 0 &lt;= int_value &lt;= 18446744073709551615\n    except ValueError:\n        return False\n</code></pre>"},{"location":"tutorials/local-llm-setup/","title":"Setting up a local LLM to work with Langroid","text":"<p>Examples scripts in <code>examples/</code> directory.</p> <p>There are numerous examples of scripts that can be run with local LLMs,   in the <code>examples/</code>   directory of the main <code>langroid</code> repo. These examples are also in the    <code>langroid-examples</code>,   although the latter repo may contain some examples that are not in the <code>langroid</code> repo.   Most of these example scripts allow you to specify an LLM in the format <code>-m &lt;model&gt;</code>,   where the specification of <code>&lt;model&gt;</code> is described in the quide below for local/open LLMs,    or in the Non-OpenAI LLM guide. Scripts    that have the string <code>local</code> in their name have been especially designed to work with    certain local LLMs, as described in the respective scripts.   If you want a pointer to a specific script that illustrates a 2-agent chat, have a look    at <code>chat-search-assistant.py</code>.   This specific script, originally designed for GPT-4/GPT-4o, works well with <code>llama3-70b</code>    (tested via Groq, mentioned below).</p>"},{"location":"tutorials/local-llm-setup/#easiest-with-ollama","title":"Easiest: with Ollama","text":"<p>As of version 0.1.24, Ollama provides an OpenAI-compatible API server for the LLMs it supports, which massively simplifies running these LLMs with Langroid. Example below.</p> <p><pre><code>ollama pull mistral:7b-instruct-v0.2-q8_0\n</code></pre> This provides an OpenAI-compatible  server for the <code>mistral:7b-instruct-v0.2-q8_0</code> model.</p> <p>You can run any Langroid script using this model, by setting the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to <code>ollama/mistral:7b-instruct-v0.2-q8_0</code>, e.g.</p> <pre><code>import langroid.language_models as lm\nimport langroid as lr\n\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"ollama/mistral:7b-instruct-v0.2-q8_0\",\n    chat_context_length=16_000, # adjust based on model\n)\nagent_config = lr.ChatAgentConfig(\n    llm=llm_config,\n    system_message=\"You are helpful but concise\",\n)\nagent = lr.ChatAgent(agent_config)\n# directly invoke agent's llm_response method\n# response = agent.llm_response(\"What is the capital of Russia?\")\ntask = lr.Task(agent, interactive=True)\ntask.run() # for an interactive chat loop\n</code></pre>"},{"location":"tutorials/local-llm-setup/#setup-ollama-with-a-gguf-model-from-huggingface","title":"Setup Ollama with a GGUF model from HuggingFace","text":"<p>Some models are not directly supported by Ollama out of the box. To server a GGUF model with Ollama, you can download the model from HuggingFace and set up a custom Modelfile for it.</p> <p>E.g. download the GGUF version of <code>dolphin-mixtral</code> from here</p> <p>(specifically, download this file <code>dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf</code>)</p> <p>To set up a custom ollama model based on this:</p> <ul> <li>Save this model at a convenient place, e.g. <code>~/.ollama/models/</code></li> <li>Create a modelfile for this model. First see what an existing modelfile   for a similar model looks like, e.g. by running:</li> </ul> <p><pre><code>ollama show --modelfile dolphin-mixtral:latest\n</code></pre> You will notice this file has a FROM line followed by a prompt template and other settings. Create a new file with these contents. Only  change the  <code>FROM ...</code> line with the path to the model you downloaded, e.g. <pre><code>FROM /Users/blah/.ollama/models/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf\n</code></pre></p> <ul> <li>Save this modelfile somewhere, e.g. <code>~/.ollama/modelfiles/dolphin-mixtral-gguf</code></li> <li> <p>Create a new ollama model based on this file: <pre><code>ollama create dolphin-mixtral-gguf -f ~/.ollama/modelfiles/dolphin-mixtral-gguf\n</code></pre></p> </li> <li> <p>Run this new model using <code>ollama run dolphin-mixtral-gguf</code></p> </li> </ul> <p>To use this model with Langroid you can then specify <code>ollama/dolphin-mixtral-gguf</code> as the <code>chat_model</code> param in the <code>OpenAIGPTConfig</code> as in the previous section. When a script supports it, you can also pass in the model name via <code>-m ollama/dolphin-mixtral-gguf</code></p>"},{"location":"tutorials/local-llm-setup/#local-llms-hosted-on-groq","title":"\"Local\" LLMs hosted on Groq","text":"<p>In this scenario, an open-source LLM (e.g. <code>llama3-8b-8192</code>) is hosted on a Groq server which provides an OpenAI-compatible API. Using this with langroid is exactly analogous to the Ollama scenario above: you can set the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to <code>groq/&lt;model_name&gt;</code>, e.g. <code>groq/llama3-8b-8192</code>.  For this to work, ensure you have a <code>GROQ_API_KEY</code> environment variable set in your <code>.env</code> file. See groq docs.</p>"},{"location":"tutorials/local-llm-setup/#other-non-ollama-llms-supported-by-litellm","title":"Other non-Ollama LLMs supported by LiteLLM","text":"<p>For other scenarios of running local/remote LLMs, it is possible that the <code>LiteLLM</code> library supports an \"OpenAI adaptor\" for these models (see their docs).</p> <p>Depending on the specific model, the <code>litellm</code> docs may say you need to  specify a model in the form <code>&lt;provider&gt;/&lt;model&gt;</code>, e.g. <code>palm/chat-bison</code>.  To use the model with Langroid, simply prepend <code>litellm/</code> to this string, e.g. <code>litellm/palm/chat-bison</code>, when you specify the <code>chat_model</code> in the <code>OpenAIGPTConfig</code>.</p> <p>To use <code>litellm</code>, ensure you have the <code>litellm</code> extra installed,  via <code>pip install langroid[litellm]</code> or equivalent.</p>"},{"location":"tutorials/local-llm-setup/#harder-with-oobabooga","title":"Harder: with oobabooga","text":"<p>Like Ollama, oobabooga/text-generation-webui provides an OpenAI-API-compatible API server, but the setup  is significantly more involved. See their github page for installation and model-download instructions.</p> <p>Once you have finished the installation, you can spin up the server for an LLM using something like this:</p> <p><pre><code>python server.py --api --model mistral-7b-instruct-v0.2.Q8_0.gguf --verbose --extensions openai --nowebui\n</code></pre> This will show a message saying that the OpenAI-compatible API is running at <code>http://127.0.0.1:5000</code></p> <p>Then in your Langroid code you can specify the LLM config using <code>chat_model=\"local/127.0.0.1:5000/v1</code> (the <code>v1</code> is the API version, which is required). As with Ollama, you can use the <code>-m</code> arg in many of the example scripts, e.g. <pre><code>python examples/docqa/rag-local-simple.py -m local/127.0.0.1:5000/v1\n</code></pre></p> <p>Recommended: to ensure accurate chat formatting (and not use the defaults from ooba),   append the appropriate HuggingFace model name to the   -m arg, separated by //, e.g.  <pre><code>python examples/docqa/rag-local-simple.py -m local/127.0.0.1:5000/v1//mistral-instruct-v0.2\n</code></pre>   (no need to include the full model name, as long as you include enough to    uniquely identify the model's chat formatting template)</p>"},{"location":"tutorials/local-llm-setup/#other-local-llm-scenarios","title":"Other local LLM scenarios","text":"<p>There may be scenarios where the above <code>local/...</code> or <code>ollama/...</code> syntactic shorthand does not work.(e.g. when using vLLM to spin up a local LLM at an OpenAI-compatible endpoint). For these scenarios, you will have to explicitly create an instance of  <code>lm.OpenAIGPTConfig</code> and set both the <code>chat_model</code> and <code>api_base</code> parameters. For example, suppose you are able to get responses from this endpoint using something like: <pre><code>curl http://192.168.0.5:5078/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"Mistral-7B-Instruct-v0.2\",\n        \"messages\": [\n             {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre> To use this endpoint with Langroid, you would create an <code>OpenAIGPTConfig</code> like this: <pre><code>import langroid.language_models as lm\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"Mistral-7B-Instruct-v0.2\",\n    api_base=\"http://192.168.0.5:5078/v1\",\n)\n</code></pre></p>"},{"location":"tutorials/local-llm-setup/#quick-testing-with-local-llms","title":"Quick testing with local LLMs","text":"<p>As mentioned here,  you can run many of the tests in the main langroid repo against a local LLM (which by default run against an OpenAI model),  by specifying the model as <code>--m &lt;model&gt;</code>,  where <code>&lt;model&gt;</code> follows the syntax described in the previous sections. Here's an example:</p> <p><pre><code>pytest tests/main/test_chat_agent.py --m ollama/mixtral\n</code></pre> Of course, bear in mind that the tests may not pass due to weaknesses of the local LLM.</p>"},{"location":"tutorials/non-openai-llms/","title":"Using Langroid with Non-OpenAI LLMs","text":"<p>Langroid was initially written to work with OpenAI models via their API. This may sound limiting, but fortunately:</p> <ul> <li>many open-source LLMs can be served via  OpenAI-compatible endpoints. See the Local LLM Setup guide for details.</li> <li>there are tools like LiteLLM    that provide an OpenAI-like API for hundreds of non-OpenAI LLM providers  (e.g. Anthropic's Claude, Google's Gemini).</li> </ul> <p>Below we show how you can use the LiteLLM library with Langroid.</p>"},{"location":"tutorials/non-openai-llms/#create-an-openaigptconfig-object-with-chat_model-litellm","title":"Create an <code>OpenAIGPTConfig</code> object with <code>chat_model = \"litellm/...\"</code>","text":"<p>Install <code>litellm</code> extra</p> <p>To use <code>litellm</code> you need to install Langroid with the <code>litellm</code> extra, e.g.: <code>pip install \"langroid[litellm]\"</code></p> <p>Next, look up the instructions in LiteLLM docs for the specific model you are  interested. Here we take the example of Anthropic's <code>claude-instant-1</code> model. Set up the necessary environment variables as specified in the LiteLLM docs, e.g. for the <code>claude-instant-1</code> model, you will need to set the <code>ANTHROPIC_API_KEY</code> <pre><code>export ANTHROPIC_API_KEY=my-api-key\n</code></pre></p> <p>Now you are ready to create an instance of <code>OpenAIGPTConfig</code> with the  <code>chat_model</code> set to <code>litellm/&lt;model_spec&gt;</code>, where you should set <code>model_spec</code> based on LiteLLM  docs. For example, for the <code>claude-instant-1</code> model, you would set <code>chat_model</code> to <code>litellm/claude-instant-1</code>. But if you are using the model via a 3rd party provider, (e.g. those via Amazon Bedrock), you may also need to have a <code>provider</code> part in the <code>model_spec</code>, e.g.  <code>litellm/bedrock/anthropic.claude-instant-v1</code>. In general you can see which of these to use, from the LiteLLM docs.</p> <pre><code>import langroid as lr\nimport langroid.language_models as lm\n\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"litellm/claude-instant-v1\",\n    chat_context_length=8000, # adjust according to model\n)\n</code></pre> <p>A similar process works for the <code>Gemini 1.5 Pro</code> LLM:</p> <ul> <li>get the API key here</li> <li>set the <code>GEMINI_API_KEY</code> environment variable in your <code>.env</code> file or shell</li> <li>set <code>chat_model=\"litellm/gemini/gemini-1.5-pro-latest\"</code> in the <code>OpenAIGPTConfig</code> object</li> </ul> <p>For other gemini models supported by litellm, see their docs</p>"},{"location":"tutorials/non-openai-llms/#working-with-the-created-openaigptconfig-object","title":"Working with the created <code>OpenAIGPTConfig</code> object","text":"<p>From here you can proceed as usual, creating instances of <code>OpenAIGPT</code>, <code>ChatAgentConfig</code>, <code>ChatAgent</code> and <code>Task</code> object as usual.</p> <p>E.g. you can create an object of class <code>OpenAIGPT</code> (which represents any LLM with an OpenAI-compatible API) and interact with it directly: <pre><code>llm = lm.OpenAIGPT(llm_config)\nmessages = [\n    LLMMessage(content=\"You are a helpful assistant\",  role=Role.SYSTEM),\n    LLMMessage(content=\"What is the capital of Ontario?\",  role=Role.USER),\n],\nresponse = mdl.chat(messages, max_tokens=50)\n</code></pre></p> <p>When you interact directly with the LLM, you are responsible for keeping dialog history. Also you would often want an LLM to have access to tools/functions and external data/documents (e.g. vector DB or traditional DB). An Agent class simplifies managing all of these. For example, you can create an Agent powered by the above LLM, wrap it in a Task and have it run as an interactive chat app:</p> <pre><code>agent_config = lr.ChatAgentConfig(llm=llm_config, name=\"my-llm-agent\")\nagent = lr.ChatAgent(agent_config)\n\ntask = lr.Task(agent, name=\"my-llm-task\")\ntask.run()\n</code></pre>"},{"location":"tutorials/non-openai-llms/#example-simple-chat-script-with-a-non-openai-proprietary-model","title":"Example: Simple Chat script with a non-OpenAI proprietary model","text":"<p>Many of the Langroid example scripts have a convenient <code>-m</code>  flag that lets you easily switch to a different model. For example, you can run  the <code>chat.py</code> script in the <code>examples/basic</code> folder with the  <code>litellm/claude-instant-v1</code> model: <pre><code>python3 examples/basic/chat.py -m litellm/claude-instant-1\n</code></pre></p>"},{"location":"tutorials/non-openai-llms/#quick-testing-with-non-openai-models","title":"Quick testing with non-OpenAI models","text":"<p>There are numerous tests in the main Langroid repo that involve LLMs, and once you setup the dev environment as described in the README of the repo,  you can run any of those tests (which run against the default GPT4 model) against local/remote models that are proxied by <code>liteLLM</code> (or served locally via the options mentioned above, such as <code>oobabooga</code>, <code>ollama</code> or <code>llama-cpp-python</code>), using the <code>--m &lt;model-name&gt;</code> option, where <code>model-name</code> takes one of the forms above. Some examples of tests are:</p> <p><pre><code>pytest -s tests/test_llm.py --m local/localhost:8000\npytest -s tests/test_llm.py --m litellm/claude-instant-1\n</code></pre> When the <code>--m</code> option is omitted, the default OpenAI GPT4 model is used.</p> <p><code>chat_context_length</code> is not affected by <code>--m</code></p> <p>Be aware that the <code>--m</code> only switches the model, but does not affect the <code>chat_context_length</code>    parameter in the <code>OpenAIGPTConfig</code> object. which you may need to adjust for different models.   So this option is only meant for quickly testing against different models, and not meant as   a way to switch between models in a production environment.</p>"},{"location":"tutorials/postgresql-agent/","title":"Chat with a PostgreSQL DB using SQLChatAgent","text":"<p>The <code>SQLChatAgent</code> is designed to facilitate interactions with an SQL database using natural language. A ready-to-use script based on the <code>SQLChatAgent</code> is available in the <code>langroid-examples</code>  repo at <code>examples/data-qa/sql-chat/sql_chat.py</code> (and also in a similar location in the main <code>langroid</code> repo). This tutorial walks you through how you might use the <code>SQLChatAgent</code> if you were to write your own script from scratch. We also show some of the internal workings of this Agent.</p> <p>The agent uses the schema context to generate SQL queries based on a user's input. Here is a tutorial on how to set up an agent with your PostgreSQL database. The steps for other databases are similar. Since the agent implementation relies  on SqlAlchemy, it should work with any SQL DB that supports SqlAlchemy. It offers enhanced functionality for MySQL and PostgreSQL by  automatically extracting schemas from the database. </p>"},{"location":"tutorials/postgresql-agent/#before-you-begin","title":"Before you begin","text":"<p>Data Privacy Considerations</p> <p>Since the SQLChatAgent uses the OpenAI GPT-4 as the underlying language model, users should be aware that database information processed by the agent may be sent to OpenAI's API and should therefore be comfortable with this.</p> <ol> <li> <p>Install PostgreSQL dev libraries for your platform, e.g.</p> <ul> <li><code>sudo apt-get install libpq-dev</code> on Ubuntu,</li> <li><code>brew install postgresql</code> on Mac, etc.</li> </ul> </li> <li> <p>Follow the general setup guide to get started with Langroid (mainly, install <code>langroid</code> into your virtual env, and set up suitable values in  the <code>.env</code> file). Note that to use the SQLChatAgent with a PostgreSQL database, you need to install the <code>langroid[postgres]</code> extra, e.g.:</p> <ul> <li><code>pip install langroid[postgres]</code> or </li> <li><code>poetry add langroid[postgres]</code> or </li> <li><code>poetry install -E postgres</code>.</li> </ul> <p>If this gives you an error, try <code>pip install psycopg2-binary</code> in your virtualenv.</p> </li> </ol>"},{"location":"tutorials/postgresql-agent/#initialize-the-agent","title":"Initialize the agent","text":"<pre><code>from langroid.agent.special.sql.sql_chat_agent import (\n    SQLChatAgent,\n    SQLChatAgentConfig,\n)\n\nagent = SQLChatAgent(\n    config=SQLChatAgentConfig(\n        database_uri=\"postgresql://example.db\",\n    )\n)\n</code></pre>"},{"location":"tutorials/postgresql-agent/#configuration","title":"Configuration","text":"<p>The following components of <code>SQLChatAgentConfig</code> are optional but strongly recommended for improved results:</p> <ul> <li><code>context_descriptions</code>: A nested dictionary that specifies the schema context for   the agent to use when generating queries, for example:</li> </ul> <pre><code>{\n  \"table1\": {\n    \"description\": \"description of table1\",\n    \"columns\": {\n      \"column1\": \"description of column1 in table1\",\n      \"column2\": \"description of column2 in table1\"\n    }\n  },\n  \"employees\": {\n    \"description\": \"The 'employees' table contains information about the employees. It relates to the 'departments' and 'sales' tables via foreign keys.\",\n    \"columns\": {\n      \"id\": \"A unique identifier for an employee. This ID is used as a foreign key in the 'sales' table.\",\n      \"name\": \"The name of the employee.\",\n      \"department_id\": \"The ID of the department the employee belongs to. This is a foreign key referencing the 'id' in the 'departments' table.\"\n    }\n  }\n}\n</code></pre> <p>By default, if no context description json file is provided in the config, the  agent will automatically generate the file using the built-in Postgres table/column comments.</p> <ul> <li> <p><code>schema_tools</code>: When set to <code>True</code>, activates a retrieval mode where the agent   systematically requests only the parts of the schemas relevant to the current query.    When this option is enabled, the agent performs the following steps:</p> <ol> <li>Asks for table names.</li> <li>Asks for table descriptions and column names from possibly relevant table    names.</li> <li>Asks for column descriptions from possibly relevant columns.</li> <li>Writes the SQL query.</li> </ol> </li> </ul> <p>Setting <code>schema_tools=True</code> is especially useful for large schemas where it is costly or impossible    to include the entire schema in a query context.    By selectively using only the relevant parts of the context descriptions, this mode   reduces token usage, though it may result in 1-3 additional OpenAI API calls before   the final SQL query is generated.</p>"},{"location":"tutorials/postgresql-agent/#putting-it-all-together","title":"Putting it all together","text":"<p>In the code below, we will allow the agent to generate the context descriptions from table comments by excluding the <code>context_descriptions</code> config option. We set <code>schema_tools</code> to <code>True</code> to enable the retrieval mode.</p> <pre><code>from langroid.agent.special.sql.sql_chat_agent import (\n    SQLChatAgent,\n    SQLChatAgentConfig,\n)\n\n# Initialize SQLChatAgent with a PostgreSQL database URI and enable schema_tools\nagent = SQLChatAgent(gi\nconfig = SQLChatAgentConfig(\n    database_uri=\"postgresql://example.db\",\n    schema_tools=True,\n)\n)\n\n# Run the task to interact with the SQLChatAgent\ntask = Task(agent)\ntask.run()\n</code></pre> <p>By following these steps, you should now be able to set up an <code>SQLChatAgent</code> that interacts with a PostgreSQL database, making querying a seamless experience.</p> <p>In the <code>langroid</code> repo we have provided a ready-to-use script <code>sql_chat.py</code> based on the above, that you can use right away to interact with your PostgreSQL database:</p> <pre><code>python3 examples/data-qa/sql-chat/sql_chat.py\n</code></pre> <p>This script will prompt you for the database URI, and then start the agent.</p>"}]}